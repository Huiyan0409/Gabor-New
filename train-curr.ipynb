{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim  \n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import argparse\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from skimage.util import random_noise\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "import kornia.augmentation.functional as FF\n",
    "import statistics \n",
    "import math\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./\"\n",
    "def default_loader(path):\n",
    "    return Image.open(path).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self,root, datatxt, transform=None, target_transform=None,loader=default_loader):\n",
    "        super(MyDataset,self).__init__()\n",
    "        fh = open(root + datatxt, 'r') \n",
    "        imgs = []     \n",
    "        data = []\n",
    "        label = []\n",
    "        for line in fh:                \n",
    "            line = line.rstrip()       \n",
    "            data.append(line)\n",
    "        for line in range(len(data)-1):\n",
    "            words = data[line].split()  \n",
    "            imgs.append((words[0])) \n",
    "            label.append(int(words[1]))\n",
    "            \n",
    "        \n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.label = torch.LongTensor(label)\n",
    " \n",
    "    # def __getitem__(self, index):    \n",
    "    def __getitem__(self, idx):    \n",
    "        image = Image.open(str(self.imgs[idx]))\n",
    "        # image = image.convert('RGB')\n",
    "        image = image.convert('L')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        label = self.label[idx]\n",
    "        return image, label\n",
    "    def __len__(self): \n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_trainData: 71432\n",
      "num_of_trainNewData: 89827\n",
      "num_of_testData: 8141\n",
      "num_of_testNewData: 10299\n",
      "num_of_originData: 8141\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(root='./',datatxt='train.txt', transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#         AddGaussianNoise(0., 0.05)\n",
    "        ]))\n",
    "trainNew_dataset = MyDataset(root='./',datatxt='train-new.txt', transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#         AddGaussianNoise(0., 0.05)\n",
    "        ]))\n",
    "test_dataset = MyDataset(root='./',datatxt='test.txt', transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#         AddGaussianNoise(0., 0.05)\n",
    "        ]))\n",
    "testNew_dataset = MyDataset(root='./',datatxt='test-new.txt', transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#         AddGaussianNoise(0., 0.05)\n",
    "        ]))\n",
    "origin_dataset = MyDataset(root='./',datatxt='origin.txt', transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#         AddGaussianNoise(0., 0.05)\n",
    "        ]))\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True,num_workers=1)\n",
    "trainNew_loader = DataLoader(dataset=trainNew_dataset, batch_size=64, shuffle=True,num_workers=1)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False,num_workers=1)\n",
    "testNew_loader = DataLoader(dataset=testNew_dataset, batch_size=64, shuffle=False,num_workers=1)\n",
    "origin_loader = DataLoader(dataset=origin_dataset, batch_size=64, shuffle=False,num_workers=1)\n",
    "print('num_of_trainData:', len(train_dataset))\n",
    "print('num_of_trainNewData:', len(trainNew_dataset))\n",
    "print('num_of_testData:', len(test_dataset))\n",
    "print('num_of_testNewData:', len(testNew_dataset))\n",
    "print('num_of_originData:', len(origin_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label： tensor(0) shape: (1, 19, 19)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "idx = 11\n",
    "img = test_dataset[idx][0].numpy()\n",
    "plt.imshow(img[0], cmap = 'gray')\n",
    "plt.imsave('test.png', img[0], cmap = 'gray')\n",
    "# figure, b = plt.subplots()\n",
    "# figure.set_size_inches(0.19, 0.19)\n",
    "plt.axis('off') \n",
    "print('label：',train_dataset[idx][1], 'shape:', img.shape)\n",
    "print(type(img[0]))\n",
    "matplotlib.image.imsave('name.png',img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative:  36885\n",
      "positive:  34546\n"
     ]
    }
   ],
   "source": [
    "positive = 0\n",
    "negative = 0\n",
    "for idx in range(1,71432):\n",
    "    if train_dataset[idx][1].item() == 0:\n",
    "        negative = negative+1\n",
    "    else:\n",
    "        positive = positive+1\n",
    "print('negative: ', negative)\n",
    "print('positive: ', positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaborConvPC(nn.Module):\n",
    "#     def __init__(self, kernel_size, in_channels, num_orientations, num_scales):\n",
    "#         super(GaborConvPC, self).__init__()\n",
    "#         self.sigma1, self.theta1, self.Lambda1, self.psi1, self.gamma1, self.bias1, self.weights1, self.w, self.b = self.generate_parameters(num_orientations*num_scales, in_channels)\n",
    "#         self.sigma2, self.theta2, self.Lambda2, self.psi2, self.gamma2, self.bias2, self.weights2, self.w, self.b = self.generate_parameters(num_orientations*num_scales, in_channels)\n",
    "#         # self.filter1 = self.whole_filter(in_channels, channel1, kernel_size, self.sigma1, self.theta1, self.Lambda1, self.psi1, self.gamma1)\n",
    "#         self.filter_cos = self.whole_filter(in_channels, num_orientations, num_scales, kernel_size, self.sigma1, self.theta1, self.Lambda1, self.psi1, self.gamma1, True)\n",
    "#         self.filter_sin = self.whole_filter(in_channels, num_orientations, num_scales, kernel_size, self.sigma1, self.theta1, self.Lambda1, self.psi1, self.gamma1, False)       \n",
    "        \n",
    "#         self.fc1 = nn.Linear(1*1*48, 24)\n",
    "#         self.fc2 = nn.Linear(24, 2)\n",
    "\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x_cos = F.conv2d(x, self.filter_cos, bias=self.bias1)\n",
    "#         x_sin = F.conv2d(x, self.filter_sin, bias=self.bias2)\n",
    "#         x_comb = torch.cat((x_cos, x_sin), 2)\n",
    "\n",
    "#         x_cos = x_cos.view(len(x), 1, 1, 48)\n",
    "#         x_sin = x_sin.view(len(x), 1, 1, 48)\n",
    "#         weighted_cos = (torch.matmul(x_cos, self.weights1)).view(len(x), 1)\n",
    "#         weighted_sin = (torch.matmul(x_sin, self.weights1)).view(len(x), 1)\n",
    "\n",
    "#         numerator = torch.norm(torch.cat([weighted_cos, weighted_sin], 1), dim=1)\n",
    "# #         print(\"numerator\", numerator.size())\n",
    "#         x_comb_norm = torch.norm(x_comb, dim=2)\n",
    "#         x_comb_norm = x_comb_norm.view(len(x), 1, 48)\n",
    "# #         print(\"x_comb_norm\", x_comb_norm.size())\n",
    "#         denominator = torch.matmul(x_comb_norm, torch.abs(self.weights1))\n",
    "#         denominator = denominator.view(len(x))\n",
    "\n",
    "#         pc = numerator / denominator                \n",
    "#         return torch.sigmoid(self.w * pc + self.b)\n",
    "\n",
    "\n",
    "#     def generate_parameters(self, dim_out, dim_in):\n",
    "#         sigma = nn.Parameter(torch.randn(1, 1))\n",
    "#         theta = nn.Parameter(torch.randn(1, 1))\n",
    "#         Lambda = nn.Parameter(torch.randn(1, 1))\n",
    "#         psi = nn.Parameter(torch.randn(1, 1))\n",
    "#         gamma = nn.Parameter(torch.randn(1, 1))\n",
    "#         bias = nn.Parameter(torch.randn(dim_out))\n",
    "#         weights = nn.Parameter(torch.randn(1, 48, 1))\n",
    "#         w = nn.Parameter(torch.randn(1, 1))\n",
    "#         b = nn.Parameter(torch.randn(1, 1))\n",
    "#         return sigma, theta, Lambda, psi, gamma, bias, weights, w, b\n",
    "    def __init__(self, kernel_size, in_channels, num_orientations, num_scales):\n",
    "        super(GaborConvPC, self).__init__()\n",
    "        self.sigma, self.theta, self.Lambda, self.psi, self.gamma, self.bias1, self.bias2, self.weights, self.w, self.b = self.generate_parameters(num_orientations*num_scales, in_channels)\n",
    "        self.filter_cos = self.whole_filter(in_channels, num_orientations, num_scales, kernel_size, self.sigma, self.theta, self.Lambda, self.psi, self.gamma, True)\n",
    "        self.filter_sin = self.whole_filter(in_channels, num_orientations, num_scales, kernel_size, self.sigma, self.theta, self.Lambda, self.psi, self.gamma, False)\n",
    "\n",
    "        self.fc1 = nn.Linear(1*1*48, 24)\n",
    "        self.fc2 = nn.Linear(24, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cos = F.conv2d(x, self.filter_cos, bias=self.bias1)\n",
    "        x_sin = F.conv2d(x, self.filter_sin, bias=self.bias2)\n",
    "        x_comb = torch.cat((x_cos, x_sin), 2)\n",
    "        x_cos = x_cos.view(len(x), 1, 1, 48)\n",
    "        x_sin = x_sin.view(len(x), 1, 1, 48)\n",
    "        weighted_cos = (torch.matmul(x_cos, self.weights)).view(len(x), 1)\n",
    "        weighted_sin = (torch.matmul(x_sin, self.weights)).view(len(x), 1)\n",
    "\n",
    "        numerator = torch.norm(torch.cat([weighted_cos, weighted_sin], 1), dim=1)\n",
    "        x_comb_norm = torch.norm(x_comb, dim=2)\n",
    "        x_comb_norm = x_comb_norm.view(len(x), 1, 48)\n",
    "        denominator = torch.matmul(x_comb_norm, torch.abs(self.weights))\n",
    "        denominator = denominator.view(len(x))\n",
    "        pc = numerator / denominator                \n",
    "        return torch.sigmoid(self.w * pc + self.b),pc\n",
    "\n",
    "\n",
    "    def generate_parameters(self, dim_out, dim_in):\n",
    "        sigma = nn.Parameter(torch.randn(1, 1))\n",
    "        theta = nn.Parameter(torch.randn(1, 1))\n",
    "        Lambda = nn.Parameter(torch.randn(1, 1))\n",
    "        psi = nn.Parameter(torch.randn(1, 1))\n",
    "        gamma = nn.Parameter(torch.randn(1, 1))\n",
    "        bias1 = nn.Parameter(torch.randn(dim_out))\n",
    "        bias2 = nn.Parameter(torch.randn(dim_out))\n",
    "        weights = nn.Parameter(torch.randn(1, 48, 1))\n",
    "        w = nn.Parameter(torch.randn(1, 1))\n",
    "        b = nn.Parameter(torch.randn(1, 1))\n",
    "        return sigma, theta, Lambda, psi, gamma, bias1, bias2, weights, w, b\n",
    "\n",
    "\n",
    "    def whole_filter(self, in_channels, num_orientations, num_scales, kernel_size, sigma, theta, Lambda, psi, gamma, cos):\n",
    "        result = torch.zeros(num_orientations*num_scales, in_channels, kernel_size, kernel_size) # \\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW\n",
    "        for i in range(num_orientations):\n",
    "            for j in range(num_scales):\n",
    "                result[i*num_scales + j] = self.one_filter(in_channels, kernel_size, sigma[0]*(2.1**j), theta[0]+i*np.pi/num_orientations, Lambda[0], psi[0], gamma[0], cos)\n",
    "        return nn.Parameter(result)\n",
    "\n",
    "\n",
    "    def one_filter(self, in_channels, kernel_size, sigma, theta, Lambda, psi, gamma, cos):\n",
    "        result = torch.zeros(in_channels, kernel_size, kernel_size)\n",
    "        for i in range(in_channels):\n",
    "            result[i] = self.gabor_fn(sigma, theta, Lambda, psi, gamma, kernel_size, cos)\n",
    "        return nn.Parameter(result)\n",
    "\n",
    "\n",
    "    def gabor_fn(self, sigma, theta, Lambda, psi, gamma, kernel_size, cos):\n",
    "        sigma_x = sigma\n",
    "        # sigma_y = float(sigma) / gamma\n",
    "        sigma_y = sigma / gamma\n",
    "\n",
    "        # Bounding box\n",
    "        half_size = (kernel_size - 1) // 2\n",
    "        ymin, xmin = -half_size, -half_size\n",
    "        ymax, xmax = half_size, half_size\n",
    "    #     (y, x) = np.meshgrid(np.arange(ymin, ymax + 1), np.arange(xmin, xmax + 1))\n",
    "        y, x = torch.meshgrid([torch.arange(ymin, ymax+1), torch.arange(xmin,xmax+1)])\n",
    "\n",
    "        if cos:\n",
    "            gb = torch.exp(-.5 * (x**2 / sigma_x**2 + y**2 / sigma_y**2)) * torch.cos(2 * np.pi / Lambda * x + psi)\n",
    "        else:\n",
    "            gb = torch.exp(-.5 * (x**2 / sigma_x**2 + y**2 / sigma_y**2)) * torch.sin(2 * np.pi / Lambda * x + psi)\n",
    "\n",
    "        # Rotation\n",
    "        degrees = theta * 180 / np.pi\n",
    "        gb = FF.apply_rotation(gb, {'degrees': torch.tensor(degrees)}, {'interpolation': torch.tensor([1]), 'align_corners': torch.tensor(True)})\n",
    "        gb = gb.squeeze()\n",
    "        return gb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myLoss(output, target):\n",
    "#     print(\"size,size:\",output.size(), target.size())\n",
    "#     print(\"size,\", ((1-2*target) * torch.log(output)).size())\n",
    "#     return -torch.sum(target * torch.log(output) + (1-target) * torch.log(1-output)) / len(output)\n",
    "    return -torch.sum((36885/34546)*target * torch.log(output) + (1-target) * torch.log(1-output)) / len(output)\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)[0]\n",
    "#         loss = F.nll_loss(output, target)\n",
    "        loss = myLoss(output, target)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item()/64)\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    train_loss = sum(loss_list)/len(loss_list)\n",
    "    return train_loss\n",
    "\n",
    "        \n",
    "def test(args, model, device, test_loader,count,epoch,train_loss):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    result= [[0,0], [0,0]] \n",
    "    correct_pc_true0 = []\n",
    "    correct_pc_true1 = []\n",
    "    wrong_pc_true0 = []\n",
    "    wrong_pc_true1 = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx,(data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)[0]\n",
    "#             test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            test_loss += myLoss(output, target)\n",
    "#             pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            t = Variable(torch.Tensor([0.5]))\n",
    "            pred = (output > t) * 1\n",
    "            pred = torch.reshape(pred, (len(target), 1))\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            cmat = confusion_matrix(target.view_as(pred), pred, labels=[0, 1]) \n",
    "            result = [[result[i][j] + cmat[i][j]  for j in range(len(result[0]))] for i in range(len(result))] \n",
    "             # Store wrongly predicted images\n",
    "            if epoch == 16:\n",
    "                scale = 0\n",
    "                ori = 0\n",
    "                for i in range(len(model.filter_cos)):\n",
    "#                 sample = model.filter_sin[i]\n",
    "#                 sample = sample * 255.\n",
    "#                 sample = sample.byte()\n",
    "#                 img = TF.to_pil_image(sample)\n",
    "                    figure, b = plt.subplots()\n",
    "                    figure.set_size_inches(0.19, 0.19)\n",
    "                    plt.axis('off')\n",
    "                    plt.imshow(model.filter_cos[i].detach().numpy()[0], cmap='gray')\n",
    "#                 np.savetxt('train-coeff.txt', model.filter_sin[i].detach().numpy()[0], delimiter='    ',fmt='%1.2f')\n",
    "                    si = model.sigma.detach().numpy()*(2.1**scale)\n",
    "                    de = model.theta.detach().numpy()+ori*np.pi/8\n",
    "#                 img.save('./filter/{}_ori_{}scale_{}.png'.ormat('sin',ori,scale))\n",
    "                    plt.savefig(\"./filter/%s_scale_%.2fdeg_%.2f.png\" % ('cos',si,de), dpi=100,pad_inches=0.0,bbox_inches='tight')\n",
    "                    if scale == 5:\n",
    "                        scale = 0\n",
    "                        ori = ori + 1\n",
    "                    else:\n",
    "                        scale = scale + 1\n",
    "                scale = 0\n",
    "                ori = 0\n",
    "                for i in range(len(model.filter_sin)):\n",
    "#                 sample = model.filter_sin[i]\n",
    "#                 sample = sample * 255.\n",
    "#                 sample = sample.byte()\n",
    "#                 img = TF.to_pil_image(sample)\n",
    "                    figure, b = plt.subplots()\n",
    "                    figure.set_size_inches(0.19, 0.19)\n",
    "                    plt.axis('off')\n",
    "                    plt.imshow(model.filter_sin[i].detach().numpy()[0], cmap='gray')\n",
    "#                 np.savetxt('train-coeff.txt', model.filter_sin[i].detach().numpy()[0], delimiter='    ',fmt='%1.2f')\n",
    "                    si = model.sigma.detach().numpy()*(2.1**scale)\n",
    "                    de = model.theta.detach().numpy()+ori*np.pi/8\n",
    "#                 img.save('./filter/{}_ori_{}scale_{}.png'.ormat('sin',ori,scale))\n",
    "                    plt.savefig(\"./filter/%s_scale_%.2fdeg_%.2f.png\" % ('sin',si,de), dpi=100,pad_inches=0.0,bbox_inches='tight')\n",
    "                    if scale == 5:\n",
    "                        scale = 0\n",
    "                        ori = ori + 1\n",
    "                    else:\n",
    "                        scale = scale + 1\n",
    "                count = 0\n",
    "                wrong_idx = (pred != target.view_as(pred)).nonzero()[:, 0]\n",
    "                wrong_samples = data[wrong_idx]\n",
    "                wrong_preds = pred[wrong_idx]\n",
    "                actual_preds = target.view_as(pred)[wrong_idx]\n",
    "                true_idx = (pred == target.view_as(pred)).nonzero()[:, 0]\n",
    "                true_samples = data[true_idx]\n",
    "                true_preds = pred[true_idx]\n",
    "                for i in range(len(true_idx)):\n",
    "                    true_pred = true_preds[i]\n",
    "                    if true_pred.item() == 0:\n",
    "                        correct_pc_true0.append(model(data)[1][true_idx[i]].item())\n",
    "                    else:\n",
    "                        correct_pc_true1.append(model(data)[1][true_idx[i]].item())\n",
    "                        \n",
    "                for i in range(len(wrong_idx)):\n",
    "                    actual_pred = actual_preds[i]\n",
    "                    if actual_pred.item() == 0:\n",
    "                        wrong_pc_true0.append(model(data)[1][wrong_idx[i]].item())\n",
    "                    else:\n",
    "                        wrong_pc_true1.append(model(data)[1][wrong_idx[i]].item())\n",
    "                    sample = wrong_samples[i]\n",
    "                    wrong_pred = wrong_preds[i]\n",
    "                    actual_pred = actual_preds[i]\n",
    "                    # Undo normalization\n",
    "            #         sample = sample * 0.3081\n",
    "            #         sample = sample + 0.1307\n",
    "                    sample = sample * 255.\n",
    "                    sample = sample.byte()\n",
    "                    img = TF.to_pil_image(sample)\n",
    "                    num = batch_idx * 64 + wrong_idx[i]\n",
    "                    count = count +1\n",
    "#                     img.save('./wrong-gabor/{}_true{}_pc{:.4f}.png'.format(\n",
    "#                     batch_idx*64+count, actual_pred.item(),model(data)[1][wrong_idx[i]].item()))\n",
    "                    \n",
    "    #                 print(batch_idx,wrong_idx[i])\n",
    "                    img_ori = origin_dataset[num][0].numpy()\n",
    "                    plt.imsave('./wrong-gabor/{}_true{}__pc{:.4f}_ori.png'.format(\n",
    "                    batch_idx*64+count, actual_pred.item(), model(data)[1][wrong_idx[i]].item()),img_ori[0], cmap = 'gray')\n",
    "                    \n",
    "            \n",
    "                \n",
    "                    \n",
    "                \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Positive accuracy: {}/{} ({:.0f}%), Negative accuracy: {}/{} ({:.0f}%), train loss: {:.4f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset), \n",
    "        result[1][1],result[1][1]+result[1][0],100. * result[1][1]/(result[1][1]+result[1][0]),\n",
    "        result[0][0],result[0][0]+result[0][1],100. * result[0][0]/(result[0][0]+result[0][1]),train_loss))\n",
    "    if len(correct_pc_true0) != 0:\n",
    "        print('\\ncorrect_0: mean: {:.4f}, std: {:.4f}'.format(statistics.mean(correct_pc_true0),statistics.pstdev(correct_pc_true0)))\n",
    "        print('\\ncorrect_1: mean: {:.4f}, std: {:.4f}'.format(statistics.mean(correct_pc_true1),statistics.pstdev(correct_pc_true1)))\n",
    "        print('\\nwrong_0: mean: {:.4f}, std: {:.4f}'.format(statistics.mean(wrong_pc_true0), statistics.pstdev(wrong_pc_true0)))\n",
    "        print('\\nwrong_1: mean: {:.4f}, std: {:.4f}'.format(statistics.mean(wrong_pc_true1), statistics.pstdev(wrong_pc_true1)))\n",
    "    return test_loss, correct\n",
    "    \n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "    parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=True,\n",
    "                        help='For Saving the current Model')\n",
    "    parser.add_argument('--std', type=float, default=0, metavar='STD',\n",
    "                        help='noise-std (default: 0)')\n",
    "    parser.add_argument('--mean', type=float, default=0, metavar='MEAN',\n",
    "                        help='noise-std (default: 0)')\n",
    "#     args = parser.parse_args()\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "#     transform=transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,))\n",
    "#         ])\n",
    "    model = GaborConv2dPC().to(device)\n",
    "    # if torch.cuda.is_available():\n",
    "    #     torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    count = 0\n",
    "    for param in model.parameters():\n",
    "        print(type(param.data), param.size())\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_accuracy_list = []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_result = test(args, model, device, test_loader,count,epoch,train_loss)\n",
    "        test_loss_list.append(test_result[0])\n",
    "        test_accuracy_list.append(test_result[1])\n",
    "        # for param in model.parameters():\n",
    "        #     print(param.size(), param.data)\n",
    "        # print(model.state_dict())\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    if (args.save_model):\n",
    "        torch.save(model.state_dict(),\"pretrain_gabor.pt\")\n",
    "    plt.plot(train_loss_list, color='blue',label='train loss')  \n",
    "    plt.plot(test_loss_list, color='red',label='test loss')  \n",
    "    plt.legend()\n",
    "    print(test_accuracy_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaborConv2dPC(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        out_channels=48*2,\n",
    "        kernel_size=19,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        bias=False,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_calculated = False\n",
    "\n",
    "        self.conv_layer = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            dilation,\n",
    "            groups,\n",
    "            bias,\n",
    "            padding_mode,\n",
    "        )\n",
    "        self.kernel_size = self.conv_layer.kernel_size\n",
    "\n",
    "        # small addition to avoid division by zero\n",
    "        self.delta = 1e-3\n",
    "        self.delta1 = 1e-3\n",
    "#         self.delta1 = nn.Parameter(torch.tensor([1e-5]), requires_grad = True)\n",
    "\n",
    "        # freq, theta, sigma are set up according to S. Meshgini,\n",
    "        # A. Aghagolzadeh and H. Seyedarabi, \"Face recognition using\n",
    "        # Gabor filter bank, kernel principal component analysis\n",
    "        # and support vector machine\"\n",
    "        self.sigma = nn.Parameter(2*torch.randn(1), requires_grad=True)\n",
    "        self.sigma_list = torch.empty((out_channels//2, in_channels))\n",
    "        self.theta = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "        self.theta_list = torch.empty((out_channels//2, in_channels))\n",
    "        for ori in range(8):\n",
    "            for scale in range(6):\n",
    "                self.sigma_list[ori*6+scale] = self.sigma * (2.1 ** (scale+1))\n",
    "                self.theta_list[ori*6+scale] = self.theta + math.pi / 8 * ori\n",
    "\n",
    "        # self.freq = Parameter(\n",
    "        #     (math.pi / 2)\n",
    "        #     * math.sqrt(2)\n",
    "        #     ** (-torch.randint(0, 5, (out_channels, in_channels))).type(torch.Tensor),\n",
    "        #     requires_grad=True,\n",
    "        # )\n",
    "\n",
    "        # self.theta = Parameter(\n",
    "        #     (math.pi / 8)\n",
    "        #     * torch.randint(0, 8, (out_channels, in_channels)).type(torch.Tensor),\n",
    "        #     requires_grad=True,\n",
    "        # )\n",
    "        # self.sigma = Parameter(math.pi / self.freq, requires_grad=True)\n",
    "        self.gamma = nn.Parameter(torch.randn(1)+0.5, requires_grad=True)\n",
    "        self.Lambda = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "        self.psi = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "        # self.psi = Parameter(\n",
    "        #     math.pi * torch.rand(out_channels, in_channels), requires_grad=True\n",
    "        # )\n",
    "\n",
    "        self.x0 = nn.Parameter(\n",
    "            torch.ceil(torch.Tensor([self.kernel_size[0] / 2]))[0], requires_grad=False\n",
    "        )\n",
    "        self.y0 = nn.Parameter(\n",
    "            torch.ceil(torch.Tensor([self.kernel_size[1] / 2]))[0], requires_grad=False\n",
    "        )\n",
    "\n",
    "        self.y, self.x = torch.meshgrid(\n",
    "            [\n",
    "                torch.linspace(-self.x0 + 1, self.x0 + 0, self.kernel_size[0]),\n",
    "                torch.linspace(-self.y0 + 1, self.y0 + 0, self.kernel_size[1]),\n",
    "            ]\n",
    "        )\n",
    "        self.y = nn.Parameter(self.y)\n",
    "        self.x = nn.Parameter(self.x)\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty(self.conv_layer.weight.shape, requires_grad=True),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        \n",
    "        self.coeff = nn.Parameter(torch.randn(1, 48, 1), requires_grad=True)\n",
    "        self.w = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "\n",
    "        self.register_parameter(\"theta\", self.theta)\n",
    "        self.register_parameter(\"sigma\", self.sigma)\n",
    "        self.register_parameter(\"gamma\", self.gamma)\n",
    "        self.register_parameter(\"Lambda\", self.Lambda)\n",
    "        self.register_parameter(\"psi\", self.psi)\n",
    "        self.register_parameter(\"x_shape\", self.x0)\n",
    "        self.register_parameter(\"y_shape\", self.y0)\n",
    "        self.register_parameter(\"y_grid\", self.y)\n",
    "        self.register_parameter(\"x_grid\", self.x)\n",
    "        self.register_parameter(\"weight\", self.weight)\n",
    "        self.register_parameter(\"coeff\", self.coeff)\n",
    "        self.register_parameter(\"w\", self.w)\n",
    "        self.register_parameter(\"b\", self.b)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        if self.training:\n",
    "            self.calculate_weights()\n",
    "            self.is_calculated = False\n",
    "        if not self.training:\n",
    "            if not self.is_calculated:\n",
    "                self.calculate_weights()\n",
    "                self.is_calculated = True\n",
    "#         return self.conv_layer(input_tensor)\n",
    "        output = self.conv_layer(input_tensor)\n",
    "#         print(output.shape)\n",
    "        x_cos = output[:,:self.conv_layer.out_channels//2,:,:]\n",
    "        x_sin = output[:,self.conv_layer.out_channels//2:,:,:]\n",
    "        \n",
    "        x_comb = torch.cat((x_cos, x_sin), 2)\n",
    "        x_cos = x_cos.view(len(input_tensor), 1, 1, 48)\n",
    "        x_sin = x_sin.view(len(input_tensor), 1, 1, 48)\n",
    "        weighted_cos = (torch.matmul(x_cos, self.coeff)).view(len(input_tensor), 1)\n",
    "        weighted_sin = (torch.matmul(x_sin, self.coeff)).view(len(input_tensor), 1)\n",
    "\n",
    "        numerator = torch.norm(torch.cat([weighted_cos, weighted_sin], 1), dim=1)\n",
    "        x_comb_norm = torch.norm(x_comb, dim=2)\n",
    "        x_comb_norm = x_comb_norm.view(len(input_tensor), 1, 48)\n",
    "        denominator = torch.matmul(x_comb_norm, torch.abs(self.coeff))\n",
    "        denominator = denominator.view(len(input_tensor))\n",
    "        pc = numerator / (denominator + self.delta1)             \n",
    "        return torch.sigmoid(self.w * pc + self.b), pc\n",
    "        \n",
    "\n",
    "    def calculate_weights(self):\n",
    "        for i in range(self.conv_layer.out_channels//2):\n",
    "            for j in range(self.conv_layer.in_channels):\n",
    "                # sigma = self.sigma[i, j].expand_as(self.y)\n",
    "                # freq = self.freq[i, j].expand_as(self.y)\n",
    "                # theta = self.theta[i, j].expand_as(self.y)\n",
    "                # psi = self.psi[i, j].expand_as(self.y)\n",
    "\n",
    "                rotx = self.x * torch.cos(self.theta_list[i, j]) + self.y * torch.sin(self.theta_list[i, j])\n",
    "                roty = -self.x * torch.sin(self.theta_list[i, j]) + self.y * torch.cos(self.theta_list[i, j])\n",
    "\n",
    "                g = torch.exp(\n",
    "                    -0.5 * ((rotx ** 2 + (self.gamma **2) * (roty ** 2)) / (self.sigma_list[i, j]**2 + self.delta))\n",
    "                )\n",
    "#                 g = torch.exp(\n",
    "#                     -0.5 * ((rotx ** 2 + (roty ** 2)) / (self.sigma_list[i, j]**2 + self.delta))\n",
    "#                 )\n",
    "                g_cos = g * torch.cos(rotx * self.Lambda + self.psi)\n",
    "                g_sin = g * torch.sin(rotx * self.Lambda + self.psi)\n",
    "                # g = g / (2 * math.pi * sigma ** 2)\n",
    "                self.conv_layer.weight.data[i, j] = g_cos\n",
    "                self.conv_layer.weight.data[i+self.conv_layer.out_channels//2, j] = g_sin\n",
    "\n",
    "    def _forward_unimplemented(self, *inputs: Any):\n",
    "        \"\"\"\n",
    "        code checkers makes implement this method,\n",
    "        looks like error in PyTorch\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.2520], requires_grad=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sigma\n",
    "model_test = GaborConv2dPC()\n",
    "model_test.calculate_weights()\n",
    "#改sigma, lambda对应的参数，画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([1])\n",
      "<class 'torch.Tensor'> torch.Size([1])\n",
      "<class 'torch.Tensor'> torch.Size([1])\n",
      "<class 'torch.Tensor'> torch.Size([1])\n",
      "<class 'torch.Tensor'> torch.Size([1])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([19, 19])\n",
      "<class 'torch.Tensor'> torch.Size([19, 19])\n",
      "<class 'torch.Tensor'> torch.Size([96, 1, 19, 19])\n",
      "<class 'torch.Tensor'> torch.Size([1, 48, 1])\n",
      "<class 'torch.Tensor'> torch.Size([1])\n",
      "<class 'torch.Tensor'> torch.Size([1])\n",
      "<class 'torch.Tensor'> torch.Size([96, 1, 19, 19])\n",
      "Train Epoch: 1 [0/71432 (0%)]\tLoss: 0.924336\n",
      "Train Epoch: 1 [6400/71432 (9%)]\tLoss: 0.720348\n",
      "Train Epoch: 1 [12800/71432 (18%)]\tLoss: 0.719067\n",
      "Train Epoch: 1 [19200/71432 (27%)]\tLoss: 0.723824\n",
      "Train Epoch: 1 [25600/71432 (36%)]\tLoss: 0.717085\n",
      "Train Epoch: 1 [32000/71432 (45%)]\tLoss: 0.714223\n",
      "Train Epoch: 1 [38400/71432 (54%)]\tLoss: 0.717747\n",
      "Train Epoch: 1 [44800/71432 (63%)]\tLoss: 0.712337\n",
      "Train Epoch: 1 [51200/71432 (72%)]\tLoss: 0.718540\n",
      "Train Epoch: 1 [57600/71432 (81%)]\tLoss: 0.713315\n",
      "Train Epoch: 1 [64000/71432 (90%)]\tLoss: 0.716503\n",
      "Train Epoch: 1 [70400/71432 (98%)]\tLoss: 0.709029\n",
      "\n",
      "Test set: Average loss: 0.0112, Accuracy: 4598/8141 (56%), Positive accuracy: 3323/4091 (81%), Negative accuracy: 1275/4050 (31%), train loss: 0.0112\n",
      "\n",
      "Train Epoch: 2 [0/71432 (0%)]\tLoss: 0.717172\n",
      "Train Epoch: 2 [6400/71432 (9%)]\tLoss: 0.705180\n",
      "Train Epoch: 2 [12800/71432 (18%)]\tLoss: 0.720164\n",
      "Train Epoch: 2 [19200/71432 (27%)]\tLoss: 0.707309\n",
      "Train Epoch: 2 [25600/71432 (36%)]\tLoss: 0.699036\n",
      "Train Epoch: 2 [32000/71432 (45%)]\tLoss: 0.684157\n",
      "Train Epoch: 2 [38400/71432 (54%)]\tLoss: 0.677714\n",
      "Train Epoch: 2 [44800/71432 (63%)]\tLoss: 0.681545\n",
      "Train Epoch: 2 [51200/71432 (72%)]\tLoss: 0.663385\n",
      "Train Epoch: 2 [57600/71432 (81%)]\tLoss: 0.663116\n",
      "Train Epoch: 2 [64000/71432 (90%)]\tLoss: 0.706340\n",
      "Train Epoch: 2 [70400/71432 (98%)]\tLoss: 0.679472\n",
      "\n",
      "Test set: Average loss: 0.0103, Accuracy: 5580/8141 (69%), Positive accuracy: 2630/4091 (64%), Negative accuracy: 2950/4050 (73%), train loss: 0.0109\n",
      "\n",
      "Train Epoch: 3 [0/71432 (0%)]\tLoss: 0.651517\n",
      "Train Epoch: 3 [6400/71432 (9%)]\tLoss: 0.668993\n",
      "Train Epoch: 3 [12800/71432 (18%)]\tLoss: 0.673820\n",
      "Train Epoch: 3 [19200/71432 (27%)]\tLoss: 0.680169\n",
      "Train Epoch: 3 [25600/71432 (36%)]\tLoss: 0.643191\n",
      "Train Epoch: 3 [32000/71432 (45%)]\tLoss: 0.648199\n",
      "Train Epoch: 3 [38400/71432 (54%)]\tLoss: 0.654150\n",
      "Train Epoch: 3 [44800/71432 (63%)]\tLoss: 0.624792\n",
      "Train Epoch: 3 [51200/71432 (72%)]\tLoss: 0.605281\n",
      "Train Epoch: 3 [57600/71432 (81%)]\tLoss: 0.545071\n",
      "Train Epoch: 3 [64000/71432 (90%)]\tLoss: 0.566465\n",
      "Train Epoch: 3 [70400/71432 (98%)]\tLoss: 0.602086\n",
      "\n",
      "Test set: Average loss: 0.0092, Accuracy: 5968/8141 (73%), Positive accuracy: 3248/4091 (79%), Negative accuracy: 2720/4050 (67%), train loss: 0.0100\n",
      "\n",
      "Train Epoch: 4 [0/71432 (0%)]\tLoss: 0.638764\n",
      "Train Epoch: 4 [6400/71432 (9%)]\tLoss: 0.625844\n",
      "Train Epoch: 4 [12800/71432 (18%)]\tLoss: 0.574467\n",
      "Train Epoch: 4 [19200/71432 (27%)]\tLoss: 0.529491\n",
      "Train Epoch: 4 [25600/71432 (36%)]\tLoss: 0.553620\n",
      "Train Epoch: 4 [32000/71432 (45%)]\tLoss: 0.613696\n",
      "Train Epoch: 4 [38400/71432 (54%)]\tLoss: 0.553700\n",
      "Train Epoch: 4 [44800/71432 (63%)]\tLoss: 0.570745\n",
      "Train Epoch: 4 [51200/71432 (72%)]\tLoss: 0.520342\n",
      "Train Epoch: 4 [57600/71432 (81%)]\tLoss: 0.480371\n",
      "Train Epoch: 4 [64000/71432 (90%)]\tLoss: 0.490751\n",
      "Train Epoch: 4 [70400/71432 (98%)]\tLoss: 0.550843\n",
      "\n",
      "Test set: Average loss: 0.0080, Accuracy: 6265/8141 (77%), Positive accuracy: 3078/4091 (75%), Negative accuracy: 3187/4050 (79%), train loss: 0.0087\n",
      "\n",
      "Train Epoch: 5 [0/71432 (0%)]\tLoss: 0.552764\n",
      "Train Epoch: 5 [6400/71432 (9%)]\tLoss: 0.512569\n",
      "Train Epoch: 5 [12800/71432 (18%)]\tLoss: 0.512499\n",
      "Train Epoch: 5 [19200/71432 (27%)]\tLoss: 0.518626\n",
      "Train Epoch: 5 [25600/71432 (36%)]\tLoss: 0.452492\n",
      "Train Epoch: 5 [32000/71432 (45%)]\tLoss: 0.543640\n",
      "Train Epoch: 5 [38400/71432 (54%)]\tLoss: 0.516661\n",
      "Train Epoch: 5 [44800/71432 (63%)]\tLoss: 0.534427\n",
      "Train Epoch: 5 [51200/71432 (72%)]\tLoss: 0.558885\n",
      "Train Epoch: 5 [57600/71432 (81%)]\tLoss: 0.426063\n",
      "Train Epoch: 5 [64000/71432 (90%)]\tLoss: 0.407262\n",
      "Train Epoch: 5 [70400/71432 (98%)]\tLoss: 0.411160\n",
      "\n",
      "Test set: Average loss: 0.0073, Accuracy: 6418/8141 (79%), Positive accuracy: 3320/4091 (81%), Negative accuracy: 3098/4050 (76%), train loss: 0.0076\n",
      "\n",
      "Train Epoch: 6 [0/71432 (0%)]\tLoss: 0.450219\n",
      "Train Epoch: 6 [6400/71432 (9%)]\tLoss: 0.553309\n",
      "Train Epoch: 6 [12800/71432 (18%)]\tLoss: 0.544657\n",
      "Train Epoch: 6 [19200/71432 (27%)]\tLoss: 0.455420\n",
      "Train Epoch: 6 [25600/71432 (36%)]\tLoss: 0.528471\n",
      "Train Epoch: 6 [32000/71432 (45%)]\tLoss: 0.439564\n",
      "Train Epoch: 6 [38400/71432 (54%)]\tLoss: 0.391841\n",
      "Train Epoch: 6 [44800/71432 (63%)]\tLoss: 0.472963\n",
      "Train Epoch: 6 [51200/71432 (72%)]\tLoss: 0.453825\n",
      "Train Epoch: 6 [57600/71432 (81%)]\tLoss: 0.474383\n",
      "Train Epoch: 6 [64000/71432 (90%)]\tLoss: 0.340957\n",
      "Train Epoch: 6 [70400/71432 (98%)]\tLoss: 0.355518\n",
      "\n",
      "Test set: Average loss: 0.0071, Accuracy: 6457/8141 (79%), Positive accuracy: 3253/4091 (80%), Negative accuracy: 3204/4050 (79%), train loss: 0.0072\n",
      "\n",
      "Train Epoch: 7 [0/71432 (0%)]\tLoss: 0.352828\n",
      "Train Epoch: 7 [6400/71432 (9%)]\tLoss: 0.538994\n",
      "Train Epoch: 7 [12800/71432 (18%)]\tLoss: 0.382077\n",
      "Train Epoch: 7 [19200/71432 (27%)]\tLoss: 0.487196\n",
      "Train Epoch: 7 [25600/71432 (36%)]\tLoss: 0.471112\n",
      "Train Epoch: 7 [32000/71432 (45%)]\tLoss: 0.421243\n",
      "Train Epoch: 7 [38400/71432 (54%)]\tLoss: 0.365360\n",
      "Train Epoch: 7 [44800/71432 (63%)]\tLoss: 0.389224\n",
      "Train Epoch: 7 [51200/71432 (72%)]\tLoss: 0.506142\n",
      "Train Epoch: 7 [57600/71432 (81%)]\tLoss: 0.450201\n",
      "Train Epoch: 7 [64000/71432 (90%)]\tLoss: 0.391664\n",
      "Train Epoch: 7 [70400/71432 (98%)]\tLoss: 0.461931\n",
      "\n",
      "Test set: Average loss: 0.0069, Accuracy: 6549/8141 (80%), Positive accuracy: 3434/4091 (84%), Negative accuracy: 3115/4050 (77%), train loss: 0.0070\n",
      "\n",
      "Train Epoch: 8 [0/71432 (0%)]\tLoss: 0.409221\n",
      "Train Epoch: 8 [6400/71432 (9%)]\tLoss: 0.374800\n",
      "Train Epoch: 8 [12800/71432 (18%)]\tLoss: 0.432490\n",
      "Train Epoch: 8 [19200/71432 (27%)]\tLoss: 0.418960\n",
      "Train Epoch: 8 [25600/71432 (36%)]\tLoss: 0.506214\n",
      "Train Epoch: 8 [32000/71432 (45%)]\tLoss: 0.469575\n",
      "Train Epoch: 8 [38400/71432 (54%)]\tLoss: 0.330836\n",
      "Train Epoch: 8 [44800/71432 (63%)]\tLoss: 0.526140\n",
      "Train Epoch: 8 [51200/71432 (72%)]\tLoss: 0.440862\n",
      "Train Epoch: 8 [57600/71432 (81%)]\tLoss: 0.381136\n",
      "Train Epoch: 8 [64000/71432 (90%)]\tLoss: 0.408723\n",
      "Train Epoch: 8 [70400/71432 (98%)]\tLoss: 0.463670\n",
      "\n",
      "Test set: Average loss: 0.0068, Accuracy: 6590/8141 (81%), Positive accuracy: 3475/4091 (85%), Negative accuracy: 3115/4050 (77%), train loss: 0.0069\n",
      "\n",
      "Train Epoch: 9 [0/71432 (0%)]\tLoss: 0.435722\n",
      "Train Epoch: 9 [6400/71432 (9%)]\tLoss: 0.405827\n",
      "Train Epoch: 9 [12800/71432 (18%)]\tLoss: 0.474962\n",
      "Train Epoch: 9 [19200/71432 (27%)]\tLoss: 0.440640\n",
      "Train Epoch: 9 [25600/71432 (36%)]\tLoss: 0.485707\n",
      "Train Epoch: 9 [32000/71432 (45%)]\tLoss: 0.361588\n",
      "Train Epoch: 9 [38400/71432 (54%)]\tLoss: 0.370880\n",
      "Train Epoch: 9 [44800/71432 (63%)]\tLoss: 0.434554\n",
      "Train Epoch: 9 [51200/71432 (72%)]\tLoss: 0.434667\n",
      "Train Epoch: 9 [57600/71432 (81%)]\tLoss: 0.449130\n",
      "Train Epoch: 9 [64000/71432 (90%)]\tLoss: 0.464721\n",
      "Train Epoch: 9 [70400/71432 (98%)]\tLoss: 0.419477\n",
      "\n",
      "Test set: Average loss: 0.0067, Accuracy: 6606/8141 (81%), Positive accuracy: 3402/4091 (83%), Negative accuracy: 3204/4050 (79%), train loss: 0.0068\n",
      "\n",
      "Train Epoch: 10 [0/71432 (0%)]\tLoss: 0.393229\n",
      "Train Epoch: 10 [6400/71432 (9%)]\tLoss: 0.429534\n",
      "Train Epoch: 10 [12800/71432 (18%)]\tLoss: 0.395975\n",
      "Train Epoch: 10 [19200/71432 (27%)]\tLoss: 0.416401\n",
      "Train Epoch: 10 [25600/71432 (36%)]\tLoss: 0.412250\n",
      "Train Epoch: 10 [32000/71432 (45%)]\tLoss: 0.496856\n",
      "Train Epoch: 10 [38400/71432 (54%)]\tLoss: 0.443129\n",
      "Train Epoch: 10 [44800/71432 (63%)]\tLoss: 0.414973\n",
      "Train Epoch: 10 [51200/71432 (72%)]\tLoss: 0.421606\n",
      "Train Epoch: 10 [57600/71432 (81%)]\tLoss: 0.440561\n",
      "Train Epoch: 10 [64000/71432 (90%)]\tLoss: 0.401994\n",
      "Train Epoch: 10 [70400/71432 (98%)]\tLoss: 0.406340\n",
      "\n",
      "Test set: Average loss: 0.0066, Accuracy: 6670/8141 (82%), Positive accuracy: 3491/4091 (85%), Negative accuracy: 3179/4050 (78%), train loss: 0.0067\n",
      "\n",
      "Train Epoch: 11 [0/71432 (0%)]\tLoss: 0.561800\n",
      "Train Epoch: 11 [6400/71432 (9%)]\tLoss: 0.340209\n",
      "Train Epoch: 11 [12800/71432 (18%)]\tLoss: 0.379362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [19200/71432 (27%)]\tLoss: 0.394033\n",
      "Train Epoch: 11 [25600/71432 (36%)]\tLoss: 0.466762\n",
      "Train Epoch: 11 [32000/71432 (45%)]\tLoss: 0.452079\n",
      "Train Epoch: 11 [38400/71432 (54%)]\tLoss: 0.624554\n",
      "Train Epoch: 11 [44800/71432 (63%)]\tLoss: 0.383826\n",
      "Train Epoch: 11 [51200/71432 (72%)]\tLoss: 0.470626\n",
      "Train Epoch: 11 [57600/71432 (81%)]\tLoss: 0.416428\n",
      "Train Epoch: 11 [64000/71432 (90%)]\tLoss: 0.376878\n",
      "Train Epoch: 11 [70400/71432 (98%)]\tLoss: 0.448155\n",
      "\n",
      "Test set: Average loss: 0.0065, Accuracy: 6684/8141 (82%), Positive accuracy: 3462/4091 (85%), Negative accuracy: 3222/4050 (80%), train loss: 0.0066\n",
      "\n",
      "Train Epoch: 12 [0/71432 (0%)]\tLoss: 0.433192\n",
      "Train Epoch: 12 [6400/71432 (9%)]\tLoss: 0.392715\n",
      "Train Epoch: 12 [12800/71432 (18%)]\tLoss: 0.408202\n",
      "Train Epoch: 12 [19200/71432 (27%)]\tLoss: 0.433478\n",
      "Train Epoch: 12 [25600/71432 (36%)]\tLoss: 0.436578\n",
      "Train Epoch: 12 [32000/71432 (45%)]\tLoss: 0.371055\n",
      "Train Epoch: 12 [38400/71432 (54%)]\tLoss: 0.458727\n",
      "Train Epoch: 12 [44800/71432 (63%)]\tLoss: 0.386521\n",
      "Train Epoch: 12 [51200/71432 (72%)]\tLoss: 0.335913\n",
      "Train Epoch: 12 [57600/71432 (81%)]\tLoss: 0.420838\n",
      "Train Epoch: 12 [64000/71432 (90%)]\tLoss: 0.356629\n",
      "Train Epoch: 12 [70400/71432 (98%)]\tLoss: 0.380847\n",
      "\n",
      "Test set: Average loss: 0.0065, Accuracy: 6707/8141 (82%), Positive accuracy: 3496/4091 (85%), Negative accuracy: 3211/4050 (79%), train loss: 0.0066\n",
      "\n",
      "Train Epoch: 13 [0/71432 (0%)]\tLoss: 0.538747\n",
      "Train Epoch: 13 [6400/71432 (9%)]\tLoss: 0.387141\n",
      "Train Epoch: 13 [12800/71432 (18%)]\tLoss: 0.467291\n",
      "Train Epoch: 13 [19200/71432 (27%)]\tLoss: 0.349066\n",
      "Train Epoch: 13 [25600/71432 (36%)]\tLoss: 0.299836\n",
      "Train Epoch: 13 [32000/71432 (45%)]\tLoss: 0.481555\n",
      "Train Epoch: 13 [38400/71432 (54%)]\tLoss: 0.362610\n",
      "Train Epoch: 13 [44800/71432 (63%)]\tLoss: 0.474360\n",
      "Train Epoch: 13 [51200/71432 (72%)]\tLoss: 0.421506\n",
      "Train Epoch: 13 [57600/71432 (81%)]\tLoss: 0.356839\n",
      "Train Epoch: 13 [64000/71432 (90%)]\tLoss: 0.466674\n",
      "Train Epoch: 13 [70400/71432 (98%)]\tLoss: 0.428187\n",
      "\n",
      "Test set: Average loss: 0.0064, Accuracy: 6710/8141 (82%), Positive accuracy: 3469/4091 (85%), Negative accuracy: 3241/4050 (80%), train loss: 0.0065\n",
      "\n",
      "Train Epoch: 14 [0/71432 (0%)]\tLoss: 0.538733\n",
      "Train Epoch: 14 [6400/71432 (9%)]\tLoss: 0.400533\n",
      "Train Epoch: 14 [12800/71432 (18%)]\tLoss: 0.378380\n",
      "Train Epoch: 14 [19200/71432 (27%)]\tLoss: 0.544810\n",
      "Train Epoch: 14 [25600/71432 (36%)]\tLoss: 0.388021\n",
      "Train Epoch: 14 [32000/71432 (45%)]\tLoss: 0.424953\n",
      "Train Epoch: 14 [38400/71432 (54%)]\tLoss: 0.340575\n",
      "Train Epoch: 14 [44800/71432 (63%)]\tLoss: 0.329183\n",
      "Train Epoch: 14 [51200/71432 (72%)]\tLoss: 0.453839\n",
      "Train Epoch: 14 [57600/71432 (81%)]\tLoss: 0.346963\n",
      "Train Epoch: 14 [64000/71432 (90%)]\tLoss: 0.370158\n",
      "Train Epoch: 14 [70400/71432 (98%)]\tLoss: 0.362398\n",
      "\n",
      "Test set: Average loss: 0.0064, Accuracy: 6763/8141 (83%), Positive accuracy: 3557/4091 (87%), Negative accuracy: 3206/4050 (79%), train loss: 0.0065\n",
      "\n",
      "Train Epoch: 15 [0/71432 (0%)]\tLoss: 0.350683\n",
      "Train Epoch: 15 [6400/71432 (9%)]\tLoss: 0.312510\n",
      "Train Epoch: 15 [12800/71432 (18%)]\tLoss: 0.456139\n",
      "Train Epoch: 15 [19200/71432 (27%)]\tLoss: 0.449229\n",
      "Train Epoch: 15 [25600/71432 (36%)]\tLoss: 0.358992\n",
      "Train Epoch: 15 [32000/71432 (45%)]\tLoss: 0.329707\n",
      "Train Epoch: 15 [38400/71432 (54%)]\tLoss: 0.370016\n",
      "Train Epoch: 15 [44800/71432 (63%)]\tLoss: 0.352087\n",
      "Train Epoch: 15 [51200/71432 (72%)]\tLoss: 0.351312\n",
      "Train Epoch: 15 [57600/71432 (81%)]\tLoss: 0.445128\n",
      "Train Epoch: 15 [64000/71432 (90%)]\tLoss: 0.423820\n",
      "Train Epoch: 15 [70400/71432 (98%)]\tLoss: 0.348893\n",
      "\n",
      "Test set: Average loss: 0.0064, Accuracy: 6698/8141 (82%), Positive accuracy: 3406/4091 (83%), Negative accuracy: 3292/4050 (81%), train loss: 0.0065\n",
      "\n",
      "Train Epoch: 16 [0/71432 (0%)]\tLoss: 0.381146\n",
      "Train Epoch: 16 [6400/71432 (9%)]\tLoss: 0.337602\n",
      "Train Epoch: 16 [12800/71432 (18%)]\tLoss: 0.522027\n",
      "Train Epoch: 16 [19200/71432 (27%)]\tLoss: 0.337408\n",
      "Train Epoch: 16 [25600/71432 (36%)]\tLoss: 0.518609\n",
      "Train Epoch: 16 [32000/71432 (45%)]\tLoss: 0.440880\n",
      "Train Epoch: 16 [38400/71432 (54%)]\tLoss: 0.356617\n",
      "Train Epoch: 16 [44800/71432 (63%)]\tLoss: 0.379686\n",
      "Train Epoch: 16 [51200/71432 (72%)]\tLoss: 0.466677\n",
      "Train Epoch: 16 [57600/71432 (81%)]\tLoss: 0.328475\n",
      "Train Epoch: 16 [64000/71432 (90%)]\tLoss: 0.618041\n",
      "Train Epoch: 16 [70400/71432 (98%)]\tLoss: 0.235201\n",
      "\n",
      "Test set: Average loss: 0.0064, Accuracy: 6729/8141 (83%), Positive accuracy: 3415/4091 (83%), Negative accuracy: 3314/4050 (82%), train loss: 0.0064\n",
      "\n",
      "Train Epoch: 17 [0/71432 (0%)]\tLoss: 0.479029\n",
      "Train Epoch: 17 [6400/71432 (9%)]\tLoss: 0.338586\n",
      "Train Epoch: 17 [12800/71432 (18%)]\tLoss: 0.492324\n",
      "Train Epoch: 17 [19200/71432 (27%)]\tLoss: 0.382870\n",
      "Train Epoch: 17 [25600/71432 (36%)]\tLoss: 0.337713\n",
      "Train Epoch: 17 [32000/71432 (45%)]\tLoss: 0.437106\n",
      "Train Epoch: 17 [38400/71432 (54%)]\tLoss: 0.378422\n",
      "Train Epoch: 17 [44800/71432 (63%)]\tLoss: 0.505383\n",
      "Train Epoch: 17 [51200/71432 (72%)]\tLoss: 0.364321\n",
      "Train Epoch: 17 [57600/71432 (81%)]\tLoss: 0.513683\n",
      "Train Epoch: 17 [64000/71432 (90%)]\tLoss: 0.454271\n",
      "Train Epoch: 17 [70400/71432 (98%)]\tLoss: 0.424504\n",
      "\n",
      "Test set: Average loss: 0.0063, Accuracy: 6766/8141 (83%), Positive accuracy: 3540/4091 (87%), Negative accuracy: 3226/4050 (80%), train loss: 0.0064\n",
      "\n",
      "Train Epoch: 18 [0/71432 (0%)]\tLoss: 0.432917\n",
      "Train Epoch: 18 [6400/71432 (9%)]\tLoss: 0.374692\n",
      "Train Epoch: 18 [12800/71432 (18%)]\tLoss: 0.503150\n",
      "Train Epoch: 18 [19200/71432 (27%)]\tLoss: 0.386286\n",
      "Train Epoch: 18 [25600/71432 (36%)]\tLoss: 0.339118\n",
      "Train Epoch: 18 [32000/71432 (45%)]\tLoss: 0.374915\n",
      "Train Epoch: 18 [38400/71432 (54%)]\tLoss: 0.612099\n",
      "Train Epoch: 18 [44800/71432 (63%)]\tLoss: 0.374360\n",
      "Train Epoch: 18 [51200/71432 (72%)]\tLoss: 0.335557\n",
      "Train Epoch: 18 [57600/71432 (81%)]\tLoss: 0.370985\n",
      "Train Epoch: 18 [64000/71432 (90%)]\tLoss: 0.303539\n",
      "Train Epoch: 18 [70400/71432 (98%)]\tLoss: 0.427745\n",
      "\n",
      "Test set: Average loss: 0.0063, Accuracy: 6789/8141 (83%), Positive accuracy: 3600/4091 (88%), Negative accuracy: 3189/4050 (79%), train loss: 0.0064\n",
      "\n",
      "Train Epoch: 19 [0/71432 (0%)]\tLoss: 0.430269\n",
      "Train Epoch: 19 [6400/71432 (9%)]\tLoss: 0.426749\n",
      "Train Epoch: 19 [12800/71432 (18%)]\tLoss: 0.265942\n",
      "Train Epoch: 19 [19200/71432 (27%)]\tLoss: 0.329898\n",
      "Train Epoch: 19 [25600/71432 (36%)]\tLoss: 0.445053\n",
      "Train Epoch: 19 [32000/71432 (45%)]\tLoss: 0.392970\n",
      "Train Epoch: 19 [38400/71432 (54%)]\tLoss: 0.413881\n",
      "Train Epoch: 19 [44800/71432 (63%)]\tLoss: 0.468637\n",
      "Train Epoch: 19 [51200/71432 (72%)]\tLoss: 0.474888\n",
      "Train Epoch: 19 [57600/71432 (81%)]\tLoss: 0.448916\n",
      "Train Epoch: 19 [64000/71432 (90%)]\tLoss: 0.382577\n",
      "Train Epoch: 19 [70400/71432 (98%)]\tLoss: 0.457290\n",
      "\n",
      "Test set: Average loss: 0.0063, Accuracy: 6819/8141 (84%), Positive accuracy: 3566/4091 (87%), Negative accuracy: 3253/4050 (80%), train loss: 0.0064\n",
      "\n",
      "Train Epoch: 20 [0/71432 (0%)]\tLoss: 0.457136\n",
      "Train Epoch: 20 [6400/71432 (9%)]\tLoss: 0.428007\n",
      "Train Epoch: 20 [12800/71432 (18%)]\tLoss: 0.318975\n",
      "Train Epoch: 20 [19200/71432 (27%)]\tLoss: 0.431342\n",
      "Train Epoch: 20 [25600/71432 (36%)]\tLoss: 0.428093\n",
      "Train Epoch: 20 [32000/71432 (45%)]\tLoss: 0.387547\n",
      "Train Epoch: 20 [38400/71432 (54%)]\tLoss: 0.297799\n",
      "Train Epoch: 20 [44800/71432 (63%)]\tLoss: 0.533627\n",
      "Train Epoch: 20 [51200/71432 (72%)]\tLoss: 0.440057\n",
      "Train Epoch: 20 [57600/71432 (81%)]\tLoss: 0.418510\n",
      "Train Epoch: 20 [64000/71432 (90%)]\tLoss: 0.563926\n",
      "Train Epoch: 20 [70400/71432 (98%)]\tLoss: 0.426487\n",
      "\n",
      "Test set: Average loss: 0.0063, Accuracy: 6807/8141 (84%), Positive accuracy: 3592/4091 (88%), Negative accuracy: 3215/4050 (79%), train loss: 0.0063\n",
      "\n",
      "Train Epoch: 21 [0/71432 (0%)]\tLoss: 0.442002\n",
      "Train Epoch: 21 [6400/71432 (9%)]\tLoss: 0.281028\n",
      "Train Epoch: 21 [12800/71432 (18%)]\tLoss: 0.405717\n",
      "Train Epoch: 21 [19200/71432 (27%)]\tLoss: 0.437284\n",
      "Train Epoch: 21 [25600/71432 (36%)]\tLoss: 0.403011\n",
      "Train Epoch: 21 [32000/71432 (45%)]\tLoss: 0.374724\n",
      "Train Epoch: 21 [38400/71432 (54%)]\tLoss: 0.416740\n",
      "Train Epoch: 21 [44800/71432 (63%)]\tLoss: 0.570190\n",
      "Train Epoch: 21 [51200/71432 (72%)]\tLoss: 0.340278\n",
      "Train Epoch: 21 [57600/71432 (81%)]\tLoss: 0.262439\n",
      "Train Epoch: 21 [64000/71432 (90%)]\tLoss: 0.382439\n",
      "Train Epoch: 21 [70400/71432 (98%)]\tLoss: 0.277092\n",
      "\n",
      "Test set: Average loss: 0.0063, Accuracy: 6817/8141 (84%), Positive accuracy: 3583/4091 (88%), Negative accuracy: 3234/4050 (80%), train loss: 0.0063\n",
      "\n",
      "Train Epoch: 22 [0/71432 (0%)]\tLoss: 0.427476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 22 [6400/71432 (9%)]\tLoss: 0.448700\n",
      "Train Epoch: 22 [12800/71432 (18%)]\tLoss: 0.372086\n",
      "Train Epoch: 22 [19200/71432 (27%)]\tLoss: 0.326988\n",
      "Train Epoch: 22 [25600/71432 (36%)]\tLoss: 0.336034\n",
      "Train Epoch: 22 [32000/71432 (45%)]\tLoss: 0.482169\n",
      "Train Epoch: 22 [38400/71432 (54%)]\tLoss: 0.498609\n",
      "Train Epoch: 22 [44800/71432 (63%)]\tLoss: 0.431225\n",
      "Train Epoch: 22 [51200/71432 (72%)]\tLoss: 0.329863\n",
      "Train Epoch: 22 [57600/71432 (81%)]\tLoss: 0.393985\n",
      "Train Epoch: 22 [64000/71432 (90%)]\tLoss: 0.396693\n",
      "Train Epoch: 22 [70400/71432 (98%)]\tLoss: 0.358926\n",
      "\n",
      "Test set: Average loss: 0.0063, Accuracy: 6802/8141 (84%), Positive accuracy: 3550/4091 (87%), Negative accuracy: 3252/4050 (80%), train loss: 0.0063\n",
      "\n",
      "Train Epoch: 23 [0/71432 (0%)]\tLoss: 0.370394\n",
      "Train Epoch: 23 [6400/71432 (9%)]\tLoss: 0.407460\n",
      "Train Epoch: 23 [12800/71432 (18%)]\tLoss: 0.483956\n",
      "Train Epoch: 23 [19200/71432 (27%)]\tLoss: 0.515624\n",
      "Train Epoch: 23 [25600/71432 (36%)]\tLoss: 0.416597\n",
      "Train Epoch: 23 [32000/71432 (45%)]\tLoss: 0.399737\n",
      "Train Epoch: 23 [38400/71432 (54%)]\tLoss: 0.465030\n",
      "Train Epoch: 23 [44800/71432 (63%)]\tLoss: 0.451728\n",
      "Train Epoch: 23 [51200/71432 (72%)]\tLoss: 0.455350\n",
      "Train Epoch: 23 [57600/71432 (81%)]\tLoss: 0.340827\n",
      "Train Epoch: 23 [64000/71432 (90%)]\tLoss: 0.378733\n",
      "Train Epoch: 23 [70400/71432 (98%)]\tLoss: 0.408346\n",
      "\n",
      "Test set: Average loss: 0.0062, Accuracy: 6762/8141 (83%), Positive accuracy: 3448/4091 (84%), Negative accuracy: 3314/4050 (82%), train loss: 0.0063\n",
      "\n",
      "Train Epoch: 24 [0/71432 (0%)]\tLoss: 0.280041\n",
      "Train Epoch: 24 [6400/71432 (9%)]\tLoss: 0.329900\n",
      "Train Epoch: 24 [12800/71432 (18%)]\tLoss: 0.543537\n",
      "Train Epoch: 24 [19200/71432 (27%)]\tLoss: 0.426998\n",
      "Train Epoch: 24 [25600/71432 (36%)]\tLoss: 0.466793\n",
      "Train Epoch: 24 [32000/71432 (45%)]\tLoss: 0.412118\n",
      "Train Epoch: 24 [38400/71432 (54%)]\tLoss: 0.349626\n",
      "Train Epoch: 24 [44800/71432 (63%)]\tLoss: 0.432359\n",
      "Train Epoch: 24 [51200/71432 (72%)]\tLoss: 0.435155\n",
      "Train Epoch: 24 [57600/71432 (81%)]\tLoss: 0.348240\n",
      "Train Epoch: 24 [64000/71432 (90%)]\tLoss: 0.400277\n",
      "Train Epoch: 24 [70400/71432 (98%)]\tLoss: 0.341286\n",
      "\n",
      "Test set: Average loss: 0.0062, Accuracy: 6804/8141 (84%), Positive accuracy: 3487/4091 (85%), Negative accuracy: 3317/4050 (82%), train loss: 0.0063\n",
      "\n",
      "Train Epoch: 25 [0/71432 (0%)]\tLoss: 0.257208\n",
      "Train Epoch: 25 [6400/71432 (9%)]\tLoss: 0.258283\n",
      "Train Epoch: 25 [12800/71432 (18%)]\tLoss: 0.471973\n",
      "Train Epoch: 25 [19200/71432 (27%)]\tLoss: 0.402492\n",
      "Train Epoch: 25 [25600/71432 (36%)]\tLoss: 0.540120\n",
      "Train Epoch: 25 [32000/71432 (45%)]\tLoss: 0.485259\n",
      "Train Epoch: 25 [38400/71432 (54%)]\tLoss: 0.407544\n",
      "Train Epoch: 25 [44800/71432 (63%)]\tLoss: 0.370354\n",
      "Train Epoch: 25 [51200/71432 (72%)]\tLoss: 0.459610\n",
      "Train Epoch: 25 [57600/71432 (81%)]\tLoss: 0.438736\n",
      "Train Epoch: 25 [64000/71432 (90%)]\tLoss: 0.473990\n",
      "Train Epoch: 25 [70400/71432 (98%)]\tLoss: 0.346531\n",
      "\n",
      "Test set: Average loss: 0.0062, Accuracy: 6857/8141 (84%), Positive accuracy: 3608/4091 (88%), Negative accuracy: 3249/4050 (80%), train loss: 0.0063\n",
      "\n",
      "Train Epoch: 26 [0/71432 (0%)]\tLoss: 0.432923\n",
      "Train Epoch: 26 [6400/71432 (9%)]\tLoss: 0.328848\n",
      "Train Epoch: 26 [12800/71432 (18%)]\tLoss: 0.420194\n",
      "Train Epoch: 26 [19200/71432 (27%)]\tLoss: 0.366326\n",
      "Train Epoch: 26 [25600/71432 (36%)]\tLoss: 0.320146\n",
      "Train Epoch: 26 [32000/71432 (45%)]\tLoss: 0.368724\n",
      "Train Epoch: 26 [38400/71432 (54%)]\tLoss: 0.368944\n",
      "Train Epoch: 26 [44800/71432 (63%)]\tLoss: 0.324174\n",
      "Train Epoch: 26 [51200/71432 (72%)]\tLoss: 0.544136\n",
      "Train Epoch: 26 [57600/71432 (81%)]\tLoss: 0.259638\n",
      "Train Epoch: 26 [64000/71432 (90%)]\tLoss: 0.387508\n",
      "Train Epoch: 26 [70400/71432 (98%)]\tLoss: 0.436736\n",
      "\n",
      "Test set: Average loss: 0.0062, Accuracy: 6799/8141 (84%), Positive accuracy: 3508/4091 (86%), Negative accuracy: 3291/4050 (81%), train loss: 0.0063\n",
      "\n",
      "Train Epoch: 27 [0/71432 (0%)]\tLoss: 0.525371\n",
      "Train Epoch: 27 [6400/71432 (9%)]\tLoss: 0.464128\n",
      "Train Epoch: 27 [12800/71432 (18%)]\tLoss: 0.347052\n",
      "Train Epoch: 27 [19200/71432 (27%)]\tLoss: 0.318613\n",
      "Train Epoch: 27 [25600/71432 (36%)]\tLoss: 0.405948\n",
      "Train Epoch: 27 [32000/71432 (45%)]\tLoss: 0.347749\n",
      "Train Epoch: 27 [38400/71432 (54%)]\tLoss: 0.375938\n",
      "Train Epoch: 27 [44800/71432 (63%)]\tLoss: 0.414788\n",
      "Train Epoch: 27 [51200/71432 (72%)]\tLoss: 0.311511\n",
      "Train Epoch: 27 [57600/71432 (81%)]\tLoss: 0.447487\n",
      "Train Epoch: 27 [64000/71432 (90%)]\tLoss: 0.499963\n",
      "Train Epoch: 27 [70400/71432 (98%)]\tLoss: 0.456182\n",
      "\n",
      "Test set: Average loss: 0.0062, Accuracy: 6847/8141 (84%), Positive accuracy: 3567/4091 (87%), Negative accuracy: 3280/4050 (81%), train loss: 0.0062\n",
      "\n",
      "Train Epoch: 28 [0/71432 (0%)]\tLoss: 0.457540\n",
      "Train Epoch: 28 [6400/71432 (9%)]\tLoss: 0.371201\n",
      "Train Epoch: 28 [12800/71432 (18%)]\tLoss: 0.411940\n",
      "Train Epoch: 28 [19200/71432 (27%)]\tLoss: 0.497454\n",
      "Train Epoch: 28 [25600/71432 (36%)]\tLoss: 0.243400\n",
      "Train Epoch: 28 [32000/71432 (45%)]\tLoss: 0.389865\n",
      "Train Epoch: 28 [38400/71432 (54%)]\tLoss: 0.576132\n",
      "Train Epoch: 28 [44800/71432 (63%)]\tLoss: 0.351606\n",
      "Train Epoch: 28 [51200/71432 (72%)]\tLoss: 0.285739\n",
      "Train Epoch: 28 [57600/71432 (81%)]\tLoss: 0.423800\n",
      "Train Epoch: 28 [64000/71432 (90%)]\tLoss: 0.486410\n",
      "Train Epoch: 28 [70400/71432 (98%)]\tLoss: 0.439903\n",
      "\n",
      "Test set: Average loss: 0.0062, Accuracy: 6826/8141 (84%), Positive accuracy: 3536/4091 (86%), Negative accuracy: 3290/4050 (81%), train loss: 0.0062\n",
      "\n",
      "Train Epoch: 29 [0/71432 (0%)]\tLoss: 0.418210\n",
      "Train Epoch: 29 [6400/71432 (9%)]\tLoss: 0.502591\n",
      "Train Epoch: 29 [12800/71432 (18%)]\tLoss: 0.376279\n",
      "Train Epoch: 29 [19200/71432 (27%)]\tLoss: 0.364057\n",
      "Train Epoch: 29 [25600/71432 (36%)]\tLoss: 0.354713\n",
      "Train Epoch: 29 [32000/71432 (45%)]\tLoss: 0.351896\n",
      "Train Epoch: 29 [38400/71432 (54%)]\tLoss: 0.376621\n",
      "Train Epoch: 29 [44800/71432 (63%)]\tLoss: 0.372350\n",
      "Train Epoch: 29 [51200/71432 (72%)]\tLoss: 0.303057\n",
      "Train Epoch: 29 [57600/71432 (81%)]\tLoss: 0.450794\n",
      "Train Epoch: 29 [64000/71432 (90%)]\tLoss: 0.308254\n",
      "Train Epoch: 29 [70400/71432 (98%)]\tLoss: 0.416782\n",
      "\n",
      "Test set: Average loss: 0.0062, Accuracy: 6769/8141 (83%), Positive accuracy: 3354/4091 (82%), Negative accuracy: 3415/4050 (84%), train loss: 0.0062\n",
      "\n",
      "Train Epoch: 30 [0/71432 (0%)]\tLoss: 0.376915\n",
      "Train Epoch: 30 [6400/71432 (9%)]\tLoss: 0.352957\n",
      "Train Epoch: 30 [12800/71432 (18%)]\tLoss: 0.382746\n",
      "Train Epoch: 30 [19200/71432 (27%)]\tLoss: 0.389581\n",
      "Train Epoch: 30 [25600/71432 (36%)]\tLoss: 0.354689\n",
      "Train Epoch: 30 [32000/71432 (45%)]\tLoss: 0.532771\n",
      "Train Epoch: 30 [38400/71432 (54%)]\tLoss: 0.452306\n",
      "Train Epoch: 30 [44800/71432 (63%)]\tLoss: 0.461135\n",
      "Train Epoch: 30 [51200/71432 (72%)]\tLoss: 0.427817\n",
      "Train Epoch: 30 [57600/71432 (81%)]\tLoss: 0.421180\n",
      "Train Epoch: 30 [64000/71432 (90%)]\tLoss: 0.450736\n",
      "Train Epoch: 30 [70400/71432 (98%)]\tLoss: 0.386238\n",
      "\n",
      "Test set: Average loss: 0.0061, Accuracy: 6851/8141 (84%), Positive accuracy: 3578/4091 (87%), Negative accuracy: 3273/4050 (81%), train loss: 0.0062\n",
      "\n",
      "Train Epoch: 31 [0/71432 (0%)]\tLoss: 0.388034\n",
      "Train Epoch: 31 [6400/71432 (9%)]\tLoss: 0.440501\n",
      "Train Epoch: 31 [12800/71432 (18%)]\tLoss: 0.416863\n",
      "Train Epoch: 31 [19200/71432 (27%)]\tLoss: 0.458617\n",
      "Train Epoch: 31 [25600/71432 (36%)]\tLoss: 0.483396\n",
      "Train Epoch: 31 [32000/71432 (45%)]\tLoss: 0.377253\n",
      "Train Epoch: 31 [38400/71432 (54%)]\tLoss: 0.384392\n",
      "Train Epoch: 31 [44800/71432 (63%)]\tLoss: 0.491824\n",
      "Train Epoch: 31 [51200/71432 (72%)]\tLoss: 0.475966\n",
      "Train Epoch: 31 [57600/71432 (81%)]\tLoss: 0.343729\n",
      "Train Epoch: 31 [64000/71432 (90%)]\tLoss: 0.323800\n",
      "Train Epoch: 31 [70400/71432 (98%)]\tLoss: 0.413098\n",
      "\n",
      "Test set: Average loss: 0.0061, Accuracy: 6871/8141 (84%), Positive accuracy: 3608/4091 (88%), Negative accuracy: 3263/4050 (81%), train loss: 0.0062\n",
      "\n",
      "Train Epoch: 32 [0/71432 (0%)]\tLoss: 0.412428\n",
      "Train Epoch: 32 [6400/71432 (9%)]\tLoss: 0.324726\n",
      "Train Epoch: 32 [12800/71432 (18%)]\tLoss: 0.299728\n",
      "Train Epoch: 32 [19200/71432 (27%)]\tLoss: 0.435255\n",
      "Train Epoch: 32 [25600/71432 (36%)]\tLoss: 0.411370\n",
      "Train Epoch: 32 [32000/71432 (45%)]\tLoss: 0.329249\n",
      "Train Epoch: 32 [38400/71432 (54%)]\tLoss: 0.328122\n",
      "Train Epoch: 32 [44800/71432 (63%)]\tLoss: 0.533087\n",
      "Train Epoch: 32 [51200/71432 (72%)]\tLoss: 0.494794\n",
      "Train Epoch: 32 [57600/71432 (81%)]\tLoss: 0.523529\n",
      "Train Epoch: 32 [64000/71432 (90%)]\tLoss: 0.382760\n",
      "Train Epoch: 32 [70400/71432 (98%)]\tLoss: 0.321847\n",
      "\n",
      "Test set: Average loss: 0.0061, Accuracy: 6851/8141 (84%), Positive accuracy: 3521/4091 (86%), Negative accuracy: 3330/4050 (82%), train loss: 0.0062\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 33 [0/71432 (0%)]\tLoss: 0.342659\n",
      "Train Epoch: 33 [6400/71432 (9%)]\tLoss: 0.379568\n",
      "Train Epoch: 33 [12800/71432 (18%)]\tLoss: 0.350221\n",
      "Train Epoch: 33 [19200/71432 (27%)]\tLoss: 0.445361\n",
      "Train Epoch: 33 [25600/71432 (36%)]\tLoss: 0.373065\n",
      "Train Epoch: 33 [32000/71432 (45%)]\tLoss: 0.418699\n",
      "Train Epoch: 33 [38400/71432 (54%)]\tLoss: 0.454190\n",
      "Train Epoch: 33 [44800/71432 (63%)]\tLoss: 0.373388\n",
      "Train Epoch: 33 [51200/71432 (72%)]\tLoss: 0.317645\n",
      "Train Epoch: 33 [57600/71432 (81%)]\tLoss: 0.405179\n",
      "Train Epoch: 33 [64000/71432 (90%)]\tLoss: 0.395136\n",
      "Train Epoch: 33 [70400/71432 (98%)]\tLoss: 0.355550\n",
      "\n",
      "Test set: Average loss: 0.0062, Accuracy: 6811/8141 (84%), Positive accuracy: 3457/4091 (85%), Negative accuracy: 3354/4050 (83%), train loss: 0.0062\n",
      "\n",
      "Train Epoch: 34 [0/71432 (0%)]\tLoss: 0.386169\n",
      "Train Epoch: 34 [6400/71432 (9%)]\tLoss: 0.504288\n",
      "Train Epoch: 34 [12800/71432 (18%)]\tLoss: 0.455410\n",
      "Train Epoch: 34 [19200/71432 (27%)]\tLoss: 0.522577\n",
      "Train Epoch: 34 [25600/71432 (36%)]\tLoss: 0.405612\n",
      "Train Epoch: 34 [32000/71432 (45%)]\tLoss: 0.508353\n",
      "Train Epoch: 34 [38400/71432 (54%)]\tLoss: 0.383300\n",
      "Train Epoch: 34 [44800/71432 (63%)]\tLoss: 0.351918\n",
      "Train Epoch: 34 [51200/71432 (72%)]\tLoss: 0.346879\n",
      "Train Epoch: 34 [57600/71432 (81%)]\tLoss: 0.445267\n",
      "Train Epoch: 34 [64000/71432 (90%)]\tLoss: 0.431084\n",
      "Train Epoch: 34 [70400/71432 (98%)]\tLoss: 0.362175\n",
      "\n",
      "Test set: Average loss: 0.0061, Accuracy: 6848/8141 (84%), Positive accuracy: 3591/4091 (88%), Negative accuracy: 3257/4050 (80%), train loss: 0.0062\n",
      "\n",
      "Train Epoch: 35 [0/71432 (0%)]\tLoss: 0.298218\n",
      "Train Epoch: 35 [6400/71432 (9%)]\tLoss: 0.379103\n",
      "Train Epoch: 35 [12800/71432 (18%)]\tLoss: 0.471775\n",
      "Train Epoch: 35 [19200/71432 (27%)]\tLoss: 0.400704\n",
      "Train Epoch: 35 [25600/71432 (36%)]\tLoss: 0.363573\n",
      "Train Epoch: 35 [32000/71432 (45%)]\tLoss: 0.365971\n",
      "Train Epoch: 35 [38400/71432 (54%)]\tLoss: 0.345311\n",
      "Train Epoch: 35 [44800/71432 (63%)]\tLoss: 0.264420\n",
      "Train Epoch: 35 [51200/71432 (72%)]\tLoss: 0.464953\n",
      "Train Epoch: 35 [57600/71432 (81%)]\tLoss: 0.380183\n",
      "Train Epoch: 35 [64000/71432 (90%)]\tLoss: 0.399791\n",
      "Train Epoch: 35 [70400/71432 (98%)]\tLoss: 0.484931\n",
      "\n",
      "Test set: Average loss: 0.0061, Accuracy: 6852/8141 (84%), Positive accuracy: 3549/4091 (87%), Negative accuracy: 3303/4050 (82%), train loss: 0.0062\n",
      "\n",
      "Train Epoch: 36 [0/71432 (0%)]\tLoss: 0.441866\n",
      "Train Epoch: 36 [6400/71432 (9%)]\tLoss: 0.398934\n",
      "Train Epoch: 36 [12800/71432 (18%)]\tLoss: 0.437398\n",
      "Train Epoch: 36 [19200/71432 (27%)]\tLoss: 0.399330\n",
      "Train Epoch: 36 [25600/71432 (36%)]\tLoss: 0.433537\n",
      "Train Epoch: 36 [32000/71432 (45%)]\tLoss: 0.396919\n",
      "Train Epoch: 36 [38400/71432 (54%)]\tLoss: 0.496624\n",
      "Train Epoch: 36 [44800/71432 (63%)]\tLoss: 0.358913\n",
      "Train Epoch: 36 [51200/71432 (72%)]\tLoss: 0.332200\n",
      "Train Epoch: 36 [57600/71432 (81%)]\tLoss: 0.373160\n",
      "Train Epoch: 36 [64000/71432 (90%)]\tLoss: 0.470426\n",
      "Train Epoch: 36 [70400/71432 (98%)]\tLoss: 0.360813\n",
      "\n",
      "Test set: Average loss: 0.0062, Accuracy: 6810/8141 (84%), Positive accuracy: 3437/4091 (84%), Negative accuracy: 3373/4050 (83%), train loss: 0.0062\n",
      "\n",
      "Train Epoch: 37 [0/71432 (0%)]\tLoss: 0.381615\n",
      "Train Epoch: 37 [6400/71432 (9%)]\tLoss: 0.465396\n",
      "Train Epoch: 37 [12800/71432 (18%)]\tLoss: 0.315533\n",
      "Train Epoch: 37 [19200/71432 (27%)]\tLoss: 0.339747\n",
      "Train Epoch: 37 [25600/71432 (36%)]\tLoss: 0.380317\n",
      "Train Epoch: 37 [32000/71432 (45%)]\tLoss: 0.386642\n",
      "Train Epoch: 37 [38400/71432 (54%)]\tLoss: 0.459333\n",
      "Train Epoch: 37 [44800/71432 (63%)]\tLoss: 0.402056\n",
      "Train Epoch: 37 [51200/71432 (72%)]\tLoss: 0.416122\n",
      "Train Epoch: 37 [57600/71432 (81%)]\tLoss: 0.331206\n",
      "Train Epoch: 37 [64000/71432 (90%)]\tLoss: 0.355168\n",
      "Train Epoch: 37 [70400/71432 (98%)]\tLoss: 0.457449\n",
      "\n",
      "Test set: Average loss: 0.0061, Accuracy: 6815/8141 (84%), Positive accuracy: 3490/4091 (85%), Negative accuracy: 3325/4050 (82%), train loss: 0.0062\n",
      "\n",
      "Train Epoch: 38 [0/71432 (0%)]\tLoss: 0.436132\n",
      "Train Epoch: 38 [6400/71432 (9%)]\tLoss: 0.355414\n",
      "Train Epoch: 38 [12800/71432 (18%)]\tLoss: 0.421524\n",
      "Train Epoch: 38 [19200/71432 (27%)]\tLoss: 0.483001\n",
      "Train Epoch: 38 [25600/71432 (36%)]\tLoss: 0.367437\n",
      "Train Epoch: 38 [32000/71432 (45%)]\tLoss: 0.403905\n",
      "Train Epoch: 38 [38400/71432 (54%)]\tLoss: 0.336312\n",
      "Train Epoch: 38 [44800/71432 (63%)]\tLoss: 0.363816\n",
      "Train Epoch: 38 [51200/71432 (72%)]\tLoss: 0.509336\n",
      "Train Epoch: 38 [57600/71432 (81%)]\tLoss: 0.416230\n",
      "Train Epoch: 38 [64000/71432 (90%)]\tLoss: 0.296903\n",
      "Train Epoch: 38 [70400/71432 (98%)]\tLoss: 0.473937\n",
      "\n",
      "Test set: Average loss: 0.0061, Accuracy: 6855/8141 (84%), Positive accuracy: 3557/4091 (87%), Negative accuracy: 3298/4050 (81%), train loss: 0.0061\n",
      "\n",
      "Train Epoch: 39 [0/71432 (0%)]\tLoss: 0.392593\n",
      "Train Epoch: 39 [6400/71432 (9%)]\tLoss: 0.405264\n",
      "Train Epoch: 39 [12800/71432 (18%)]\tLoss: 0.290895\n",
      "Train Epoch: 39 [19200/71432 (27%)]\tLoss: 0.369262\n",
      "Train Epoch: 39 [25600/71432 (36%)]\tLoss: 0.313393\n",
      "Train Epoch: 39 [32000/71432 (45%)]\tLoss: 0.367427\n",
      "Train Epoch: 39 [38400/71432 (54%)]\tLoss: 0.423600\n",
      "Train Epoch: 39 [44800/71432 (63%)]\tLoss: 0.361028\n",
      "Train Epoch: 39 [51200/71432 (72%)]\tLoss: 0.402730\n",
      "Train Epoch: 39 [57600/71432 (81%)]\tLoss: 0.492358\n",
      "Train Epoch: 39 [64000/71432 (90%)]\tLoss: 0.461677\n",
      "Train Epoch: 39 [70400/71432 (98%)]\tLoss: 0.494310\n",
      "\n",
      "Test set: Average loss: 0.0061, Accuracy: 6820/8141 (84%), Positive accuracy: 3441/4091 (84%), Negative accuracy: 3379/4050 (83%), train loss: 0.0061\n",
      "\n",
      "Train Epoch: 40 [0/71432 (0%)]\tLoss: 0.417122\n",
      "Train Epoch: 40 [6400/71432 (9%)]\tLoss: 0.483613\n",
      "Train Epoch: 40 [12800/71432 (18%)]\tLoss: 0.332480\n",
      "Train Epoch: 40 [19200/71432 (27%)]\tLoss: 0.474447\n",
      "Train Epoch: 40 [25600/71432 (36%)]\tLoss: 0.331514\n",
      "Train Epoch: 40 [32000/71432 (45%)]\tLoss: 0.457140\n",
      "Train Epoch: 40 [38400/71432 (54%)]\tLoss: 0.455714\n",
      "Train Epoch: 40 [44800/71432 (63%)]\tLoss: 0.353122\n",
      "Train Epoch: 40 [51200/71432 (72%)]\tLoss: 0.382472\n",
      "Train Epoch: 40 [57600/71432 (81%)]\tLoss: 0.391772\n",
      "Train Epoch: 40 [64000/71432 (90%)]\tLoss: 0.433762\n",
      "Train Epoch: 40 [70400/71432 (98%)]\tLoss: 0.522713\n",
      "\n",
      "Test set: Average loss: 0.0061, Accuracy: 6835/8141 (84%), Positive accuracy: 3460/4091 (85%), Negative accuracy: 3375/4050 (83%), train loss: 0.0061\n",
      "\n",
      "[4598, 5580, 5968, 6265, 6418, 6457, 6549, 6590, 6606, 6670, 6684, 6707, 6710, 6763, 6698, 6729, 6766, 6789, 6819, 6807, 6817, 6802, 6762, 6804, 6857, 6799, 6847, 6826, 6769, 6851, 6871, 6851, 6811, 6848, 6852, 6810, 6815, 6855, 6820, 6835]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def myLoss(output, target):\n",
    "#     print(\"size,size:\",output.size(), target.size())\n",
    "#     print(\"size,\", ((1-2*target) * torch.log(output)).size())\n",
    "#     return -torch.sum(target * torch.log(output) + (1-target) * torch.log(1-output)) / len(output)\n",
    "    return -torch.sum((36885/34546)*target * torch.log(output) + (1-target) * torch.log(1-output)) / len(output)\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)[0]\n",
    "#         loss = F.nll_loss(output, target)\n",
    "        loss = myLoss(output, target)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item()/64)\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    train_loss = sum(loss_list)/len(loss_list)\n",
    "    return train_loss\n",
    "\n",
    "        \n",
    "def test(args, model, device, test_loader,count,epoch,train_loss):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    result= [[0,0], [0,0]] \n",
    "    correct_pc_true0 = []\n",
    "    correct_pc_true1 = []\n",
    "    wrong_pc_true0 = []\n",
    "    wrong_pc_true1 = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx,(data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)[0]\n",
    "#             test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            test_loss += myLoss(output, target)\n",
    "#             pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            t = Variable(torch.Tensor([0.5]))\n",
    "            pred = (output > t) * 1\n",
    "            pred = torch.reshape(pred, (len(target), 1))\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            cmat = confusion_matrix(target.view_as(pred), pred, labels=[0, 1]) \n",
    "            result = [[result[i][j] + cmat[i][j]  for j in range(len(result[0]))] for i in range(len(result))] \n",
    "             # Store wrongly predicted images\n",
    "            if epoch == 41:\n",
    "                for ori in range(8):\n",
    "                    for scale in range(6):\n",
    "                        plt.axis('off')\n",
    "                        plt.imshow(model.conv_layer.weight[ori*6 + scale][0].detach().numpy(), cmap=plt.cm.gray)\n",
    "                        plt.savefig('gabor_plot/cos_ori'+str(ori+1)+'_fq'+str(scale+1)+'.png', pad_inches=0)\n",
    "                        plt.axis('off')\n",
    "                        plt.imshow(model.conv_layer.weight[ori*6 + scale + 48][0].detach().numpy(), cmap=plt.cm.gray)\n",
    "                        plt.savefig('gabor_plot/sin_ori'+str(ori+1)+'_fq'+str(scale+1)+'.png', pad_inches=0)\n",
    "                count = 0\n",
    "                wrong_idx = (pred != target.view_as(pred)).nonzero()[:, 0]\n",
    "                wrong_samples = data[wrong_idx]\n",
    "                wrong_preds = pred[wrong_idx]\n",
    "                actual_preds = target.view_as(pred)[wrong_idx]\n",
    "                true_idx = (pred == target.view_as(pred)).nonzero()[:, 0]\n",
    "                true_samples = data[true_idx]\n",
    "                true_preds = pred[true_idx]\n",
    "                for i in range(len(true_idx)):\n",
    "                    true_pred = true_preds[i]\n",
    "                    if true_pred.item() == 0:\n",
    "                        correct_pc_true0.append(model(data)[1][true_idx[i]].item())\n",
    "                    else:\n",
    "                        correct_pc_true1.append(model(data)[1][true_idx[i]].item())\n",
    "                        \n",
    "                for i in range(len(wrong_idx)):\n",
    "                    actual_pred = actual_preds[i]\n",
    "                    if actual_pred.item() == 0:\n",
    "                        wrong_pc_true0.append(model(data)[1][wrong_idx[i]].item())\n",
    "                    else:\n",
    "                        wrong_pc_true1.append(model(data)[1][wrong_idx[i]].item())\n",
    "                    sample = wrong_samples[i]\n",
    "                    wrong_pred = wrong_preds[i]\n",
    "                    actual_pred = actual_preds[i]\n",
    "                    # Undo normalization\n",
    "            #         sample = sample * 0.3081\n",
    "            #         sample = sample + 0.1307\n",
    "                    sample = sample * 255.\n",
    "                    sample = sample.byte()\n",
    "                    img = TF.to_pil_image(sample)\n",
    "                    num = batch_idx * 64 + wrong_idx[i]\n",
    "                    count = count +1\n",
    "#                     img.save('./wrong-gabor/{}_true{}_pc{:.4f}.png'.format(\n",
    "#                     batch_idx*64+count, actual_pred.item(),model(data)[1][wrong_idx[i]].item()))\n",
    "                    \n",
    "    #                 print(batch_idx,wrong_idx[i])\n",
    "                    img_ori = origin_dataset[num][0].numpy()\n",
    "                    plt.imsave('./wrong-gabor/{}_true{}__pc{:.4f}_ori.png'.format(\n",
    "                    batch_idx*64+count, actual_pred.item(), model(data)[1][wrong_idx[i]].item()),img_ori[0], cmap = 'gray')\n",
    "                    \n",
    "            \n",
    "                \n",
    "                    \n",
    "                \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Positive accuracy: {}/{} ({:.0f}%), Negative accuracy: {}/{} ({:.0f}%), train loss: {:.4f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset), \n",
    "        result[1][1],result[1][1]+result[1][0],100. * result[1][1]/(result[1][1]+result[1][0]),\n",
    "        result[0][0],result[0][0]+result[0][1],100. * result[0][0]/(result[0][0]+result[0][1]),train_loss))\n",
    "    if len(correct_pc_true0) != 0:\n",
    "        print('\\ncorrect_0: mean: {:.4f}, std: {:.4f}'.format(statistics.mean(correct_pc_true0),statistics.pstdev(correct_pc_true0)))\n",
    "        print('\\ncorrect_1: mean: {:.4f}, std: {:.4f}'.format(statistics.mean(correct_pc_true1),statistics.pstdev(correct_pc_true1)))\n",
    "        print('\\nwrong_0: mean: {:.4f}, std: {:.4f}'.format(statistics.mean(wrong_pc_true0), statistics.pstdev(wrong_pc_true0)))\n",
    "        print('\\nwrong_1: mean: {:.4f}, std: {:.4f}'.format(statistics.mean(wrong_pc_true1), statistics.pstdev(wrong_pc_true1)))\n",
    "    return test_loss, correct\n",
    "    \n",
    "\n",
    "# def main():\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=40, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save-model', action='store_true', default=True,\n",
    "                    help='For Saving the current Model')\n",
    "parser.add_argument('--std', type=float, default=0, metavar='STD',\n",
    "                    help='noise-std (default: 0)')\n",
    "parser.add_argument('--mean', type=float, default=0, metavar='MEAN',\n",
    "                    help='noise-std (default: 0)')\n",
    "#     args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "#     transform=transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,))\n",
    "#         ])\n",
    "model = GaborConv2dPC().to(device)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "count = 0\n",
    "for param in model.parameters():\n",
    "    print(type(param.data), param.size())\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "test_accuracy_list = []\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
    "    train_loss_list.append(train_loss)\n",
    "    test_result = test(args, model, device, test_loader,count,epoch,train_loss)\n",
    "    test_loss_list.append(test_result[0])\n",
    "    test_accuracy_list.append(test_result[1])\n",
    "    # for param in model.parameters():\n",
    "    #     print(param.size(), param.data)\n",
    "    # print(model.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(),\"pretrain_gabor.pt\")\n",
    "plt.plot(train_loss_list, color='blue',label='train loss')  \n",
    "plt.plot(test_loss_list, color='red',label='test loss')  \n",
    "plt.legend()\n",
    "print(test_accuracy_list)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('sigma', tensor([0.2520])),\n",
       "             ('theta', tensor([0.1560])),\n",
       "             ('gamma', tensor([0.5384])),\n",
       "             ('Lambda', tensor([1.5396])),\n",
       "             ('psi', tensor([0.7818])),\n",
       "             ('x0', tensor(10.)),\n",
       "             ('y0', tensor(10.)),\n",
       "             ('y',\n",
       "              tensor([[-9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000,\n",
       "                       -9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000,\n",
       "                       -9.0000, -9.0000, -9.0000],\n",
       "                      [-7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444,\n",
       "                       -7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444,\n",
       "                       -7.9444, -7.9444, -7.9444],\n",
       "                      [-6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889,\n",
       "                       -6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889,\n",
       "                       -6.8889, -6.8889, -6.8889],\n",
       "                      [-5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333,\n",
       "                       -5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333,\n",
       "                       -5.8333, -5.8333, -5.8333],\n",
       "                      [-4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778,\n",
       "                       -4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778,\n",
       "                       -4.7778, -4.7778, -4.7778],\n",
       "                      [-3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222,\n",
       "                       -3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222,\n",
       "                       -3.7222, -3.7222, -3.7222],\n",
       "                      [-2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667,\n",
       "                       -2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667,\n",
       "                       -2.6667, -2.6667, -2.6667],\n",
       "                      [-1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111,\n",
       "                       -1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111,\n",
       "                       -1.6111, -1.6111, -1.6111],\n",
       "                      [-0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556,\n",
       "                       -0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556,\n",
       "                       -0.5556, -0.5556, -0.5556],\n",
       "                      [ 0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
       "                        0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
       "                        0.5000,  0.5000,  0.5000],\n",
       "                      [ 1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,\n",
       "                        1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,\n",
       "                        1.5556,  1.5556,  1.5556],\n",
       "                      [ 2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,\n",
       "                        2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,\n",
       "                        2.6111,  2.6111,  2.6111],\n",
       "                      [ 3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,\n",
       "                        3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,\n",
       "                        3.6667,  3.6667,  3.6667],\n",
       "                      [ 4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,\n",
       "                        4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,\n",
       "                        4.7222,  4.7222,  4.7222],\n",
       "                      [ 5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,\n",
       "                        5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,\n",
       "                        5.7778,  5.7778,  5.7778],\n",
       "                      [ 6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,\n",
       "                        6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,\n",
       "                        6.8333,  6.8333,  6.8333],\n",
       "                      [ 7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,\n",
       "                        7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,\n",
       "                        7.8889,  7.8889,  7.8889],\n",
       "                      [ 8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,\n",
       "                        8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,\n",
       "                        8.9444,  8.9444,  8.9444],\n",
       "                      [10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "                       10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "                       10.0000, 10.0000, 10.0000]])),\n",
       "             ('x',\n",
       "              tensor([[-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000]])),\n",
       "             ('weight',\n",
       "              tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        ...,\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        ...,\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        ...,\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        ...,\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        ...,\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        ...,\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                        [1., 1., 1.,  ..., 1., 1., 1.]]]])),\n",
       "             ('coeff',\n",
       "              tensor([[[-2.9860e-01],\n",
       "                       [-2.1052e-01],\n",
       "                       [ 2.5030e+00],\n",
       "                       [-1.6125e+00],\n",
       "                       [-1.0513e+00],\n",
       "                       [-3.0339e-02],\n",
       "                       [ 1.9337e+00],\n",
       "                       [-1.5086e-01],\n",
       "                       [-4.9648e-01],\n",
       "                       [ 1.6021e+00],\n",
       "                       [-8.7296e-03],\n",
       "                       [-2.4856e+00],\n",
       "                       [-4.8020e-01],\n",
       "                       [ 5.6800e-01],\n",
       "                       [ 6.0592e-01],\n",
       "                       [ 5.9419e-01],\n",
       "                       [ 2.7937e-03],\n",
       "                       [-5.3930e-03],\n",
       "                       [-3.8290e-02],\n",
       "                       [-8.9642e-03],\n",
       "                       [-3.1593e+00],\n",
       "                       [-9.5828e-01],\n",
       "                       [ 1.2895e-03],\n",
       "                       [ 1.3469e+00],\n",
       "                       [ 8.8311e-01],\n",
       "                       [ 2.6145e+00],\n",
       "                       [ 2.1076e+00],\n",
       "                       [-2.0106e+00],\n",
       "                       [-6.4776e-03],\n",
       "                       [ 4.1298e-03],\n",
       "                       [ 5.4809e-01],\n",
       "                       [-1.5001e+00],\n",
       "                       [-4.2478e-01],\n",
       "                       [ 6.5488e-01],\n",
       "                       [-1.0728e-02],\n",
       "                       [-1.1045e+00],\n",
       "                       [-6.3305e-01],\n",
       "                       [ 1.0380e+00],\n",
       "                       [ 5.4037e-01],\n",
       "                       [ 9.6189e-02],\n",
       "                       [ 6.3872e-03],\n",
       "                       [ 4.3211e-03],\n",
       "                       [-2.3442e+00],\n",
       "                       [-1.0651e+00],\n",
       "                       [ 1.8701e+00],\n",
       "                       [-7.9379e-01],\n",
       "                       [-8.1553e-03],\n",
       "                       [-6.3120e-03]]])),\n",
       "             ('w', tensor([17.2165])),\n",
       "             ('b', tensor([-4.0766])),\n",
       "             ('x_shape', tensor(10.)),\n",
       "             ('y_shape', tensor(10.)),\n",
       "             ('y_grid',\n",
       "              tensor([[-9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000,\n",
       "                       -9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000,\n",
       "                       -9.0000, -9.0000, -9.0000],\n",
       "                      [-7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444,\n",
       "                       -7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444,\n",
       "                       -7.9444, -7.9444, -7.9444],\n",
       "                      [-6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889,\n",
       "                       -6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889,\n",
       "                       -6.8889, -6.8889, -6.8889],\n",
       "                      [-5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333,\n",
       "                       -5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333,\n",
       "                       -5.8333, -5.8333, -5.8333],\n",
       "                      [-4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778,\n",
       "                       -4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778,\n",
       "                       -4.7778, -4.7778, -4.7778],\n",
       "                      [-3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222,\n",
       "                       -3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222,\n",
       "                       -3.7222, -3.7222, -3.7222],\n",
       "                      [-2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667,\n",
       "                       -2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667,\n",
       "                       -2.6667, -2.6667, -2.6667],\n",
       "                      [-1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111,\n",
       "                       -1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111,\n",
       "                       -1.6111, -1.6111, -1.6111],\n",
       "                      [-0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556,\n",
       "                       -0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556,\n",
       "                       -0.5556, -0.5556, -0.5556],\n",
       "                      [ 0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
       "                        0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
       "                        0.5000,  0.5000,  0.5000],\n",
       "                      [ 1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,\n",
       "                        1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,\n",
       "                        1.5556,  1.5556,  1.5556],\n",
       "                      [ 2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,\n",
       "                        2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,\n",
       "                        2.6111,  2.6111,  2.6111],\n",
       "                      [ 3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,\n",
       "                        3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,\n",
       "                        3.6667,  3.6667,  3.6667],\n",
       "                      [ 4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,\n",
       "                        4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,\n",
       "                        4.7222,  4.7222,  4.7222],\n",
       "                      [ 5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,\n",
       "                        5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,\n",
       "                        5.7778,  5.7778,  5.7778],\n",
       "                      [ 6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,\n",
       "                        6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,\n",
       "                        6.8333,  6.8333,  6.8333],\n",
       "                      [ 7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,\n",
       "                        7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,\n",
       "                        7.8889,  7.8889,  7.8889],\n",
       "                      [ 8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,\n",
       "                        8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,\n",
       "                        8.9444,  8.9444,  8.9444],\n",
       "                      [10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "                       10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "                       10.0000, 10.0000, 10.0000]])),\n",
       "             ('x_grid',\n",
       "              tensor([[-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000],\n",
       "                      [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "                       -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "                        7.8889,  8.9444, 10.0000]])),\n",
       "             ('conv_layer.weight',\n",
       "              tensor([[[[-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "                          0.0000e+00,  0.0000e+00],\n",
       "                        [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "                          0.0000e+00,  0.0000e+00],\n",
       "                        [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "                          0.0000e+00, -0.0000e+00],\n",
       "                        ...,\n",
       "                        [ 0.0000e+00, -0.0000e+00,  1.0229e-43,  ..., -0.0000e+00,\n",
       "                         -0.0000e+00,  0.0000e+00],\n",
       "                        [-0.0000e+00, -0.0000e+00,  1.4013e-45,  ..., -0.0000e+00,\n",
       "                         -0.0000e+00,  0.0000e+00],\n",
       "                        [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "                         -0.0000e+00,  0.0000e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.7605e-22,  6.2091e-19,  8.6521e-16,  ..., -1.4434e-13,\n",
       "                          7.4702e-16,  1.8980e-19],\n",
       "                        [-4.5812e-21,  1.5291e-17,  1.1361e-14,  ..., -2.0527e-13,\n",
       "                          3.1600e-15,  1.0978e-19],\n",
       "                        [-4.8297e-20,  2.5805e-16,  9.0969e-14,  ...,  8.0827e-13,\n",
       "                          9.4985e-15, -1.3150e-18],\n",
       "                        ...,\n",
       "                        [ 5.5267e-17, -1.3932e-12,  1.5471e-11,  ..., -1.6405e-17,\n",
       "                         -1.1241e-20,  1.2663e-24],\n",
       "                        [-1.3675e-16, -3.1014e-13,  2.0765e-11,  ..., -1.2083e-18,\n",
       "                         -4.3586e-22,  7.3299e-26],\n",
       "                        [-5.3788e-17, -4.8843e-14,  5.8598e-12,  ..., -5.9002e-20,\n",
       "                         -1.1108e-23,  2.8621e-27]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0762e-05,  5.1837e-05,  3.0687e-04,  ..., -5.5426e-04,\n",
       "                          3.5374e-04,  2.1987e-05],\n",
       "                        [-1.6778e-05,  1.2938e-04,  4.4062e-04,  ..., -2.4969e-04,\n",
       "                          5.1141e-04,  4.6894e-06],\n",
       "                        [-2.0579e-05,  2.7410e-04,  4.7784e-04,  ...,  3.8576e-04,\n",
       "                          6.5074e-04, -2.5657e-05],\n",
       "                        ...,\n",
       "                        [ 1.1213e-05, -2.0411e-03,  3.2471e-04,  ..., -9.1677e-05,\n",
       "                         -2.6122e-05,  2.4276e-06],\n",
       "                        [-7.9945e-05, -1.4126e-03,  1.4619e-03,  ..., -6.5614e-05,\n",
       "                         -1.0619e-05,  1.5895e-06],\n",
       "                        [-1.1223e-04, -8.5670e-04,  1.7140e-03,  ..., -3.8562e-05,\n",
       "                         -3.5141e-06,  8.6957e-07]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-1.9609e-01,  5.6924e-04,  3.3750e-01,  ..., -1.0150e-01,\n",
       "                          9.7934e-03,  4.1837e-02],\n",
       "                        [-1.9245e-01, -1.0394e-01,  3.3867e-01,  ..., -1.1886e-01,\n",
       "                         -1.8902e-02,  5.0081e-02],\n",
       "                        [-1.5599e-01, -2.0365e-01,  2.8144e-01,  ..., -1.1694e-01,\n",
       "                         -5.5625e-02,  5.0443e-02],\n",
       "                        ...,\n",
       "                        [-7.8757e-02,  2.2647e-02,  1.7179e-01,  ..., -2.7123e-01,\n",
       "                          6.9086e-02,  1.4167e-01],\n",
       "                        [-6.5123e-02, -1.9427e-02,  1.4521e-01,  ..., -2.6742e-01,\n",
       "                         -9.8337e-03,  1.4280e-01],\n",
       "                        [-4.5391e-02, -4.5340e-02,  1.0358e-01,  ..., -2.2363e-01,\n",
       "                         -7.7552e-02,  1.2212e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.9111e-01,  1.5985e-03,  7.8164e-01,  ..., -5.9094e-01,\n",
       "                          7.8867e-02,  4.8236e-01],\n",
       "                        [-6.4831e-01, -2.7738e-01,  7.4105e-01,  ..., -6.0276e-01,\n",
       "                         -1.3182e-01,  4.9713e-01],\n",
       "                        [-5.0824e-01, -5.2256e-01,  5.8874e-01,  ..., -5.2268e-01,\n",
       "                         -3.3991e-01,  4.3621e-01],\n",
       "                        ...,\n",
       "                        [-5.5501e-01,  1.1588e-01,  6.6063e-01,  ..., -7.1397e-01,\n",
       "                          2.2922e-01,  6.1325e-01],\n",
       "                        [-5.2979e-01, -1.1409e-01,  6.3717e-01,  ..., -7.4052e-01,\n",
       "                         -3.4125e-02,  6.4276e-01],\n",
       "                        [-4.3134e-01, -3.0923e-01,  5.2477e-01,  ..., -6.5918e-01,\n",
       "                         -2.8481e-01,  5.7835e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-9.1962e-01,  2.0203e-03,  9.4562e-01,  ..., -8.8112e-01,\n",
       "                          1.2657e-01,  8.3976e-01],\n",
       "                        [-8.5388e-01, -3.4653e-01,  8.8505e-01,  ..., -8.7104e-01,\n",
       "                         -2.0476e-01,  8.3658e-01],\n",
       "                        [-6.6434e-01, -6.4706e-01,  6.9599e-01,  ..., -7.3399e-01,\n",
       "                         -5.1241e-01,  7.1147e-01],\n",
       "                        ...,\n",
       "                        [-8.6417e-01,  1.6779e-01,  8.9662e-01,  ..., -8.8920e-01,\n",
       "                          3.0086e-01,  8.5495e-01],\n",
       "                        [-8.5221e-01, -1.7044e-01,  8.9104e-01,  ..., -9.3293e-01,\n",
       "                         -4.5248e-02,  9.0407e-01],\n",
       "                        [-7.1873e-01, -4.7792e-01,  7.5819e-01,  ..., -8.4230e-01,\n",
       "                         -3.8253e-01,  8.2291e-01]]]]))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0583],\n",
       "        [ 2.2225],\n",
       "        [ 4.6672],\n",
       "        [ 9.8010],\n",
       "        [20.5822],\n",
       "        [43.2225],\n",
       "        [ 1.0583],\n",
       "        [ 2.2225],\n",
       "        [ 4.6672],\n",
       "        [ 9.8010],\n",
       "        [20.5822],\n",
       "        [43.2225],\n",
       "        [ 1.0583],\n",
       "        [ 2.2225],\n",
       "        [ 4.6672],\n",
       "        [ 9.8010],\n",
       "        [20.5822],\n",
       "        [43.2225],\n",
       "        [ 1.0583],\n",
       "        [ 2.2225],\n",
       "        [ 4.6672],\n",
       "        [ 9.8010],\n",
       "        [20.5822],\n",
       "        [43.2225],\n",
       "        [ 1.0583],\n",
       "        [ 2.2225],\n",
       "        [ 4.6672],\n",
       "        [ 9.8010],\n",
       "        [20.5822],\n",
       "        [43.2225],\n",
       "        [ 1.0583],\n",
       "        [ 2.2225],\n",
       "        [ 4.6672],\n",
       "        [ 9.8010],\n",
       "        [20.5822],\n",
       "        [43.2225],\n",
       "        [ 1.0583],\n",
       "        [ 2.2225],\n",
       "        [ 4.6672],\n",
       "        [ 9.8010],\n",
       "        [20.5822],\n",
       "        [43.2225],\n",
       "        [ 1.0583],\n",
       "        [ 2.2225],\n",
       "        [ 4.6672],\n",
       "        [ 9.8010],\n",
       "        [20.5822],\n",
       "        [43.2225]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sigma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.2520], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1560], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.5384], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.5396], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.7818], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor(10.),\n",
       " Parameter containing:\n",
       " tensor(10.),\n",
       " Parameter containing:\n",
       " tensor([[-9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000,\n",
       "          -9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000, -9.0000,\n",
       "          -9.0000, -9.0000, -9.0000],\n",
       "         [-7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444,\n",
       "          -7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444, -7.9444,\n",
       "          -7.9444, -7.9444, -7.9444],\n",
       "         [-6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889,\n",
       "          -6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889, -6.8889,\n",
       "          -6.8889, -6.8889, -6.8889],\n",
       "         [-5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333,\n",
       "          -5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333, -5.8333,\n",
       "          -5.8333, -5.8333, -5.8333],\n",
       "         [-4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778,\n",
       "          -4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778, -4.7778,\n",
       "          -4.7778, -4.7778, -4.7778],\n",
       "         [-3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222,\n",
       "          -3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222, -3.7222,\n",
       "          -3.7222, -3.7222, -3.7222],\n",
       "         [-2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667,\n",
       "          -2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667, -2.6667,\n",
       "          -2.6667, -2.6667, -2.6667],\n",
       "         [-1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111,\n",
       "          -1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111, -1.6111,\n",
       "          -1.6111, -1.6111, -1.6111],\n",
       "         [-0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556,\n",
       "          -0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556, -0.5556,\n",
       "          -0.5556, -0.5556, -0.5556],\n",
       "         [ 0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
       "           0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
       "           0.5000,  0.5000,  0.5000],\n",
       "         [ 1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,\n",
       "           1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,\n",
       "           1.5556,  1.5556,  1.5556],\n",
       "         [ 2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,\n",
       "           2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,  2.6111,\n",
       "           2.6111,  2.6111,  2.6111],\n",
       "         [ 3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,\n",
       "           3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,  3.6667,\n",
       "           3.6667,  3.6667,  3.6667],\n",
       "         [ 4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,\n",
       "           4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,  4.7222,\n",
       "           4.7222,  4.7222,  4.7222],\n",
       "         [ 5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,\n",
       "           5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,  5.7778,\n",
       "           5.7778,  5.7778,  5.7778],\n",
       "         [ 6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,\n",
       "           6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,  6.8333,\n",
       "           6.8333,  6.8333,  6.8333],\n",
       "         [ 7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,\n",
       "           7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,  7.8889,\n",
       "           7.8889,  7.8889,  7.8889],\n",
       "         [ 8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,\n",
       "           8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,  8.9444,\n",
       "           8.9444,  8.9444,  8.9444],\n",
       "         [10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "          10.0000, 10.0000, 10.0000]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000],\n",
       "         [-9.0000, -7.9444, -6.8889, -5.8333, -4.7778, -3.7222, -2.6667, -1.6111,\n",
       "          -0.5556,  0.5000,  1.5556,  2.6111,  3.6667,  4.7222,  5.7778,  6.8333,\n",
       "           7.8889,  8.9444, 10.0000]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[-2.9860e-01],\n",
       "          [-2.1052e-01],\n",
       "          [ 2.5030e+00],\n",
       "          [-1.6125e+00],\n",
       "          [-1.0513e+00],\n",
       "          [-3.0339e-02],\n",
       "          [ 1.9337e+00],\n",
       "          [-1.5086e-01],\n",
       "          [-4.9648e-01],\n",
       "          [ 1.6021e+00],\n",
       "          [-8.7296e-03],\n",
       "          [-2.4856e+00],\n",
       "          [-4.8020e-01],\n",
       "          [ 5.6800e-01],\n",
       "          [ 6.0592e-01],\n",
       "          [ 5.9419e-01],\n",
       "          [ 2.7937e-03],\n",
       "          [-5.3930e-03],\n",
       "          [-3.8290e-02],\n",
       "          [-8.9642e-03],\n",
       "          [-3.1593e+00],\n",
       "          [-9.5828e-01],\n",
       "          [ 1.2895e-03],\n",
       "          [ 1.3469e+00],\n",
       "          [ 8.8311e-01],\n",
       "          [ 2.6145e+00],\n",
       "          [ 2.1076e+00],\n",
       "          [-2.0106e+00],\n",
       "          [-6.4776e-03],\n",
       "          [ 4.1298e-03],\n",
       "          [ 5.4809e-01],\n",
       "          [-1.5001e+00],\n",
       "          [-4.2478e-01],\n",
       "          [ 6.5488e-01],\n",
       "          [-1.0728e-02],\n",
       "          [-1.1045e+00],\n",
       "          [-6.3305e-01],\n",
       "          [ 1.0380e+00],\n",
       "          [ 5.4037e-01],\n",
       "          [ 9.6189e-02],\n",
       "          [ 6.3872e-03],\n",
       "          [ 4.3211e-03],\n",
       "          [-2.3442e+00],\n",
       "          [-1.0651e+00],\n",
       "          [ 1.8701e+00],\n",
       "          [-7.9379e-01],\n",
       "          [-8.1553e-03],\n",
       "          [-6.3120e-03]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([17.2165], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-4.0766], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "             0.0000e+00,  0.0000e+00],\n",
       "           [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "             0.0000e+00, -0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00, -0.0000e+00,  1.0229e-43,  ..., -0.0000e+00,\n",
       "            -0.0000e+00,  0.0000e+00],\n",
       "           [-0.0000e+00, -0.0000e+00,  1.4013e-45,  ..., -0.0000e+00,\n",
       "            -0.0000e+00,  0.0000e+00],\n",
       "           [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "            -0.0000e+00,  0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[-2.7605e-22,  6.2091e-19,  8.6521e-16,  ..., -1.4434e-13,\n",
       "             7.4702e-16,  1.8980e-19],\n",
       "           [-4.5812e-21,  1.5291e-17,  1.1361e-14,  ..., -2.0527e-13,\n",
       "             3.1600e-15,  1.0978e-19],\n",
       "           [-4.8297e-20,  2.5805e-16,  9.0969e-14,  ...,  8.0827e-13,\n",
       "             9.4985e-15, -1.3150e-18],\n",
       "           ...,\n",
       "           [ 5.5267e-17, -1.3932e-12,  1.5471e-11,  ..., -1.6405e-17,\n",
       "            -1.1241e-20,  1.2663e-24],\n",
       "           [-1.3675e-16, -3.1014e-13,  2.0765e-11,  ..., -1.2083e-18,\n",
       "            -4.3586e-22,  7.3299e-26],\n",
       "           [-5.3788e-17, -4.8843e-14,  5.8598e-12,  ..., -5.9002e-20,\n",
       "            -1.1108e-23,  2.8621e-27]]],\n",
       " \n",
       " \n",
       "         [[[-1.0762e-05,  5.1837e-05,  3.0687e-04,  ..., -5.5426e-04,\n",
       "             3.5374e-04,  2.1987e-05],\n",
       "           [-1.6778e-05,  1.2938e-04,  4.4062e-04,  ..., -2.4969e-04,\n",
       "             5.1141e-04,  4.6894e-06],\n",
       "           [-2.0579e-05,  2.7410e-04,  4.7784e-04,  ...,  3.8576e-04,\n",
       "             6.5074e-04, -2.5657e-05],\n",
       "           ...,\n",
       "           [ 1.1213e-05, -2.0411e-03,  3.2471e-04,  ..., -9.1677e-05,\n",
       "            -2.6122e-05,  2.4276e-06],\n",
       "           [-7.9945e-05, -1.4126e-03,  1.4619e-03,  ..., -6.5614e-05,\n",
       "            -1.0619e-05,  1.5895e-06],\n",
       "           [-1.1223e-04, -8.5670e-04,  1.7140e-03,  ..., -3.8562e-05,\n",
       "            -3.5141e-06,  8.6957e-07]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-1.9609e-01,  5.6924e-04,  3.3750e-01,  ..., -1.0150e-01,\n",
       "             9.7934e-03,  4.1837e-02],\n",
       "           [-1.9245e-01, -1.0394e-01,  3.3867e-01,  ..., -1.1886e-01,\n",
       "            -1.8902e-02,  5.0081e-02],\n",
       "           [-1.5599e-01, -2.0365e-01,  2.8144e-01,  ..., -1.1694e-01,\n",
       "            -5.5625e-02,  5.0443e-02],\n",
       "           ...,\n",
       "           [-7.8757e-02,  2.2647e-02,  1.7179e-01,  ..., -2.7123e-01,\n",
       "             6.9086e-02,  1.4167e-01],\n",
       "           [-6.5123e-02, -1.9427e-02,  1.4521e-01,  ..., -2.6742e-01,\n",
       "            -9.8337e-03,  1.4280e-01],\n",
       "           [-4.5391e-02, -4.5340e-02,  1.0358e-01,  ..., -2.2363e-01,\n",
       "            -7.7552e-02,  1.2212e-01]]],\n",
       " \n",
       " \n",
       "         [[[-6.9111e-01,  1.5985e-03,  7.8164e-01,  ..., -5.9094e-01,\n",
       "             7.8867e-02,  4.8236e-01],\n",
       "           [-6.4831e-01, -2.7738e-01,  7.4105e-01,  ..., -6.0276e-01,\n",
       "            -1.3182e-01,  4.9713e-01],\n",
       "           [-5.0824e-01, -5.2256e-01,  5.8874e-01,  ..., -5.2268e-01,\n",
       "            -3.3991e-01,  4.3621e-01],\n",
       "           ...,\n",
       "           [-5.5501e-01,  1.1588e-01,  6.6063e-01,  ..., -7.1397e-01,\n",
       "             2.2922e-01,  6.1325e-01],\n",
       "           [-5.2979e-01, -1.1409e-01,  6.3717e-01,  ..., -7.4052e-01,\n",
       "            -3.4125e-02,  6.4276e-01],\n",
       "           [-4.3134e-01, -3.0923e-01,  5.2477e-01,  ..., -6.5918e-01,\n",
       "            -2.8481e-01,  5.7835e-01]]],\n",
       " \n",
       " \n",
       "         [[[-9.1962e-01,  2.0203e-03,  9.4562e-01,  ..., -8.8112e-01,\n",
       "             1.2657e-01,  8.3976e-01],\n",
       "           [-8.5388e-01, -3.4653e-01,  8.8505e-01,  ..., -8.7104e-01,\n",
       "            -2.0476e-01,  8.3658e-01],\n",
       "           [-6.6434e-01, -6.4706e-01,  6.9599e-01,  ..., -7.3399e-01,\n",
       "            -5.1241e-01,  7.1147e-01],\n",
       "           ...,\n",
       "           [-8.6417e-01,  1.6779e-01,  8.9662e-01,  ..., -8.8920e-01,\n",
       "             3.0086e-01,  8.5495e-01],\n",
       "           [-8.5221e-01, -1.7044e-01,  8.9104e-01,  ..., -9.3293e-01,\n",
       "            -4.5248e-02,  9.0407e-01],\n",
       "           [-7.1873e-01, -4.7792e-01,  7.5819e-01,  ..., -8.4230e-01,\n",
       "            -3.8253e-01,  8.2291e-01]]]], requires_grad=True)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = GaborConv2dPC()\n",
    "pic = torch.rand((1000,1,19,19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mod(pic)\n",
    "c1 = mod.conv_layer.weight[48][0].detach().numpy()\n",
    "c2 = mod.conv_layer.weight[49][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for ori in range(8):\n",
    "    for scale in range(6):\n",
    "        plt.axis('off')\n",
    "        plt.imshow(model.conv_layer.weight[ori*6 + scale][0].detach().numpy(), cmap=plt.cm.gray)\n",
    "        plt.savefig('gabor_plot/cos_ori'+str(ori+1)+'_fq'+str(scale+1)+'.png', pad_inches=0)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(model.conv_layer.weight[ori*6 + scale + 48][0].detach().numpy(), cmap=plt.cm.gray)\n",
    "        plt.savefig('gabor_plot/sin_ori'+str(ori+1)+'_fq'+str(scale+1)+'.png', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 96, kernel_size=(19, 19), stride=(1, 1), bias=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.rand((1, 96, 19, 19))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/71432 (0%)]\tLoss: 0.692208\n",
      "Train Epoch: 1 [6400/71432 (9%)]\tLoss: 0.681850\n",
      "Train Epoch: 1 [12800/71432 (18%)]\tLoss: 0.620283\n",
      "Train Epoch: 1 [19200/71432 (27%)]\tLoss: 0.434132\n",
      "Train Epoch: 1 [25600/71432 (36%)]\tLoss: 0.380064\n",
      "Train Epoch: 1 [32000/71432 (45%)]\tLoss: 0.387745\n",
      "Train Epoch: 1 [38400/71432 (54%)]\tLoss: 0.371992\n",
      "Train Epoch: 1 [44800/71432 (63%)]\tLoss: 0.411371\n",
      "Train Epoch: 1 [51200/71432 (72%)]\tLoss: 0.384105\n",
      "Train Epoch: 1 [57600/71432 (81%)]\tLoss: 0.270502\n",
      "Train Epoch: 1 [64000/71432 (90%)]\tLoss: 0.394748\n",
      "Train Epoch: 1 [70400/71432 (98%)]\tLoss: 0.372798\n",
      "\n",
      "Test set: Average loss: 0.2699, Accuracy: 7186/8141 (88%), Positive accuracy: 3743/4091 (91%), Negative accuracy: 3443/4050 (85%), f1 score: 0.8837, Train loss: 0.4389\n",
      "\n",
      "Train Epoch: 2 [0/71432 (0%)]\tLoss: 0.243357\n",
      "Train Epoch: 2 [6400/71432 (9%)]\tLoss: 0.303426\n",
      "Train Epoch: 2 [12800/71432 (18%)]\tLoss: 0.293365\n",
      "Train Epoch: 2 [19200/71432 (27%)]\tLoss: 0.222958\n",
      "Train Epoch: 2 [25600/71432 (36%)]\tLoss: 0.250056\n",
      "Train Epoch: 2 [32000/71432 (45%)]\tLoss: 0.212768\n",
      "Train Epoch: 2 [38400/71432 (54%)]\tLoss: 0.237044\n",
      "Train Epoch: 2 [44800/71432 (63%)]\tLoss: 0.241775\n",
      "Train Epoch: 2 [51200/71432 (72%)]\tLoss: 0.142465\n",
      "Train Epoch: 2 [57600/71432 (81%)]\tLoss: 0.157836\n",
      "Train Epoch: 2 [64000/71432 (90%)]\tLoss: 0.192026\n",
      "Train Epoch: 2 [70400/71432 (98%)]\tLoss: 0.137392\n",
      "\n",
      "Test set: Average loss: 0.1470, Accuracy: 7662/8141 (94%), Positive accuracy: 3926/4091 (96%), Negative accuracy: 3736/4050 (92%), f1 score: 0.9416, Train loss: 0.2447\n",
      "\n",
      "Train Epoch: 3 [0/71432 (0%)]\tLoss: 0.115863\n",
      "Train Epoch: 3 [6400/71432 (9%)]\tLoss: 0.151987\n",
      "Train Epoch: 3 [12800/71432 (18%)]\tLoss: 0.124285\n",
      "Train Epoch: 3 [19200/71432 (27%)]\tLoss: 0.147770\n",
      "Train Epoch: 3 [25600/71432 (36%)]\tLoss: 0.150912\n",
      "Train Epoch: 3 [32000/71432 (45%)]\tLoss: 0.181994\n",
      "Train Epoch: 3 [38400/71432 (54%)]\tLoss: 0.071258\n",
      "Train Epoch: 3 [44800/71432 (63%)]\tLoss: 0.159391\n",
      "Train Epoch: 3 [51200/71432 (72%)]\tLoss: 0.183982\n",
      "Train Epoch: 3 [57600/71432 (81%)]\tLoss: 0.089612\n",
      "Train Epoch: 3 [64000/71432 (90%)]\tLoss: 0.112651\n",
      "Train Epoch: 3 [70400/71432 (98%)]\tLoss: 0.177634\n",
      "\n",
      "Test set: Average loss: 0.1286, Accuracy: 7716/8141 (95%), Positive accuracy: 3872/4091 (95%), Negative accuracy: 3844/4050 (95%), f1 score: 0.9478, Train loss: 0.1606\n",
      "\n",
      "Train Epoch: 4 [0/71432 (0%)]\tLoss: 0.104506\n",
      "Train Epoch: 4 [6400/71432 (9%)]\tLoss: 0.111383\n",
      "Train Epoch: 4 [12800/71432 (18%)]\tLoss: 0.189019\n",
      "Train Epoch: 4 [19200/71432 (27%)]\tLoss: 0.209996\n",
      "Train Epoch: 4 [25600/71432 (36%)]\tLoss: 0.276096\n",
      "Train Epoch: 4 [32000/71432 (45%)]\tLoss: 0.110612\n",
      "Train Epoch: 4 [38400/71432 (54%)]\tLoss: 0.118561\n",
      "Train Epoch: 4 [44800/71432 (63%)]\tLoss: 0.100010\n",
      "Train Epoch: 4 [51200/71432 (72%)]\tLoss: 0.104701\n",
      "Train Epoch: 4 [57600/71432 (81%)]\tLoss: 0.127103\n",
      "Train Epoch: 4 [64000/71432 (90%)]\tLoss: 0.166843\n",
      "Train Epoch: 4 [70400/71432 (98%)]\tLoss: 0.166044\n",
      "\n",
      "Test set: Average loss: 0.1277, Accuracy: 7727/8141 (95%), Positive accuracy: 3811/4091 (93%), Negative accuracy: 3916/4050 (97%), f1 score: 0.9494, Train loss: 0.1484\n",
      "\n",
      "Train Epoch: 5 [0/71432 (0%)]\tLoss: 0.108233\n",
      "Train Epoch: 5 [6400/71432 (9%)]\tLoss: 0.154660\n",
      "Train Epoch: 5 [12800/71432 (18%)]\tLoss: 0.115341\n",
      "Train Epoch: 5 [19200/71432 (27%)]\tLoss: 0.076972\n",
      "Train Epoch: 5 [25600/71432 (36%)]\tLoss: 0.118043\n",
      "Train Epoch: 5 [32000/71432 (45%)]\tLoss: 0.106744\n",
      "Train Epoch: 5 [38400/71432 (54%)]\tLoss: 0.174676\n",
      "Train Epoch: 5 [44800/71432 (63%)]\tLoss: 0.050830\n",
      "Train Epoch: 5 [51200/71432 (72%)]\tLoss: 0.155986\n",
      "Train Epoch: 5 [57600/71432 (81%)]\tLoss: 0.163319\n",
      "Train Epoch: 5 [64000/71432 (90%)]\tLoss: 0.141706\n",
      "Train Epoch: 5 [70400/71432 (98%)]\tLoss: 0.146558\n",
      "\n",
      "Test set: Average loss: 0.1201, Accuracy: 7748/8141 (95%), Positive accuracy: 3903/4091 (95%), Negative accuracy: 3845/4050 (95%), f1 score: 0.9517, Train loss: 0.1417\n",
      "\n",
      "Train Epoch: 6 [0/71432 (0%)]\tLoss: 0.242830\n",
      "Train Epoch: 6 [6400/71432 (9%)]\tLoss: 0.126483\n",
      "Train Epoch: 6 [12800/71432 (18%)]\tLoss: 0.081789\n",
      "Train Epoch: 6 [19200/71432 (27%)]\tLoss: 0.178466\n",
      "Train Epoch: 6 [25600/71432 (36%)]\tLoss: 0.112615\n",
      "Train Epoch: 6 [32000/71432 (45%)]\tLoss: 0.190780\n",
      "Train Epoch: 6 [38400/71432 (54%)]\tLoss: 0.160519\n",
      "Train Epoch: 6 [44800/71432 (63%)]\tLoss: 0.121867\n",
      "Train Epoch: 6 [51200/71432 (72%)]\tLoss: 0.147107\n",
      "Train Epoch: 6 [57600/71432 (81%)]\tLoss: 0.202652\n",
      "Train Epoch: 6 [64000/71432 (90%)]\tLoss: 0.122766\n",
      "Train Epoch: 6 [70400/71432 (98%)]\tLoss: 0.121599\n",
      "\n",
      "Test set: Average loss: 0.1186, Accuracy: 7751/8141 (95%), Positive accuracy: 3883/4091 (95%), Negative accuracy: 3868/4050 (96%), f1 score: 0.9521, Train loss: 0.1410\n",
      "\n",
      "Train Epoch: 7 [0/71432 (0%)]\tLoss: 0.175651\n",
      "Train Epoch: 7 [6400/71432 (9%)]\tLoss: 0.128935\n",
      "Train Epoch: 7 [12800/71432 (18%)]\tLoss: 0.069513\n",
      "Train Epoch: 7 [19200/71432 (27%)]\tLoss: 0.082556\n",
      "Train Epoch: 7 [25600/71432 (36%)]\tLoss: 0.195547\n",
      "Train Epoch: 7 [32000/71432 (45%)]\tLoss: 0.148574\n",
      "Train Epoch: 7 [38400/71432 (54%)]\tLoss: 0.121438\n",
      "Train Epoch: 7 [44800/71432 (63%)]\tLoss: 0.129285\n",
      "Train Epoch: 7 [51200/71432 (72%)]\tLoss: 0.154456\n",
      "Train Epoch: 7 [57600/71432 (81%)]\tLoss: 0.227768\n",
      "Train Epoch: 7 [64000/71432 (90%)]\tLoss: 0.153303\n",
      "Train Epoch: 7 [70400/71432 (98%)]\tLoss: 0.137018\n",
      "\n",
      "Test set: Average loss: 0.1179, Accuracy: 7758/8141 (95%), Positive accuracy: 3890/4091 (95%), Negative accuracy: 3868/4050 (96%), f1 score: 0.9529, Train loss: 0.1386\n",
      "\n",
      "Train Epoch: 8 [0/71432 (0%)]\tLoss: 0.148044\n",
      "Train Epoch: 8 [6400/71432 (9%)]\tLoss: 0.102044\n",
      "Train Epoch: 8 [12800/71432 (18%)]\tLoss: 0.078511\n",
      "Train Epoch: 8 [19200/71432 (27%)]\tLoss: 0.126448\n",
      "Train Epoch: 8 [25600/71432 (36%)]\tLoss: 0.078731\n",
      "Train Epoch: 8 [32000/71432 (45%)]\tLoss: 0.140754\n",
      "Train Epoch: 8 [38400/71432 (54%)]\tLoss: 0.173438\n",
      "Train Epoch: 8 [44800/71432 (63%)]\tLoss: 0.094143\n",
      "Train Epoch: 8 [51200/71432 (72%)]\tLoss: 0.088574\n",
      "Train Epoch: 8 [57600/71432 (81%)]\tLoss: 0.138766\n",
      "Train Epoch: 8 [64000/71432 (90%)]\tLoss: 0.203075\n",
      "Train Epoch: 8 [70400/71432 (98%)]\tLoss: 0.205882\n",
      "\n",
      "Test set: Average loss: 0.1196, Accuracy: 7748/8141 (95%), Positive accuracy: 3864/4091 (94%), Negative accuracy: 3884/4050 (96%), f1 score: 0.9517, Train loss: 0.1369\n",
      "\n",
      "Train Epoch: 9 [0/71432 (0%)]\tLoss: 0.181974\n",
      "Train Epoch: 9 [6400/71432 (9%)]\tLoss: 0.395076\n",
      "Train Epoch: 9 [12800/71432 (18%)]\tLoss: 0.176565\n",
      "Train Epoch: 9 [19200/71432 (27%)]\tLoss: 0.102381\n",
      "Train Epoch: 9 [25600/71432 (36%)]\tLoss: 0.208083\n",
      "Train Epoch: 9 [32000/71432 (45%)]\tLoss: 0.107434\n",
      "Train Epoch: 9 [38400/71432 (54%)]\tLoss: 0.148754\n",
      "Train Epoch: 9 [44800/71432 (63%)]\tLoss: 0.152815\n",
      "Train Epoch: 9 [51200/71432 (72%)]\tLoss: 0.148400\n",
      "Train Epoch: 9 [57600/71432 (81%)]\tLoss: 0.151803\n",
      "Train Epoch: 9 [64000/71432 (90%)]\tLoss: 0.065334\n",
      "Train Epoch: 9 [70400/71432 (98%)]\tLoss: 0.104367\n",
      "\n",
      "Test set: Average loss: 0.1180, Accuracy: 7759/8141 (95%), Positive accuracy: 3892/4091 (95%), Negative accuracy: 3867/4050 (95%), f1 score: 0.9531, Train loss: 0.1381\n",
      "\n",
      "Train Epoch: 10 [0/71432 (0%)]\tLoss: 0.091237\n",
      "Train Epoch: 10 [6400/71432 (9%)]\tLoss: 0.154971\n",
      "Train Epoch: 10 [12800/71432 (18%)]\tLoss: 0.151702\n",
      "Train Epoch: 10 [19200/71432 (27%)]\tLoss: 0.095162\n",
      "Train Epoch: 10 [25600/71432 (36%)]\tLoss: 0.065879\n",
      "Train Epoch: 10 [32000/71432 (45%)]\tLoss: 0.194076\n",
      "Train Epoch: 10 [38400/71432 (54%)]\tLoss: 0.248538\n",
      "Train Epoch: 10 [44800/71432 (63%)]\tLoss: 0.128466\n",
      "Train Epoch: 10 [51200/71432 (72%)]\tLoss: 0.146634\n",
      "Train Epoch: 10 [57600/71432 (81%)]\tLoss: 0.118423\n",
      "Train Epoch: 10 [64000/71432 (90%)]\tLoss: 0.098908\n",
      "Train Epoch: 10 [70400/71432 (98%)]\tLoss: 0.091219\n",
      "\n",
      "Test set: Average loss: 0.1175, Accuracy: 7759/8141 (95%), Positive accuracy: 3892/4091 (95%), Negative accuracy: 3867/4050 (95%), f1 score: 0.9531, Train loss: 0.1356\n",
      "\n",
      "Train Epoch: 11 [0/71432 (0%)]\tLoss: 0.129147\n",
      "Train Epoch: 11 [6400/71432 (9%)]\tLoss: 0.115831\n",
      "Train Epoch: 11 [12800/71432 (18%)]\tLoss: 0.048753\n",
      "Train Epoch: 11 [19200/71432 (27%)]\tLoss: 0.270948\n",
      "Train Epoch: 11 [25600/71432 (36%)]\tLoss: 0.134912\n",
      "Train Epoch: 11 [32000/71432 (45%)]\tLoss: 0.174958\n",
      "Train Epoch: 11 [38400/71432 (54%)]\tLoss: 0.160644\n",
      "Train Epoch: 11 [44800/71432 (63%)]\tLoss: 0.133561\n",
      "Train Epoch: 11 [51200/71432 (72%)]\tLoss: 0.122680\n",
      "Train Epoch: 11 [57600/71432 (81%)]\tLoss: 0.080557\n",
      "Train Epoch: 11 [64000/71432 (90%)]\tLoss: 0.271946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [70400/71432 (98%)]\tLoss: 0.150002\n",
      "\n",
      "Test set: Average loss: 0.1182, Accuracy: 7753/8141 (95%), Positive accuracy: 3876/4091 (95%), Negative accuracy: 3877/4050 (96%), f1 score: 0.9523, Train loss: 0.1366\n",
      "\n",
      "Train Epoch: 12 [0/71432 (0%)]\tLoss: 0.095757\n",
      "Train Epoch: 12 [6400/71432 (9%)]\tLoss: 0.087062\n",
      "Train Epoch: 12 [12800/71432 (18%)]\tLoss: 0.054357\n",
      "Train Epoch: 12 [19200/71432 (27%)]\tLoss: 0.178713\n",
      "Train Epoch: 12 [25600/71432 (36%)]\tLoss: 0.216402\n",
      "Train Epoch: 12 [32000/71432 (45%)]\tLoss: 0.073902\n",
      "Train Epoch: 12 [38400/71432 (54%)]\tLoss: 0.141404\n",
      "Train Epoch: 12 [44800/71432 (63%)]\tLoss: 0.185083\n",
      "Train Epoch: 12 [51200/71432 (72%)]\tLoss: 0.179244\n",
      "Train Epoch: 12 [57600/71432 (81%)]\tLoss: 0.098940\n",
      "Train Epoch: 12 [64000/71432 (90%)]\tLoss: 0.151811\n",
      "Train Epoch: 12 [70400/71432 (98%)]\tLoss: 0.073463\n",
      "\n",
      "Test set: Average loss: 0.1192, Accuracy: 7746/8141 (95%), Positive accuracy: 3861/4091 (94%), Negative accuracy: 3885/4050 (96%), f1 score: 0.9515, Train loss: 0.1364\n",
      "\n",
      "Train Epoch: 13 [0/71432 (0%)]\tLoss: 0.136108\n",
      "Train Epoch: 13 [6400/71432 (9%)]\tLoss: 0.180498\n",
      "Train Epoch: 13 [12800/71432 (18%)]\tLoss: 0.113093\n",
      "Train Epoch: 13 [19200/71432 (27%)]\tLoss: 0.158720\n",
      "Train Epoch: 13 [25600/71432 (36%)]\tLoss: 0.056105\n",
      "Train Epoch: 13 [32000/71432 (45%)]\tLoss: 0.140744\n",
      "Train Epoch: 13 [38400/71432 (54%)]\tLoss: 0.103274\n",
      "Train Epoch: 13 [44800/71432 (63%)]\tLoss: 0.226260\n",
      "Train Epoch: 13 [51200/71432 (72%)]\tLoss: 0.073560\n",
      "Train Epoch: 13 [57600/71432 (81%)]\tLoss: 0.091134\n",
      "Train Epoch: 13 [64000/71432 (90%)]\tLoss: 0.127875\n",
      "Train Epoch: 13 [70400/71432 (98%)]\tLoss: 0.177573\n",
      "\n",
      "Test set: Average loss: 0.1179, Accuracy: 7761/8141 (95%), Positive accuracy: 3888/4091 (95%), Negative accuracy: 3873/4050 (96%), f1 score: 0.9533, Train loss: 0.1359\n",
      "\n",
      "Train Epoch: 14 [0/71432 (0%)]\tLoss: 0.129438\n",
      "Train Epoch: 14 [6400/71432 (9%)]\tLoss: 0.130128\n",
      "Train Epoch: 14 [12800/71432 (18%)]\tLoss: 0.200594\n",
      "Train Epoch: 14 [19200/71432 (27%)]\tLoss: 0.192493\n",
      "Train Epoch: 14 [25600/71432 (36%)]\tLoss: 0.164397\n",
      "Train Epoch: 14 [32000/71432 (45%)]\tLoss: 0.062227\n",
      "Train Epoch: 14 [38400/71432 (54%)]\tLoss: 0.157084\n",
      "Train Epoch: 14 [44800/71432 (63%)]\tLoss: 0.152892\n",
      "Train Epoch: 14 [51200/71432 (72%)]\tLoss: 0.137447\n",
      "Train Epoch: 14 [57600/71432 (81%)]\tLoss: 0.200480\n",
      "Train Epoch: 14 [64000/71432 (90%)]\tLoss: 0.151265\n",
      "Train Epoch: 14 [70400/71432 (98%)]\tLoss: 0.157580\n",
      "\n",
      "Test set: Average loss: 0.1181, Accuracy: 7753/8141 (95%), Positive accuracy: 3876/4091 (95%), Negative accuracy: 3877/4050 (96%), f1 score: 0.9523, Train loss: 0.1356\n",
      "\n",
      "Train Epoch: 15 [0/71432 (0%)]\tLoss: 0.159277\n",
      "Train Epoch: 15 [6400/71432 (9%)]\tLoss: 0.051979\n",
      "Train Epoch: 15 [12800/71432 (18%)]\tLoss: 0.087838\n",
      "Train Epoch: 15 [19200/71432 (27%)]\tLoss: 0.156636\n",
      "Train Epoch: 15 [25600/71432 (36%)]\tLoss: 0.098722\n",
      "Train Epoch: 15 [32000/71432 (45%)]\tLoss: 0.117296\n",
      "Train Epoch: 15 [38400/71432 (54%)]\tLoss: 0.054884\n",
      "Train Epoch: 15 [44800/71432 (63%)]\tLoss: 0.179438\n",
      "Train Epoch: 15 [51200/71432 (72%)]\tLoss: 0.083850\n",
      "Train Epoch: 15 [57600/71432 (81%)]\tLoss: 0.119676\n",
      "Train Epoch: 15 [64000/71432 (90%)]\tLoss: 0.180205\n",
      "Train Epoch: 15 [70400/71432 (98%)]\tLoss: 0.136525\n",
      "\n",
      "Test set: Average loss: 0.1175, Accuracy: 7760/8141 (95%), Positive accuracy: 3890/4091 (95%), Negative accuracy: 3870/4050 (96%), f1 score: 0.9532, Train loss: 0.1368\n",
      "\n",
      "Train Epoch: 16 [0/71432 (0%)]\tLoss: 0.218062\n",
      "Train Epoch: 16 [6400/71432 (9%)]\tLoss: 0.161739\n",
      "Train Epoch: 16 [12800/71432 (18%)]\tLoss: 0.108264\n",
      "Train Epoch: 16 [19200/71432 (27%)]\tLoss: 0.027026\n",
      "Train Epoch: 16 [25600/71432 (36%)]\tLoss: 0.207318\n",
      "Train Epoch: 16 [32000/71432 (45%)]\tLoss: 0.154826\n",
      "Train Epoch: 16 [38400/71432 (54%)]\tLoss: 0.163696\n",
      "Train Epoch: 16 [44800/71432 (63%)]\tLoss: 0.043278\n",
      "Train Epoch: 16 [51200/71432 (72%)]\tLoss: 0.066706\n",
      "Train Epoch: 16 [57600/71432 (81%)]\tLoss: 0.106392\n",
      "Train Epoch: 16 [64000/71432 (90%)]\tLoss: 0.139189\n",
      "Train Epoch: 16 [70400/71432 (98%)]\tLoss: 0.112713\n",
      "\n",
      "Test set: Average loss: 0.1178, Accuracy: 7758/8141 (95%), Positive accuracy: 3884/4091 (95%), Negative accuracy: 3874/4050 (96%), f1 score: 0.9529, Train loss: 0.1356\n",
      "\n",
      "Train Epoch: 17 [0/71432 (0%)]\tLoss: 0.113888\n",
      "Train Epoch: 17 [6400/71432 (9%)]\tLoss: 0.136599\n",
      "Train Epoch: 17 [12800/71432 (18%)]\tLoss: 0.171526\n",
      "Train Epoch: 17 [19200/71432 (27%)]\tLoss: 0.081305\n",
      "Train Epoch: 17 [25600/71432 (36%)]\tLoss: 0.222466\n",
      "Train Epoch: 17 [32000/71432 (45%)]\tLoss: 0.163328\n",
      "Train Epoch: 17 [38400/71432 (54%)]\tLoss: 0.136237\n",
      "Train Epoch: 17 [44800/71432 (63%)]\tLoss: 0.170806\n",
      "Train Epoch: 17 [51200/71432 (72%)]\tLoss: 0.093356\n",
      "Train Epoch: 17 [57600/71432 (81%)]\tLoss: 0.104392\n",
      "Train Epoch: 17 [64000/71432 (90%)]\tLoss: 0.115690\n",
      "Train Epoch: 17 [70400/71432 (98%)]\tLoss: 0.143813\n",
      "\n",
      "Test set: Average loss: 0.1184, Accuracy: 7748/8141 (95%), Positive accuracy: 3868/4091 (95%), Negative accuracy: 3880/4050 (96%), f1 score: 0.9517, Train loss: 0.1359\n",
      "\n",
      "Train Epoch: 18 [0/71432 (0%)]\tLoss: 0.164745\n",
      "Train Epoch: 18 [6400/71432 (9%)]\tLoss: 0.201513\n",
      "Train Epoch: 18 [12800/71432 (18%)]\tLoss: 0.260859\n",
      "Train Epoch: 18 [19200/71432 (27%)]\tLoss: 0.137848\n",
      "Train Epoch: 18 [25600/71432 (36%)]\tLoss: 0.169054\n",
      "Train Epoch: 18 [32000/71432 (45%)]\tLoss: 0.176174\n",
      "Train Epoch: 18 [38400/71432 (54%)]\tLoss: 0.123624\n",
      "Train Epoch: 18 [44800/71432 (63%)]\tLoss: 0.148382\n",
      "Train Epoch: 18 [51200/71432 (72%)]\tLoss: 0.069301\n",
      "Train Epoch: 18 [57600/71432 (81%)]\tLoss: 0.095589\n",
      "Train Epoch: 18 [64000/71432 (90%)]\tLoss: 0.248083\n",
      "Train Epoch: 18 [70400/71432 (98%)]\tLoss: 0.102457\n",
      "\n",
      "Test set: Average loss: 0.1181, Accuracy: 7755/8141 (95%), Positive accuracy: 3876/4091 (95%), Negative accuracy: 3879/4050 (96%), f1 score: 0.9526, Train loss: 0.1347\n",
      "\n",
      "Train Epoch: 19 [0/71432 (0%)]\tLoss: 0.090915\n",
      "Train Epoch: 19 [6400/71432 (9%)]\tLoss: 0.152030\n",
      "Train Epoch: 19 [12800/71432 (18%)]\tLoss: 0.073032\n",
      "Train Epoch: 19 [19200/71432 (27%)]\tLoss: 0.112364\n",
      "Train Epoch: 19 [25600/71432 (36%)]\tLoss: 0.119500\n",
      "Train Epoch: 19 [32000/71432 (45%)]\tLoss: 0.050380\n",
      "Train Epoch: 19 [38400/71432 (54%)]\tLoss: 0.173245\n",
      "Train Epoch: 19 [44800/71432 (63%)]\tLoss: 0.160533\n",
      "Train Epoch: 19 [51200/71432 (72%)]\tLoss: 0.209896\n",
      "Train Epoch: 19 [57600/71432 (81%)]\tLoss: 0.146953\n",
      "Train Epoch: 19 [64000/71432 (90%)]\tLoss: 0.051926\n",
      "Train Epoch: 19 [70400/71432 (98%)]\tLoss: 0.145201\n",
      "\n",
      "Test set: Average loss: 0.1180, Accuracy: 7754/8141 (95%), Positive accuracy: 3876/4091 (95%), Negative accuracy: 3878/4050 (96%), f1 score: 0.9525, Train loss: 0.1357\n",
      "\n",
      "Train Epoch: 20 [0/71432 (0%)]\tLoss: 0.096634\n",
      "Train Epoch: 20 [6400/71432 (9%)]\tLoss: 0.115606\n",
      "Train Epoch: 20 [12800/71432 (18%)]\tLoss: 0.089061\n",
      "Train Epoch: 20 [19200/71432 (27%)]\tLoss: 0.133923\n",
      "Train Epoch: 20 [25600/71432 (36%)]\tLoss: 0.175875\n",
      "Train Epoch: 20 [32000/71432 (45%)]\tLoss: 0.182372\n",
      "Train Epoch: 20 [38400/71432 (54%)]\tLoss: 0.085683\n",
      "Train Epoch: 20 [44800/71432 (63%)]\tLoss: 0.089595\n",
      "Train Epoch: 20 [51200/71432 (72%)]\tLoss: 0.188211\n",
      "Train Epoch: 20 [57600/71432 (81%)]\tLoss: 0.140887\n",
      "Train Epoch: 20 [64000/71432 (90%)]\tLoss: 0.165253\n",
      "Train Epoch: 20 [70400/71432 (98%)]\tLoss: 0.186826\n",
      "\n",
      "Test set: Average loss: 0.1179, Accuracy: 7756/8141 (95%), Positive accuracy: 3879/4091 (95%), Negative accuracy: 3877/4050 (96%), f1 score: 0.9527, Train loss: 0.1351\n",
      "\n",
      "[7186, 7662, 7716, 7727, 7748, 7751, 7758, 7748, 7759, 7759, 7753, 7746, 7761, 7753, 7760, 7758, 7748, 7755, 7754, 7756]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD5CAYAAAAp8/5SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3hV1Z3/8fc3CRCDgAjxwqWGpFTlJmAIdphWqRZRWqA/2z506lTbTq3Pr/bmjCNOO7Zj69RWqw4dWod2mHZqq7X6a0vHtKCVi+2USqCoKFouooAXAgqCgJDk+/tjnWMO4SQ595Ps83k9z37Ovq2zv9k5+Z6dtdda29wdERGJrrJiByAiIvmlRC8iEnFK9CIiEadELyIScUr0IiIRp0QvIhJxFansZGYzgX8DyoEfuPstnex3GXA/MMXdm8ysBtgIPBvbZbW7X93VsYYOHeo1NTUpBS8iIsHatWt3u3t1sm3dJnozKwcWAu8FdgBrzGyJuz/dYb8BwOeBP3V4iy3uPjHVYGtqamhqakp1dxERAczs+c62pVJ10wBsdvet7n4EuBeYk2S/rwHfBA5nFKWIiORFKol+OLA9YXlHbN1bzGwyMNLdH0xSfpSZ/dnMVprZu5IdwMyuMrMmM2tqbm5ONXYREUlB1jdjzawMuB34+ySbXwLe5u6TgGuBn5rZwI47ufsid6939/rq6qRVTCIikqFUbsbuBEYmLI+IrYsbAIwDVpgZwGnAEjOb7e5NwJsA7r7WzLYA7wBUCS9Sgo4ePcqOHTs4fFg1vJmqrKxkxIgR9OnTJ+UyqST6NcBoMxtFSPDzgL+Jb3T3fcDQ+LKZrQD+Idbqphp41d1bzawWGA1sTTk6EYmUHTt2MGDAAGpqaohdGEoa3J09e/awY8cORo0alXK5bqtu3L0FuAZYSmgqeZ+7P2VmN5nZ7G6Kvxt4wszWE5pdXu3ur6YcnYhEyuHDhxkyZIiSfIbMjCFDhqT9H1FK7ejdvRFo7LDuxk72vSBh/gHggbQiEpFIU5LPTibnLzI9Y/fuhZtugjVrih2JiEjPEplEbwZf+QqsWFHsSESkp9q7dy/f/e53Myp76aWXsnfv3pT3/+pXv8ptt92W0bFyLTKJftAgGDIEtmwpdiQi0lN1lehbWlq6LNvY2MhJJ52Uj7DyLjKJHqC2FraqTY+IdGL+/Pls2bKFiRMnct1117FixQre9a53MXv2bMaMGQPA3LlzOffccxk7diyLFi16q2xNTQ27d+9m27ZtnH322XzqU59i7NixzJgxg0OHDnV53PXr13PeeecxYcIEPvCBD/Daa68BsGDBAsaMGcOECROYN28eACtXrmTixIlMnDiRSZMmsX///qx/7pRuxvYWtbWgYXJEeocvfAHWr8/te06cCHfe2fn2W265hQ0bNrA+duAVK1awbt06NmzY8FZzxcWLF3PyySdz6NAhpkyZwmWXXcaQIUOOeZ9NmzZxzz338P3vf58Pf/jDPPDAA1x++eWdHvdjH/sY3/nOdzj//PO58cYb+Zd/+RfuvPNObrnlFp577jn69ev3VrXQbbfdxsKFC5k2bRoHDhygsrIyy7MSsSv6ujp4/nno5j8wEZG3NDQ0HNMmfcGCBZxzzjmcd955bN++nU2bNh1XZtSoUUycGMZqPPfcc9m2bVun779v3z727t3L+eefD8AVV1zBqlWrAJgwYQIf/ehHufvuu6moCNfd06ZN49prr2XBggXs3bv3rfXZiNwVfUsLbN8OafQlEJEi6OrKu5D69+//1vyKFSt4+OGH+eMf/0hVVRUXXHBB0jbr/fr1e2u+vLy826qbzjz44IOsWrWKX//619x88808+eSTzJ8/n1mzZtHY2Mi0adNYunQpZ511VkbvHxepK/ra2vCqenoRSWbAgAFd1nnv27ePwYMHU1VVxTPPPMPq1auzPuagQYMYPHgwjz76KAA//vGPOf/882lra2P79u1Mnz6db37zm+zbt48DBw6wZcsWxo8fz/XXX8+UKVN45plnso4hUlf0dXXhdcsWuPDC4sYiIj3PkCFDmDZtGuPGjeOSSy5h1qxZx2yfOXMmd911F2effTZnnnkm5513Xk6O+6Mf/Yirr76agwcPUltby3/913/R2trK5Zdfzr59+3B3Pve5z3HSSSfxz//8zyxfvpyysjLGjh3LJZdckvXxzd1z8GPkTn19vWf64JHWVjjhBLj2Wrgl6TOwRKSYNm7cyNlnn13sMHq9ZOfRzNa6e32y/SNVdVNeDjU1qroREUkUqUQPofpGnaZERNpFLtGr05SIyLEimej37oVYxzMRkZIXuUSf2PJGREQimOjVll5E5FiRS/TxHrFK9CLSUTbDFAPceeedHDx4MOm2Cy64gEybhudb5BL9gAFwyimquhGR4+Uz0fdkkUv0oJY3IpJcx2GKAW699VamTJnChAkT+MpXvgLAG2+8waxZszjnnHMYN24cP/vZz1iwYAEvvvgi06dPZ/r06V0e55577mH8+PGMGzeO66+/HoDW1lauvPJKxo0bx/jx47njjjuA5EMV51qkhkCIq62F//3fYkchIl0qwjjFHYcpXrZsGZs2beKxxx7D3Zk9ezarVq2iubmZYcOG8eCDDwJhDJxBgwZx++23s3z5coYOHdrpMV588UWuv/561q5dy+DBg5kxYwa//OUvGTlyJDt37mTDhg0Abw1LnGyo4lyL5BV9XR288AIcOVLsSESkJ1u2bBnLli1j0qRJTJ48mWeeeYZNmzYxfvx4HnroIa6//noeffRRBg0alPJ7rlmzhgsuuIDq6moqKir46Ec/yqpVq6itrWXr1q189rOf5be//S0DBw4Ekg9VnGuRvaJvawvJ/u1vL3Y0IpJUDxin2N254YYb+PSnP33ctnXr1tHY2MiXv/xlLrzwQm688casjjV48GAef/xxli5dyl133cV9993H4sWLkw5VnOuEn9IVvZnNNLNnzWyzmc3vYr/LzMzNrD5h3Q2xcs+a2cW5CLo7amIpIsl0HKb44osvZvHixRw4cACAnTt3smvXLl588UWqqqq4/PLLue6661i3bl3S8sk0NDSwcuVKdu/eTWtrK/fccw/nn38+u3fvpq2tjcsuu4yvf/3rrFu3rtOhinOt268NMysHFgLvBXYAa8xsibs/3WG/AcDngT8lrBsDzAPGAsOAh83sHe7emrsf4XjqNCUiyXQcpvjWW29l48aNvPOd7wTgxBNP5O6772bz5s1cd911lJWV0adPH773ve8BcNVVVzFz5kyGDRvG8uXLkx7j9NNP55ZbbmH69Om4O7NmzWLOnDk8/vjjfPzjH6etrQ2Ab3zjG50OVZxr3Q5TbGbvBL7q7hfHlm8AcPdvdNjvTuAh4DrgH9y9qeO+ZrY09l5/7Ox42QxTHNfWBlVV8NnPwq23ZvVWIpJDGqY4N/IxTPFwYHvC8o7YusQDTAZGuvuD6ZaNlb/KzJrMrKm5uTmFkLpWVhY6TqnqRkQkB61uzKwMuB34+0zfw90XuXu9u9dXV1dnGxKg4YpFROJSSfQ7gZEJyyNi6+IGAOOAFWa2DTgPWBK7Idtd2byJd5rqYQ/QEil5Pe2pdr1NJucvlUS/BhhtZqPMrC/h5uqShIPuc/eh7l7j7jXAamC2uzfF9ptnZv3MbBQwGngs7SgzUFcH+/fD7t2FOJqIpKKyspI9e/Yo2WfI3dmzZw+VlZVpleu21Y27t5jZNcBSoBxY7O5PmdlNQJO7L+mi7FNmdh/wNNACfCbfLW7iEptY5qg2SESyNGLECHbs2EEu7sWVqsrKSkaMGJFWmZRa5bt7I9DYYV3S3gPufkGH5ZuBm9OKKgcSE/3UqYU+uogk06dPH0bFh5iVgonkEAjQPlyxbsiKSKmLbKKvqoLTT1cTSxGRyCZ60HDFIiIQ8USvtvQiIhFP9LW1sHMnHD5c7EhERIon8oneHZ5/vtiRiIgUT6QTvUaxFBGJeKLXuPQiIhFP9KeeGppZKtGLSCmLdKI3C1f1qroRkVIW6UQPaksvIlIyiV6D5YlIqYp8oq+rg4MH4ZVXih2JiEhxRD7Rq+WNiJQ6JXoRkYiLfKKvqQmtb9TyRkRKVeQTfWUlDB+uK3oRKV2RT/SgJpYiUtpKItFruGIRKWUlkehra+Gll0IzSxGRUlMyiR5g27aihiEiUhQlkeg1XLGIlLKUEr2ZzTSzZ81ss5nNT7L9ajN70szWm9nvzWxMbH2NmR2KrV9vZnfl+gdIhdrSi0gpq+huBzMrBxYC7wV2AGvMbIm7P52w20/d/a7Y/rOB24GZsW1b3H1ibsNOz9ChMGCAEr2IlKZUrugbgM3uvtXdjwD3AnMSd3D31xMW+wM9aggxDVcsIqUslUQ/HNiesLwjtu4YZvYZM9sCfAv4XMKmUWb2ZzNbaWbvSnYAM7vKzJrMrKm5uTmN8FOntvQiUqpydjPW3Re6ex1wPfDl2OqXgLe5+yTgWuCnZjYwSdlF7l7v7vXV1dW5CukYdXXw3HPQ1paXtxcR6bFSSfQ7gZEJyyNi6zpzLzAXwN3fdPc9sfm1wBbgHZmFmp3aWjh8OLSnFxEpJakk+jXAaDMbZWZ9gXnAksQdzGx0wuIsYFNsfXXsZi5mVguMBopSgaKWNyJSqrptdePuLWZ2DbAUKAcWu/tTZnYT0OTuS4BrzOwi4CjwGnBFrPi7gZvM7CjQBlzt7q/m4wfpTrwt/dat8K6kdwpERKKp20QP4O6NQGOHdTcmzH++k3IPAA9kE2CuvO1tUFamljciUnpKomcsQN++MHKkqm5EpPSUTKKHUH2jRC8ipaakEr06TYlIKSq5RL9rFxw4UOxIREQKp6QSfbzlzXPPFTcOEZFCKqlEH29Lr+obESklJZnodUNWREpJSSX6k0+Gk05SoheR0lJSiR7U8kZESk9JJnpd0YtIKSm5RF9XFx4S3tpa7EhERAqj5BJ9bS0cOQI7uxpoWUQkQkoy0YOqb0SkdJRcok8crlhEpBSUXKIfORLKy9XyRkRKR8kl+ooKOOMMXdGLSOkouUQPGq5YREpLSSZ6dZoSkVJSkom+rg727IF9+4odiYhI/pVkoo83sdRwxSJSCko60av6RkRKQUqJ3sxmmtmzZrbZzOYn2X61mT1pZuvN7PdmNiZh2w2xcs+a2cW5DD5T6jQlIqWk20RvZuXAQuASYAzwkcREHvNTdx/v7hOBbwG3x8qOAeYBY4GZwHdj71dUgwbBkCFK9CJSGlK5om8ANrv7Vnc/AtwLzEncwd1fT1jsD3hsfg5wr7u/6e7PAZtj71d0ankjIqUilUQ/HNiesLwjtu4YZvYZM9tCuKL/XDpli0Ft6UWkVOTsZqy7L3T3OuB64MvplDWzq8ysycyampubcxVSl2pr4fnnoaWlIIcTESmaVBL9TmBkwvKI2LrO3AvMTaesuy9y93p3r6+urk4hpOzV1oYkv3179/uKiPRmqST6NcBoMxtlZn0JN1eXJO5gZqMTFmcBm2LzS4B5ZtbPzEYBo4HHsg87exrFUkRKRUV3O7h7i5ldAywFyoHF7v6Umd0ENLn7EuAaM7sIOAq8BlwRK/uUmd0HPA20AJ9x9x7xbKfEtvQXXljcWERE8qnbRA/g7o1AY4d1NybMf76LsjcDN2caYL4MHw59+uiKXkSiryR7xkIYk37UKCV6EYm+kk30oLb0IlIaSj7R64peRKKupBN9XR3s3QuvvVbsSERE8qekE71GsRSRUqBEj6pvRCTalOhRoheRaCvpRH/iiXDKKaq6EZFoK+lED2p5IyLRV/KJXsMVi0jUlXyir62FF16AI0eKHYmISH4o0ddCW1tI9iIiURSdRL97N/zjP8Lq1WkV03DFIhJ1KY1e2Sv06we33Qb9+8N556VcTJ2mRCTqonNFP2AAjB0Lf/pTWsVOPz18R+iKXkSiKjqJHqChAR57DNxTLlJWpiaWIhJt0Uv0e/bAc8+lVUzDFYtIlEUv0UO4qk9DvC19Gv8IiIj0GtFK9OPGwQknpF1PX1sL+/eHfwZERKImWom+Tx+YPDntK3q1vBGRKItWoodQfbNuHRw9mnIRtaUXkSiLZqI/fBg2bEi5SE1NeFWiF5EoSinRm9lMM3vWzDab2fwk2681s6fN7Akz+52ZnZGwrdXM1semJbkMPqmpU8NrGvX0VVWhPb2qbkQkirpN9GZWDiwELgHGAB8xszEddvszUO/uE4D7gW8lbDvk7hNj0+wcxd25mhoYOjTjljciIlGTyhV9A7DZ3be6+xHgXmBO4g7uvtzdD8YWVwMjchtmGszaO06lQZ2mRCSqUkn0w4HtCcs7Yus680ngNwnLlWbWZGarzWxusgJmdlVsn6bm5uYUQupGQwM8/XRoM5mi2lrYsQPefDP7w4uI9CQ5vRlrZpcD9cCtCavPcPd64G+AO82srmM5d1/k7vXuXl9dXZ19IFOnht5PTU0pF6mrC0W2bcv+8CIiPUkqiX4nMDJheURs3THM7CLgS8Bsd3/rutjdd8ZetwIrgElZxJuaKVPCaxrVN3pQuIhEVSqJfg0w2sxGmVlfYB5wTOsZM5sE/Achye9KWD/YzPrF5ocC04CncxV8p4YMCZfoaST6eFv6jRvzFJOISJF0m+jdvQW4BlgKbATuc/enzOwmM4u3orkVOBH4eYdmlGcDTWb2OLAcuMXd85/oIe0bsqeeCmedBb/5Tff7ioj0Jik9eMTdG4HGDutuTJi/qJNy/wuMzybAjE2dCvfcAy++CMOGpVRkzhz49rdh71446aQ8xyciUiDR6xkbl8FIlnPnQksLNDZ2v6+ISG8R3UQ/cSJUVKSV6Bsa4LTT4Fe/ymNcIiIFFt1Ef8IJMGFCWom+rAze//5QT6/29CISFdFN9BDq6desgba2lIvMmRP6WS1fnse4REQKKNqJvqEBXn8dnn025SIXXgj9+6v6RkSiI/qJHtKqvqmshJkzYcmStP4REBHpsaKd6M88EwYMSHuAszlzQqvMtWvzFJeISAFFO9GXl4fhENJ8huysWaHoL3+Zp7hERAoo2okeQvXN44+Hp06l6OST4d3vVj29iERDaST6lhZYvz6tYnPmwFNPwebNeYpLRKRASiPRQ0b19KCrehHp/aKf6IcPD1Oa9fQ1NXDOOUr0ItL7RT/RQ0aPFoRwVf+HP8Du3XmISUSkQEon0W/eDK++mlaxOXNCW/r/+Z88xSUiUgClk+ghDIeQhkmTYORINbMUkd6tNBJ9fT2YpV1Pbxau6pctg4MH8xSbiEielUaiHzgQzj4743r6Q4fg4YfzEJeISAGURqKH9huy7mkVO/98GDRI1Tci0nuVVqJvbobnn0+rWJ8+cOml4YZsa2ueYhMRyaPSSfRTp4bXNOvpITxisLkZ/vjHHMckIlIApZPox4+Hfv0yqqefOTNc2av6RkR6o9JJ9H36wOTJGSX6gQPhPe8JvWTTrOIXESm6lBK9mc00s2fNbLOZzU+y/Voze9rMnjCz35nZGQnbrjCzTbHpilwGn7aGhjDIfEtL2kXnzg19rjZuzENcIiJ51G2iN7NyYCFwCTAG+IiZjemw25+BenefANwPfCtW9mTgK8BUoAH4ipkNzl34aZo6NbSV3LAh7aKzZ4dXjX0jIr1NKlf0DcBmd9/q7keAe4E5iTu4+3J3j3cpWg2MiM1fDDzk7q+6+2vAQ8DM3ISegQxHsgQYNiw8w0T19CLS26SS6IcD2xOWd8TWdeaTwG/SKWtmV5lZk5k1NTc3pxBShmprw1NFMkj0EDpPPfZYeMygiEhvkdObsWZ2OVAP3JpOOXdf5O717l5fXV2dy5COZZbxSJYQ6ukBfv3rHMYkIpJnqST6ncDIhOURsXXHMLOLgC8Bs939zXTKFtTUqeHRUQcOpF10zBioq1P1jYj0Lqkk+jXAaDMbZWZ9gXnAksQdzGwS8B+EJL8rYdNSYIaZDY7dhJ0RW1c8DQ1h7OG1a9MuGh/k7JFHYP/+PMQmIpIH3SZ6d28BriEk6I3Afe7+lJndZGaxtijcCpwI/NzM1pvZkljZV4GvEb4s1gA3xdYVz5Qp4TWLevojR+C3v81hTCIieWTew3oA1dfXe1NTU34PUlsL554LP/952kVbWuD00+Hii+Huu/MQm4hIBsxsrbvXJ9tWOj1jE02dmtGYNwAVFfC+98GDD8LRozmOS0QkD0oz0Tc0wPbt8NJLGRWfMwf27oVVq3Icl4hIHpRuooe0Hy0Y9973QmWlesmKSO9Qmol+0iQoL8/4hmz//jBjRmhm2cNucYiIHKc0E31VFUyYkHGih1B9s307rF+fw7hERPKgNBM9tPeQbWvLqPj73hfa1av6RkR6utJO9Pv2waZNGRU/5RSYNk2JXkR6vtJO9JB19c369bBtW25CEhHJh9JN9GefDSeemHWiB1iypOv9RESKqXQTfXk51Ndn3HEKYPTo8H2h6hsR6clKN9FDqL5Zvx7efLP7fTsxdy6sXAmvFncEHxGRTinRHz0Kjz+e8VvMmQOtrdDYmMO4RERyqLQT/dSp4TWLevopU8IgZ6q+EZGeqrQT/fDhIUtnUU9fVgbvf38Ytvjw4RzGJiKSI6Wd6LN8tGDc3LnhgVWPPJKjuEREcqi0Ez2ERP+Xv8Brr2X8Fu95T2ipqeobEemJlOjj9fRZPOykXz+YOTO0p89wRAURkbxRoq+PPZAli3p6CNU3L78Mv/99DmISEckhJfpBg+Css7Kup7/00vBW730vfOpToTZIRKQnUKKH9huyWQwuP3gw/PnP8Hd/F54le9ZZ8KEPZVUjJCKSE0r0EOrpX3klDDCfhVGjYOHCMMjZDTfAQw+FdvYXXQQPP6yHlIhIcSjRQ/tIllnW08edeircfDO88AJ861vw1FOhSmfKFLj//tCTVkSkUFJK9GY208yeNbPNZjY/yfZ3m9k6M2sxsw922NZqZutjU88c53HCBOjbN+t6+o4GDoTrroPnnoNFi8Lw9x/6UBgI7Qc/yGqIHRGRlHWb6M2sHFgIXAKMAT5iZmM67PYCcCXw0yRvccjdJ8am2VnGmx99+4bnyOY40cdVVoYbtM88A/fdBwMGhOVRo+C222D//rwcVkQESO2KvgHY7O5b3f0IcC8wJ3EHd9/m7k8AvbcV+dSp4c5pS0veDlFe3n6D9qGHYMyYcMX/trfBl78Mu3bl7dAiUsJSSfTDgcS7lDti61JVaWZNZrbazOYm28HMrort09Tc3JzGW+dQQwMcPAhPP533Q5m136B97DG48EL413+FM84I7fEXLAj1+rp5KyK5UIibsWe4ez3wN8CdZlbXcQd3X+Tu9e5eX11dXYCQkojfkL3jjoKOTha/QbtxI3ziE/Dkk/D5z8O4cXDaafCRj4T6/K1bCxaSiERMKol+JzAyYXlEbF1K3H1n7HUrsAKYlEZ8hfP2t8O118IPfxiS/pNPFvTwZ54ZmmZu2RJu3i5eHFrqrFwZ6vPr6kKd/ic/CT/5Cbz0UkHDE5FeLJVEvwYYbWajzKwvMA9IqfWMmQ02s36x+aHANCD/dSOZMINvfzs8QWTXrjA0wh13FGXwmpoa+PjHQ8ernTvD1f6//ztMngy/+AVcfjkMGxbq+K+5JqzTE65EpDPmKVQEm9mlwJ1AObDY3W82s5uAJndfYmZTgF8Ag4HDwMvuPtbM/gr4D8JN2jLgTnf/z66OVV9f703F7k7a3By6uC5ZEirTf/jDMHZ9D9DaGh6I9cgj8LvfwaOPwhtvhO+piRNhxIgwyFrfvsmn7radfHIYov+000JvX7Ni/8QikgozWxurJj9+WyqJvpB6RKKHcCf0Bz+AL3whZMBFi+CDH+y+XIEdOQJr1oSkv3JlGG35yJHQRv/IkeRTqvr2DQk/nvg7ez311LCviBSPEn02/vKXUFeyZg1ceSX827+FnlC9lHtoQZrsi+DwYdizJ9T/v/xy8tfdu5O/75AhIfEPGQInnXTsNGjQ8evi08CBodlpZ44eDf+xHDiQfOq47cQTQxyJU3V1eBKYSJR1legrCh1Mr/OOd8Af/gBf+1oY12DlSvjxj2HatGJHlhEz6NMnTJk4ejQMC5TsS+Cll8J/FM89B3v3hun117t/z4ED278QWluPTeLp9B6uqEjeDaK8PPzX0fELoON02mlh38OHQ0vbgwfh0KGu5zuua20NXypmmb1WVITfTd++mb1WVIT3KSsLP0via2fz8VezY79YM3ktK4P+/cMXbscp2frEdVVV7VWF7uFctrWF1/jU3XLfvu3v2a9f4aoe3cO5i8fQ1cVLMeiKPh1/+AP87d/C88/DP/0T3Hhj5hmzRLS2hp6/8cTf3VRRkTxJpJIw+vYNyTbZF1DHadeu3PZT6NsXTjgh/IG7hwSU6mtvVVZ27O+jf//wM3X8jytVZuH8tbbm5ndTXp7aZydxgvYv7jfeaJ9PttxxXeIYVmbpfUnH58eMCb3lM6Er+lyZNg3Wrw8N3b/+dVi6NDSNecc7ih1Zj1Ve3l5NUwgnnBCaoY4a1fV+LS0h2Xf8AmhrC1eWVVXhvTrOd7Yumys49/ak39oaqtGOHs3sNfFKN/5+Xc13XJd4RdzdaypXzG1t4cu3uyq3+BT/j6i8vH1KZ/no0e6r+F55JTRj7njcZJ+l/v3bf9/x+cGDQ6OH+PrEbWVlIYb47yOV39mRI+GL4siR0A4kH3RFn6n774errgp1C3fcERq7q4mKSK/jHv6M9+8Pf8JVVWF8qt52X6erK/pe9qP0IB/8YOhU9c53wqc/HcYuKNbwDSKSMbOQ2KurYejQ9ivzKFHVTTaGD4dly0JLnPnzYfx4eM97wv98qUzx//sTl4cPj96nTESKSok+W2Vl8MUvhpHJvvjF0Azz0KH25hjpjptz6qlwySXhIbQzZoSmKCIiWVCiz5UJE0KvpY7cQ7KPJ/+uptdfD803f/Wr0Bu3oiLcAJ41KyT+MWN0H0BE0qabsT1RSwusXh3G3XnwQXjiibD+jDNCwp81C6ZPD1U9IiKoZ2zvt2NHSPqNjWEQ+zfeCHePpk9vT/zdtScUkUhToo+SN9+EVavClX5jI2zaFNafdVZI+qNHt5WsaxMAAAcXSURBVHcz7TideGLPrPpxD1VX+/aFXiODBqkjmkialOijbNOm9iqelSu7HrWsrCyMN5DsSyA+VVVlNuxlvGvfoUOpd4NNnI4ePTbWqqr2mBK/uDqbjy9XVrbHkjjF12XyRRdvaP3mm+F+S1eviWMjdBwnIZXXiorwUOFUp4EDj12uqGjvAZU4JfaM6mp7WVlqXTnz0TIsPhBTfFS++IBM6cy7t/eeqqg4tmdVKsvxn8us/bOSznwmU3z8CbPwt5Thw5eU6EvFm2+GUcn27Tt+2rs3+fqOU7IugrlQWdn5yGaJI5y1tBwba8e448uZPgWsvPz45J84tbYen8DTGfIzmbKy47vWJns94YTw8+/f3/nUU/5eO/tCMOv+y6SrqdRNnRruz2VAQyCUin79whNJhg3LrHx8JKmuxjnubn1VVfLhKysrc/uzHjmS/Ivg0KH2PuiJfc27Wpe4XF4eYu3Xr/PXrrYlS+DxBJgt93B/pqsvgvgYAulMiaOatbV13We/u/78kNqxOpviXxjx89zZfGfbzI4d5aylJfXllpb28SjiX6jpzscHMUp3ipc75ZTsPydJKNFLO7Pw72tFRc9v0dO3b/gXt1jPGC4Gs/bRt04/vdjRSC+iLpgiIhGnRC8iEnFK9CIiEadELyIScUr0IiIRp0QvIhJxSvQiIhGnRC8iEnE9bggEM2sGns/iLYYCu3MUTj4ovuwovuwovuz05PjOcPekPQh7XKLPlpk1dTbeQ0+g+LKj+LKj+LLT0+PrjKpuREQiToleRCTiopjoFxU7gG4ovuwovuwovuz09PiSilwdvYiIHCuKV/QiIpJAiV5EJOJ6ZaI3s5lm9qyZbTaz+Um29zOzn8W2/8nMagoY20gzW25mT5vZU2b2+ST7XGBm+8xsfWy6sVDxJcSwzcyejB3/uGc3WrAgdg6fMLPJBYztzIRzs97MXjezL3TYp6Dn0MwWm9kuM9uQsO5kM3vIzDbFXgd3UvaK2D6bzOyKAsZ3q5k9E/v9/cLMTuqkbJefhTzG91Uz25nwO7y0k7Jd/r3nMb6fJcS2zczWd1I27+cva+7eqyagHNgC1AJ9gceBMR32+b/AXbH5ecDPChjf6cDk2PwA4C9J4rsA+J8in8dtwNAutl8K/AYw4DzgT0X8fb9M6AxStHMIvBuYDGxIWPctYH5sfj7wzSTlTga2xl4Hx+YHFyi+GUBFbP6byeJL5bOQx/i+CvxDCr//Lv/e8xVfh+3fBm4s1vnLduqNV/QNwGZ33+ruR4B7gTkd9pkD/Cg2fz9woVkuHtrZPXd/yd3Xxeb3AxuB4YU4do7NAf7bg9XASWZWjOfXXQhscfdsektnzd1XAa92WJ34OfsRMDdJ0YuBh9z9VXd/DXgImFmI+Nx9mbu3xBZXAyNyfdxUdXL+UpHK33vWuoovljs+DNyT6+MWSm9M9MOB7QnLOzg+kb61T+yDvg8YUpDoEsSqjCYBf0qy+Z1m9riZ/cbMxhY0sMCBZWa21syuSrI9lfNcCPPo/A+s2OfwVHd/KTb/MnBqkn16ynn8BOE/tGS6+yzk0zWxqqXFnVR99YTz9y7gFXff1Mn2Yp6/lPTGRN8rmNmJwAPAF9z99Q6b1xGqIs4BvgP8stDxAX/t7pOBS4DPmNm7ixBDl8ysLzAb+HmSzT3hHL7Fw//wPbKtspl9CWgBftLJLsX6LHwPqAMmAi8Rqkd6oo/Q9dV8j/9b6o2JficwMmF5RGxd0n3MrAIYBOwpSHThmH0ISf4n7v7/Om5399fd/UBsvhHoY2ZDCxVf7Lg7Y6+7gF8Q/kVOlMp5zrdLgHXu/krHDT3hHAKvxKuzYq+7kuxT1PNoZlcC7wM+GvsyOk4Kn4W8cPdX3L3V3duA73dy3GKfvwrg/wA/62yfYp2/dPTGRL8GGG1mo2JXfPOAJR32WQLEWzd8EHiksw95rsXq8/4T2Ojut3eyz2nxewZm1kD4PRTyi6i/mQ2IzxNu2m3osNsS4GOx1jfnAfsSqikKpdMrqWKfw5jEz9kVwK+S7LMUmGFmg2NVEzNi6/LOzGYC/wjMdveDneyTymchX/El3vP5QCfHTeXvPZ8uAp5x9x3JNhbz/KWl2HeDM5kILUL+Qrgb/6XYupsIH2iASsK/+5uBx4DaAsb214R/4Z8A1semS4Grgatj+1wDPEVoQbAa+KsCn7/a2LEfj8URP4eJMRqwMHaOnwTqCxxjf0LiHpSwrmjnkPCF8xJwlFBP/EnCfZ/fAZuAh4GTY/vWAz9IKPuJ2GdxM/DxAsa3mVC/Hf8cxluiDQMau/osFCi+H8c+W08QkvfpHeOLLR/3916I+GLrfxj/zCXsW/Dzl+2kIRBERCKuN1bdiIhIGpToRUQiToleRCTilOhFRCJOiV5EJOKU6EVEIk6JXkQk4v4/y47eC78vVDAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, 1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(6400, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2)\n",
    "#         self.fc3 = nn.Linear(128, 2)\n",
    "#         self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 3, 1)\n",
    "# #         self.conv4 = nn.Conv2d(64, 128, 3, 1)\n",
    "# #         self.conv5 = nn.Conv2d(128, 256, 3, 1)\n",
    "#         # self.fc1 = nn.Linear(9216, 128)\n",
    "# #         self.fc2 = nn.Linear(128, 2)\n",
    "#         self.fc1 = nn.Linear(2304, 1024)\n",
    "#         self.fc2 = nn.Linear(1024, 128)\n",
    "#         self.fc3 = nn.Linear(128, 2)\n",
    "# #         self.fc4 = nn.Linear(256, 64)\n",
    "# # #         self.fc5 = nn.Linear(64, 16)\n",
    "# # #         self.fc6 = nn.Linear(16, 6)\n",
    "# #         self.fc5 = nn.Linear(64, 2)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x_ori):\n",
    "        x = self.conv1(x_ori)\n",
    "        conv1 = F.relu(x)  #store conv1\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv2 = F.relu(conv2) #store conv2\n",
    "#         x = F.max_pool2d(conv2, 2)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv3 = F.relu(conv3) #store conv2\n",
    "        conv4 = self.conv4(conv3)\n",
    "        conv4 = F.relu(conv4) #store conv2\n",
    "        x = F.max_pool2d(conv4, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dropout3(x)\n",
    "#         x = self.fc3(x)\n",
    "#         x = self.conv1(x_ori)\n",
    "#         conv1 = F.relu(x)  #store conv1\n",
    "#         conv2 = self.conv2(conv1)\n",
    "#         conv2 = F.relu(conv2) #store conv2\n",
    "#         conv3 = self.conv3(conv2)\n",
    "#         conv3 = F.relu(conv3) #store conv2\n",
    "# #         conv4 = self.conv4(conv3)\n",
    "# #         conv4 = F.relu(conv4) #store conv2\n",
    "# #         conv5 = self.conv5(conv4)\n",
    "# #         conv5 = F.relu(conv5) #store conv2\n",
    "#         x = F.max_pool2d(conv3, 2)\n",
    "# #         x = self.dropout1(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "# #         x = self.dropout2(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.fc3(x)\n",
    "# #         x = F.relu(x)\n",
    "# #         x = self.fc4(x)\n",
    "# #         x = F.relu(x)\n",
    "# # #         x = self.fc5(x)\n",
    "# # #         x = F.relu(x)\n",
    "# # #         x = self.fc6(x)\n",
    "# # #         x = F.relu(x)\n",
    "# #         x = self.fc5(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    num = 0\n",
    "    loss_list = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        weight = torch.tensor([34546/(36885+34546),36885/(36885+34546)])\n",
    "        loss = F.nll_loss(output, target, weight = weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "#         if epoch == 3 and batch_idx == 10:\n",
    "#             for i in range(64):\n",
    "#                 img_ori = output[1][i].numpy()\n",
    "#                 real = target[i].detach().numpy()\n",
    "#                 predict = output[0][i].detach().numpy()\n",
    "#                 plt.imsave('./CNN/real{}_{}_a.png'.format(real,num),img_ori[0],cmap = 'gray')\n",
    "#                 img_conv1 = output[2][i].detach().numpy()\n",
    "#                 for j in range(len(img_conv1)):\n",
    "#                     plt.imsave('./CNN/real{}_{}_conv1_{}.png'.format(real, num, j),img_conv1[0],cmap = 'gray')\n",
    "#                 img_conv2 = output[3][i].detach().numpy()\n",
    "#                 for k in range(len(img_conv2)):\n",
    "#                     plt.imsave('./CNN/real{}_{}_conv2_{}.png'.format(real,num,k),img_conv2[0],cmap = 'gray')\n",
    "#                 num = num+1\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "        \n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} '.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "            if args.dry_run:\n",
    "                break\n",
    "    train_loss = sum(loss_list)/len(loss_list)\n",
    "    return train_loss\n",
    "                \n",
    "\n",
    "\n",
    "def test(model, device, test_loader,count,epoch,train_loss):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    result= [[0,0], [0,0]] \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            cmat = confusion_matrix(target.view_as(pred), pred, labels=[0, 1]) \n",
    "            result = [[result[i][j] + cmat[i][j]  for j in range(len(result[0]))] for i in range(len(result))] \n",
    "            \n",
    "            if epoch == 13:\n",
    "                wrong_idx = (pred != target.view_as(pred)).nonzero()[:, 0]\n",
    "                wrong_samples = data[wrong_idx]\n",
    "                wrong_preds = pred[wrong_idx]\n",
    "                actual_preds = target.view_as(pred)[wrong_idx]\n",
    "                for i in range(len(wrong_idx)):\n",
    "                    sample = wrong_samples[i]\n",
    "                    wrong_pred = wrong_preds[i]\n",
    "                    actual_pred = actual_preds[i]\n",
    "                    # Undo normalization\n",
    "            #         sample = sample * 0.3081\n",
    "            #         sample = sample + 0.1307\n",
    "                    sample = sample * 255.\n",
    "                    sample = sample.byte()\n",
    "                    img = TF.to_pil_image(sample)\n",
    "                    count = count+1\n",
    "                    img.save('./wrong-cnn/batch{}_i{}_actual{}.png'.format(\n",
    "                    batch_idx,wrong_idx[i], actual_pred.item()))\n",
    "                    num = batch_idx * 64 + wrong_idx[i]\n",
    "    #                 print(batch_idx,wrong_idx[i])\n",
    "                    img_ori = origin_dataset[num][0].numpy()\n",
    "                    plt.imsave('./wrong-cnn/batch{}_i{}_actual{}_ori.png'.format(\n",
    "                    batch_idx,wrong_idx[i], actual_pred.item()), img_ori[0], cmap = 'gray')\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    precision = result[1][1]/(result[1][1]+result[0][1])\n",
    "    recall = result[0][0]/(result[0][0]+result[1][0])\n",
    "    f1score = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Positive accuracy: {}/{} ({:.0f}%), Negative accuracy: {}/{} ({:.0f}%), f1 score: {:.4f}, Train loss: {:.4f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset), \n",
    "        result[1][1],result[1][1]+result[1][0],100. * result[1][1]/(result[1][1]+result[1][0]),\n",
    "        result[0][0],result[0][0]+result[0][1],100. * result[0][0]/(result[0][0]+result[0][1]),f1score,train_loss))\n",
    "    return test_loss, correct\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'batch_size': args.batch_size}\n",
    "    if use_cuda:\n",
    "        kwargs.update({'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True},\n",
    "                     )\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "    count = 0\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_accuracy_list = []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_result = test(model, device, test_loader, count, epoch,train_loss)\n",
    "        test_loss_list.append(test_result[0])\n",
    "        test_accuracy_list.append(test_result[1])\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "    plt.plot(train_loss_list, color='blue',label='train loss')  \n",
    "    plt.plot(test_loss_list, color='red',label='test loss')  \n",
    "    plt.legend()\n",
    "    print(test_accuracy_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-14-d90b47fe309a>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-d90b47fe309a>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    scale = 0\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "                figure, b = plt.subplots()\n",
    "                figure.set_size_inches(19, 19)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(model.filter_cos[i].detach().numpy()[0], cmap='gray')\n",
    "#                 np.savetxt('train-coeff.txt', model.filter_cos[i].detach().numpy()[0], delimiter='    ',fmt='%1.2f')\n",
    "#                 img.save('./filter/{}_ori_{}scale_{}.png'.format('cos',ori,scale))\n",
    "                si = model.sigma1.detach().numpy()*(2.1**scale)\n",
    "                de = model.theta1.detach().numpy()+ori*np.pi/8\n",
    "                plt.savefig(\"./filter-11.3_9/%s_scale_%.2fdeg_%.2f.png\" % ('cos',si,de), dpi=1,pad_inches=0.0,bbox_inches='tight')\n",
    "                if scale == 5:\n",
    "                    scale = 0\n",
    "                    ori = ori+1\n",
    "                else:\n",
    "                    scale = scale + 1\n",
    "\n",
    "            scale = 0\n",
    "            ori = 0\n",
    "            for i in range(len(model.filter_sin)):\n",
    "#                 sample = model.filter_sin[i]\n",
    "#                 sample = sample * 255.\n",
    "#                 sample = sample.byte()\n",
    "#                 img = TF.to_pil_image(sample)\n",
    "                figure, b = plt.subplots()\n",
    "                figure.set_size_inches(0.19, 0.19)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(model.filter_sin[i].detach().numpy()[0], cmap='gray')\n",
    "#                 np.savetxt('train-coeff.txt', model.filter_sin[i].detach().numpy()[0], delimiter='    ',fmt='%1.2f')\n",
    "                si = model.sigma1.detach().numpy()*(2.1**scale)\n",
    "                de = model.theta1.detach().numpy()+ori*np.pi/8\n",
    "#                 img.save('./filter/{}_ori_{}scale_{}.png'.ormat('sin',ori,scale))\n",
    "                plt.savefig(\"./filter-11.3_9/%s_scale_%.2fdeg_%.2f.png\" % ('sin',si,de), dpi=100,pad_inches=0.0,bbox_inches='tight')\n",
    "                if scale == 5:\n",
    "                    scale = 0\n",
    "                    ori = ori + 1\n",
    "                else:\n",
    "                    scale = scale + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:43: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:49: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAoElEQVRIie2UMQoEIQxFf5YV7awED+FBvLlXsbIWFLPVFjvMyggKDswv1fgS/QkxM1bqtfT2B7AF4N3btNZetliMkc7W93oiotMk5wCICFJKCCHWAADAGAPnHEop8wHMjNYavPcYGS9DFaSUEEKA1vpyDPWyOdqUiKCUQikFtdafs/9s2u2Do5gZOeeRkDHAFzKi5Y3W/YMZ2mtUPIB7Aj5sAjMD8BsHqAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAkUlEQVRIie3VMQoEIQwF0G9YBAVbL2Fp6dVtPYY38AIiJNsvw+DsjjAL/jJiHsSASkSwMrS0+wYeAbzODlNK0ytWSlFH9f8f0SWAiGCthVKH0/gNYGYwM2KMcM7dDwBAaw0hBHjv7weICMYY5JwxxpgGTtf0M8YY1FqhtZ6+c/mRRQS99zXAN1kOqP3hbOD5wBuRISKHoJf7rAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAmklEQVRIie2TwQ3DIAxFP7gnzhESC7AaMzILG7ACGOdciSK3StREyj8azLPNtxERnCl76usP4BKA1+rQe6+2WK3VzOL3H5EaYIwBEeHbxVQDrLWIMWLbNvTejweMMQAAKSU459SdqAEiglIKcs4IIWjTYFaVzGxKRCAitNbeuvhk0+UezMTMYGb1/evY9Fct/+AI3X9ED+D/gB0ydTMWEAIe5QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAkElEQVRIie3VMQoEIQwF0G8USe05tLLxop7MTvAWVhZxLrAMuuzALJhSjQ9+BNWcE08WPXr7AV4BmLvNEMLyEyulqE/r/x/RFsDMMOY21e8BEYFzDiklMPPvAaUUWmuIMcJ7D6K11i1Aa42cM3rvy8BWoNZajDFQa13u2ZsYACKCiKyf3wV263FAnQ/nAO8HLjqPHzvvA0mOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAkklEQVRIie3VPQoDIRAF4KcEsbG1EeaeXtIbeAG9gT9MqkCKZdFkF1zwlQrzgT5RMDPujLx1+gaWAF5nm9ba4YqllMTR+vOPaApgZsw+zGFAaw3nHKSUaK1dD/TeQUTw3oOIhoHTFn2n1ooQAnLOMMZAKYVSynUAAJRSEGOEEIeN/B/4ZOai16rpLxH7w9nA+sAbb5IsbDIOa20AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAApElEQVRIie2UsQoDIRBER8nC/oWNreW1/n9vaeMXXKceCrfpQgjJYXFHcuGmdBmfjMMqEcGR0ofefgF+AnDbGjrnhisWQlDvzv88omdpraGUgohgXdf9AUQEay2ICDFG5JzHHjYKqLUipQTvPaZpAjMP+dTWLnpt0bIs6L3DGINSClprj7g+tWg4IgBgZjAz5nke9py/pocDNj95D50/ogvwfcAdZ1w2F4k/goIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAn0lEQVRIie2UsQ0DIQxFv60UFOzAGDA/PUMwAo0lEIIUUaIUCSHRnXJI90tjeMb+MvXesad419dPwCEAl9GhMWbaYjFGehVfv0XHATAziAgiglIKZjfAcMh3ERGUUnDOwVoL7z1CCMg5fwTRKOHZRcwMpRS01mitIaWEWusD8M5FUz8AgNYaRAQiMnvlVthX2T9ofcBwyFto/RadgP8DrjNqOrxr4JoOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAn0lEQVRIie2VMQoDIRBF/0gUi10sBM/gOQRvLdt5H9lqK02VkGJJxoBkA/7SYf6brwNSaw0jJYa6T8AlALd3xRACe8W2baOz8/+/ot++wauklLDWQmuNUgr2fUet9WMfOwERwTmHGCO891BKsfrYCY7jQM4ZKSUYY6CUAtHp4nwHEEJgXVcsywIALPMuwENc4+dgvYBeDQfQ/HAm4PqAO9BMHNXxm8q7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAApElEQVRIie2UwQrDIBBEZ4sScs/B7/Av/PAc8iGCJ8+6INtTIYXU2pBQA3lHZR2cHYZEBGfyOPX1W6ALAVW7NMY0RyyEQFvn17eoLwFmBjNjHEcopUC0afsb1SWv0VrDWgvnHOZ5xrIsKKV8naNaF71SREQYhgHTNEFEEGNESgnr2U8pavqBiCDnDO89fi3HZov2tm5fKdpDdclHcH2LboH/CzwBF4E65ZtAxRIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAqklEQVRIie1UMQ7DIAw8mw7xwMTAM/KATHl8/pKFB+DBdOjQqkopiRo1kXIjYN/5fIJKKdgTvGv3i+AQBLfa5TAMzRGbpomWzs9v0SYCokU3FlHdwXtT7z1ijFBVzPOMnPPXuqYJiAgigr7vMY4jnHNQ1TZhtb/oNUXMDDNDSgld10FEwPzU9ylFzRaZGcwMIYTWkoewVY95fSaOGdM1qC75Fzi/RRfB/wnuxn0qxsPMqsYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAnklEQVRIie2UQQrDIBREx1ohe0G3Ljyix9ULeAZ1uioEmorShCaQt1T8D8bhC5I4kseh02/BKQTP3qUxZrhiOWexdX79iKYFJEESQmwm8puglIJlWeCcg1JqXwFJWGsRQoDWGqWUoXeit4vWLZJSwnsPAEgpfQi+tahb0zWtNcQY0VrDzIIcFpBErXV48Jvz1XSW7ifvwfUjugX/F7wAGf0551nGqNUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAkklEQVRIie3VMQoDIRAF0O84yBzE1koL7+4xbBcrD2EhmDqwBCVZMKCl+n2DDqjGGHhy0KOnH2ALgD8tOuemWyznrO7m//+KlgEiAjODaC66BIgIvPew1s4XNLuRmRFCQIwR13Wh9z6XWwFKKUgpQWsNY8xvgdYaaq0QESh125HfAQCmH/Yts5zYDVDnwznA/sALTHQc5S+0/bQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAkUlEQVRIie2VQQoDIQxFf0rRGwjixpt5Uk/jTvEAEiGzncUwaNsBC/6lMXnk81ESETyp16PTN2AJwPuuaIwZjljOma7O/9+ijwBaaxBdOvIdgJnhnIP3frhnGCAiUEohhIDeO0afmKkNrLWIMSKlNNxzG9OziAi1VpRSwMy/BwBAa23mOoBVYzoj2h/OBqwPOAAFiyiYEw92HwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAkklEQVRIie3VQQpFERgF4EOXxDrMWIDN2tQdGtmFKP+bvsHrxusqA2dIfPk7hRERVoYvvf0AWwDX06b3frhi932zX+t7jojz8WPTgFIKUso1gDEGIQRord8HOOew1sI5h5zz+wBjDK01xBghhBgGHmv6HSJCSgmlFCilhoHhF/TeUWudatAU8G+WA+x8OAfYH/gA9FMiPP0i/loAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAkUlEQVRIie3VUQpFERAG4H/kQXmwBHuwAOtnGfagKDJnA6cT96Y88Ejj0xiDmBk7h9i6+wWOAOTXorV2usRSSvQ2f16KiAhEr4f9H2BmKKUg5Wdmfwdaa3DOwRizB+i9w3sPIebDlgCtNWKMyDnvAYgIIQTUWqdj5m8LwBgDpRSsdODlMl1t79sfGt0P5wLnAw/O1iswmBnaWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAkUlEQVRIie2VMQoEIQxFvzKNCuI5rDyDB/cA3iWVTcDCLbbbHQYzMOCAr4wkD8wX1RgDT6Ifnb4FSwiOq8Oc83TESinqrP7+KxIJtNaw1kLyOEUC7z1SSmDmaYlIEEJAjBGttekekYCZUWuFMQZKnYbmj8uY/kJEICI456Z7RILeO4DvsmdZK6Z3UPvD2YL1BR/L9SWA9QE6wQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA7UlEQVRIie2VTQqDMBCFX2ImegC3ghsP4AG8gAf2LOIyK5dB0IAJSRcFobTIlLbQhW+bvPkyf0SklPBLyZ9GvwB/AVBnh33fHyOWZRlSSogxQkqJGOPD3WEYxKsY7AyEENBaI4RwQDh6q0RlWcJ7D+8928MGOOdQ1zWKooC19m5mZMEG7PsOpRTatsU8z089+BgAAMYYdF0H5xy7TCyAlBJEBGMM1nVF0zTYtu17AAAgIizLgnEcUVUV18YDxBghhAARYZomKKWQ5zlrXN/qARHBWgvvPbTWB/xMp5v8SiGE+8uYiyauD+cC/D/gBhTnWzn7gSebAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA8UlEQVRIie3VMaqEMBCA4X80KqyNoCAWFjZezTN5Ic9gYeMeYC1sRDfOq17xEJbI24UtTJvMfCQzQ0RV+eTyPpr9Ar4CMK82m6ZRgN9O832ffd9RVUTkz9m2beWY4cQNns8nQRDg+z5nWtsZ2LYNESFNU6y1zogTICKs68o0TdR1DcC+7+8FPM9jHEeKoiDPc7Ztex+gqsRxzP1+Z1kWqqpyfibnGhhjyLKMrutI09S52KfmoCxL+r5nnmfCMHSKcQZUlSRJMMYwDANRFDnFvRy0w2FjuN1uPB4PgMOw/RsQEYIgwFqLiDgBcn04F/D9wA/7AFg+9fXPmAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA+0lEQVRIie3VPY6DMBCG4XcMRkGpk5IDcBRamlyFW1GgHCMnSMNPhWiRLCPjrXaL1W4AiUgpmHbGfmTJny3ee95Z6q27H8BHAOGrZpZlP1fMe4+I/Dt7v9//bK46gVKKeZ5xzq0Z3w5orTmfz4zjyDRN+wLee6ZpIkkSLpcLTdNsQhYBEcEYwzAM5HnO9Xrl+XzuBwCEYUhd17RtS1EURFFE3/f7AUoptNZUVUXXddxuN4wxWGv3AQBOpxNaa8qyJAgC0jTFOcfSY7kpaHEcY63l8XgQRRFxHC+ueRm03yUiaK0xxuCcIwiCxWxsAr4REVkdOjk+nAP4fOALCeFeUBKjss0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABCklEQVRIie2VQarCMBCGvyQNaaF0U/UEtuvepOAdPIqncC14JHe5ghshJZh5i4eutK2g4MLZzmQ+JvP/jBIRPhn6o91/gK8AZGPJ7XZ7l9hNbUqph7X7/f5hYnICEUFrjbWWlBKvynoSkFIixkhZllRVRYyR6/X6PoDWmhAC5/OZruuo65oQwuxJJgFKKay1nE4nvPf0fU+e5wzDMAsyS0XOORaLBcfjkcvlwmazAf6/7y0AEWG1WrFcLtntdgA0TTNr6bN9ICK0bYvWmsPhgHOOoigmIS8BsixjvV4TY8R7jzEGY8zou1GjPYNUVcUwDPcdPDPfy4BbGGMQEVJKo80B1O/g/ADfD/gDeOpzMjRLMb4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABGElEQVRIie2UsYqDUBBFz3sqNloKfkOa9JIqjfiH/kks8wOxE9IrKazEkICJcbYILlnY1YQYdhdy22HugTuXUSLCK6Vf6v4G/AmAOTSMoujuiq1WK/Uw4LbCSn27P6rBiHrTtm1pmoau66YFANi2jeu6NE1DURRUVfUF/hRARDifz7iuSxiGBEFAnudkWcbhcHgeANd4iqJgu92yXC6J45j5fE5Zluz3+9HY1NAviqJIRISu6zgejyilWCwWzGYzNpsNWZYhIpimSZIkj7cIrlkbhoHjOJxOJ9brNWma4vs+nudR1zVt2/64Pwq4Bdm2jWVZXC4XdrsdlmV9zp4G9NJao7Wmj25yQK97azp45Cn0/7/pG/D7gA98F3Uq/XUT9AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABDklEQVRIie3VP66CMBzA8S+txhBodDQsDEwM3AWPQMIFuIVHcPAynkVWFkICDdA6vPji8B5ijIkDHdu0n/7+pHWstXxyiI+evgBfAaymFvM8n91i5/PZ+Wt+VgTGGIwxc63XACEEq9VPoMMwvAxNAsYYrLVst1vCMMT3fdq2RWs9G3oagdaa6/WK1prD4UCapnieR9d1jOP4HvCYnsvlwvF4xHVdsiwjjmMcx3lan1k1UEoRRRFN01AUBafTid1uRxAErNfryf2TbXof1lo2mw1JklDXNWVZUlUV+/0eKSVSyveAO2KtRSmF7/v0fY/WGiklQvyfiNnAIwT8puZZN739VEzdHsBZPpwF+H7gBku7ZyVDfjhwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA70lEQVRIie3Vu4qEQBCF4b+6lEHBBhFzEx/Hp/VBjMxMBc1FvKBusMzCwqwjgjAsVthd9Aenb7JtG1eWuXT1G/gIwNmbzLLs8BHL81xejf/ziLZtY11XlmVhnmeMMbiui4hgjEHkZSrHARHBcb5bhmGgrmvatsX3feI4JgxDPM9DVc8BxhgejwdRFJEkCWmaYq2lqirKsqRpGoZhYO+52QWWZaHrOrquo65riqLAWou1FlUlCAJEhHEczwHPrNd1ZZom+r6nbVtEBFXFcZyfnlPAE1FVVBXXdQF+RfJuo98Cf6FH6/J7IPeHcwOfD3wBeDZOXntEZkUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA7ElEQVRIie2VMcqEMBCFXxKDhQFRKy0k5/NA3sbT2GijIkQxmuSv/Nli1xVhwcJXJsz7yJsZQpxz+KXoT90fwC0A3tFlURSnR6wsS/Lu/D4RXd2Xw4h2Y+ccjDEwxsBaC0opPM8DYwyEvE3mHIAxBs45wjBEkiQIggBaa/R9j67rME0TrLXXAdu2QWuNeZ6hlEKe55BSIssytG2LpmkwDAOWZbkG2J+vlEJd16iqCuM4Io5jpGmKKIrg+z4o/dzKrz1gjEEIASEEpJSw1mJdVzjn/o2PBuAr4FV73pzz0zX32YOrIs+H8wDuD/gD3JxeHIGFhywAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA+0lEQVRIie2VvaqEMBSEJ/GeiGCpnYWNb+EDiG9r47NoYyWkFUUw5set7nILcWX3Cls41SHhzMfAkLBt23Cl+KXuN+ArAD9Hl2VZnq5YXdds7/yjBJy/Xj9M8MrMOfc54K+xcw7rumKeZ3DO4fs+hBCHSQ4BnHM456C1xjAMkFJiWRZkWYYkSUBEUEpBKfUewDkHYwystYjjGEVRIM9zTNOEpmnQti2UUjDGvJ8gDENEUYQ0TeF5HqqqQt/3GMcRRAQiOrI4BjDGYIyBlBJd10FrDQAgIgRBAMZ2m3keYK19zkIICCGeyYB/atGezhj/6vKngt0fzg34fsADJv5gvaHrQ04AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA/UlEQVRIie2VQYqDQBBFX2l1BHEVXBhcCIJn80yeJXsPEdzoAUSyCISmu2c1Q2BCJggyWfiX3VCvml/1W0IIbKlo0+o74CMA+uqybdu3R6zrOnl2/r8veKYQAo+7IyKIPG3+fUAIAe891lqcc8RxzOFwIEkSvPd479cBvrt1zgFQliV1XXM8HrlerwzDwDzPvEqDlwARQVU5nU40TUNRFNxuN/q+53K5YIwhTVOMMesAURShqizLwvl8ZhxHpmkiz3OqqiLLMlR1vQfOOe73O9ZarLWUZUnTNBhjfpm9CgCgqqgqaZoiIj+Gv6s/PXjUmmjffNFk/3B2wOcDvgDRlGdBw32caAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABAklEQVRIie2VP46EIBTGP3AZNLEy0cTCeABvYmnnrbyMnMFKD0BHSW2BEmWrSbaYUXZ2J5nCr4TH+33vTwJxzuGdom/NfgE+AvB1dFnXtfeKCSHIo/OXK3DOwWfFXwIYY2CMASEEQRD8L0BrDa01sixDkiSn8Ycz+ClrLaSU4JyjbVtQSjGOI6y1oPS5T68KrLVQSqEsS3RdhzzPMQwD5nkGIQ9n61/Btm0wxqCqKjRNg2ma0Pc9GGOIouhvgPuWpGmKoigghIBSCpxzhGF45u0cQAgBYwzOOUgpsa4r4jg+7PmvAACw7zuWZYFzDrfbzTvxXd5Wznr99N314VyAzwd8A/RrW3QTdsNYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABF0lEQVRIie2VPY7CMBCFv9n8WamgCKJDwmWknCUFV+AqXIKWJneg5BIcgoLOIrFnq2y167CRkCiY1p736b0Zy6KqvLK+Xqr+AbwFII0d7vf7P1ds3D4RAeB4PMpv92Y5UFVCCD/isfoXQFXx3jMMA8YYjDGICLG39DRAVXHOMQwD1lo2mw1939P3fbQvOoOxvPc45zDGsNvtCCFwOp14PB4sFotoVJOAMZb1ek3bttzvdw6HA3mes91uSZJkvoNxmMvlkqZpOJ/PdF3HarXCWkuaptH8n3JQFAVZlnG5XLher9R1TVVVqOqk+CRARAghcLvdcM5hrSXPc0IIk8JPO/DeA1CW5eRKzgKIyOQgo/2fD+cDeH/AN0uOdmZQj0zXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABCElEQVRIie2VMYrDMBBF30hKwC4CLtylTJnKt/BRfD9DTpAjuHTapEhIQNiFBZqtNrAQ1gq7gRRRKf2vNzP6IFFVXrnMS2//AN4C4H47rOv6YcS+kyci973dbiePtE93EGNkmiacc4gIczF/GjAMA9ZaVqtVkj4ZICKM48jlcmG73bJYLAgh/B9AVTkcDmw2G6qq4ng8oqo/3uFPgOv1iqrSNA1d1+G9x1o760sCxBjx3lPXNbfbjf1+T5ZlGDNvTwKEECiKgvV6Tdu2qCrL5TLFOg9QVYwxlGVJ3/ecTifyPJ+dfTIAwDlHjJHz+YxzLmk0d2+KSESYpuneTWr1APL5cD6A9wd8Adw2ZC3AWHRSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABCElEQVRIie3VMY7CMBCF4T/Ghi4UOEoamkgcJAU34UiUXIWCA+QUFEmZIoosFI+pFq12WfAikCiY1vL7ZHnGTkIIvLLUS9M/wFsA+tbiZrP51WIiAsBkMuF7B+52u+RaxkMnMMZgjEFELuBf9S/Aew9AlmWM44j3HqVuR0QDIoJzjrIsyfOcpmmi9kUDp9OJ+XzOer2mrmuccxhjngOICN57qqqibVsOhwOLxYIkuXqvjwHWWqy1bLdbiqIgTVNi3rG7gIiglMJay36/ZxgGVqtVVDjcmYOvMsbQ9z3H45HlcslsNnseoJQihEDXdUynU7TW0eFRAHAZKK313b7/Wcnnw/kA7w+cAQ0jai7mCdnmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA6UlEQVRIie2VvYqEMBRGj0lkUoilaG/lu/i0Po6FDyAopDCI0bvFMssOLENkZ2AKv/by3ZP7RxIR4Z1Sb81+AT4CYJ4F27Z9WLH7xmmt2fedJEl+Yl3XJfyhUxXck95uN47jiPKcAnjvsdaSZRkiQswNRQNEhHmeqaqKPM8JIUT5ogHHcTCOI03TYIxh3/fXAkQE7z11XeOcA3gY8r8B27ZRFAXOOZxzaK2jfKdaVJYlfd+zLEs04Okd/FaaplhrGYaBEAJKxb0tugKtNcYYpmn6Nr4aoJRCRFjXFaVU1IABkuvDuQCfD/gCvRpbJ7d7V8gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA30lEQVRIie3Vy4qEMBAF0FtV4wMX4gOE/KIf5PcJLtxFRDeKVnrlwHRDE4Y09MJa5nVI5ULIOYdPFn/09Bv4CuDn3WTbti8RExGc5wnnHIjod7zrOnpeC/zjBkmSQFXhG29vQFXBzCiKAtu2QVXDA1mWwRiDaZrCA8dxoCxLNE2DcRz/9D8IoKqoqgpEBGstoigKCzAz4jhG3/fI8xzMflvfxvQq5xxEBOu6YhgGGGPCvgERQURgrcWyLKjr2utwb+BC5nmGqiJNU2/Aq0UXsO87mNk7QQBA94dzA98PPADWy1Ip+qJIYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABd0lEQVRIie2VsYrqUBCGv3MMkmOUBItYBFJYCRZCHs8H8AVtRGMhaLQQjIlI4klObrVb7U2We1nYwmkH/o9//hlGNE3DT5b8UfU34FcArLbmcrlsAIqiIAgCHo8Hu92OxWLB/X6nqiosy8IYw2q1El9ptDqQUqKU4na7obUmDEOyLKMsS2zbxhiDMabVQSvAGEOv16Pf75MkCUEQYFkWl8sF13X5zg11AgBc1yWOY8bjMb7vczgcUEp9jkfKv8t0hlxVFaPRiKIoOB6PRFFEkiRorVFKUdf1vzsQQvB6vXAcB9u2Wa/XRFHE8/nker3iOA5N07Tm0AmoqgqAyWTCdrvFsixmsxn7/R6lVOt4OgEf9bGmp9OJLMuYz+ecz2eAzxz+C1CWJcPhkMFgwGazYTqdIqUkTVNs227dpk6AlBKtNUIIfN8njmM8z8NxHPI8R0qJEF/e2PcdNE2D1hrP88jznDRNCcOQuq4py7I1B/F+OG/A7wf8AedJqTcLW0b9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABb0lEQVRIie2VsarqQBCGv+wmkaAihiDapDCxs/TJfBAfzEawsbAQArFTTLJiYLO75wXujcK9B05xph3m//j/GRjPOcd3lvhW9V/AjwD4fc3tduuklGitqeuazWbD8Xik6zoWiwVKKTzPA2C323l/0uh1YIwhCAKiKKIoCsIwJEkSqqoCQErJuzPvBTjnMMYwHA4xxnC9XsnznKqq6LqOIAj+DSCEoG1bRqMRURRxOp3IsgyAx+NBGIa94h8BtNYYY0jTlPP5jHOOPM8py5IgCPA8r9fFR1fUNA1ZllGWJbfbjfV6zf1+/yimtw6EECilGI/HzGYz9vs9y+US3/ep6/ptTL0Aay2+79O2LV3XsVqtOBwOTCYTptMpTdMgRH8IbyMSQmCtRSlFmqY8n0+KoiDPc5RSWGt7IR/tQEpJVVXEccxgMOByuTCfz3HOobX+P4DX64WUkiRJKMsSrTVxHKO1xlr711nv9+H8An4+4AuTLKwWnDKrbgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABqUlEQVRIie3VMarqUBDG8f+JCQREjWgjogYLUYuQysplWLkCa2tLN6ILsHABksbexsbCIKidQjRqSKLnVg9eccl7CBdu4WmH8/0YmGGElJKffMqPpn+AXwGoScXRaCQBwjDE933q9TqPxwPf99E0DSEEf6ZwPB6L7zISO3i9Xui6TrVa5Xw+s91uMQwDXdcJwxApJUJ8m/t/QBRFHI9HCoUC3W6XzWbD4XCgVCqRSqWIoigx/J9ALpfjfr+zXC6xbZtOp4PjOFyvV8rlMlJK4jhO7CIRUBQF0zTZ7/csFgv6/T71ep35fI6qqhSLReI45vl8vgf4vk86nabRaOA4DqvViuFwiKZpOI6DYRhkMpn3ASklvu9TKpUwTZPJZILneQwGA1zXZb1eYxgGmqa9B6RSKcIw5Ha7YVkW+Xye6XRKrVaj1+ux2Ww4nU5ks9n3AAAhBLfbjSAIsG0bz/OYzWZYlkWr1WK32xEEwfuAoigIIbhcLqiqSrvdZrfbsVqtaDabFIvFRCBxk/9GXq8X1+uVbDZLpVLBdV2CIEDX9cS/4nNwPsDvB74Awm6yQle4u/QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABsklEQVRIie3VP6riUBTH8W+eMcRgBAsFUUGw0EJBRBtxARYWYusGXICLsLCzdyPBHfintxBUomIUIbHQKLmvmhlmmBdBePAKT3vh97kH7jlXEkLwnfXxrelv4EcAst9hr9cTAKqqst1ukWWZRCLB9Xrlfr/z8fHnfoPBQPpfxtMOLpcL0WiUYrHI8XjEsizC4TCBQADP85524AuEQiFs22Y6nZLJZCiVSiwWC2zbRtd1gKeILyDLMrlcjt1uh2EYNBoN8vk88/kcAF3XEUL4Ir7A+XxG0zQqlQqGYTAej+l0OmiaxmQyIRgMoqoqftvAF/A8D8uyyGaz1Ot1RqMRh8OBbreL4zgsl0tCoRCy/PVb8QUUReFyuWCaJrVajXQ6Tb/fJxKJ0G632W63v7t8CZAkCVVVOZ1OnE4nWq0WrusyHA4pFAoUi0U2mw232+01QAhBIBBAURRM08TzPJrNJqvVCsMwKJfLxONxbNv+MsN30H5VMBjE8zzW6zXJZJJqtcpsNkPTNGKxGI7jvA4IIZAkCUVRcF0Xy7JIpVI8Hg/2+z2qqv410f+W9P5w3sDPBz4BYamsg/I1PscAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABt0lEQVRIie3Vv+txYRjH8bfjt0Gh0CmnM1hIBv8BGUwWNv+AP8Fm9U9I2YyyWAyMwiApSUlRZPGr4/h1P9P3WZ6n0/Oob30Hd13T3XW9+tz3cJmEEHznkb51+gf4EYDF6LJUKglJkrDZbGiaxu12w2w2Y7H82VapVEx/m2GYQAjB8Xjk8Xjg9/vx+XwA6LrO8/n8pwSGgN1u5/l8MhqNGI/HWK1WVFXF4/HweDzQdZ3X62UIGD6RzWYjlUqxXq9pt9tMJhMSiQSJRAKv18tms0HTNMxm83sJptMpvV6PYDBIuVwmn88zGAyoVqssl0tkWSYYDBoChgkURWG1WlGv14nH4+RyOdLpNLVajU6ng6IoxOPx33/z34DT6SQWi7Hdbun3+wyHQ7LZLMVikWQySavVYjgcEgqF3gPO5zNWqxVZlpFlmdlsRqPRoNvtkslkKBQKTCYT5vP5e4AkSdzvd/b7PS6Xi0gkgqIoLBYLms0mqqoSDocJBALvASaTCUmSEEJwuVy4Xq+4XC6i0SiHw4HdbsdsNsPtdr8HfCFfJYTgdDphsVhwOByEQiF0XTfu/yycD/DzgV/qAaPYEm1iVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABtUlEQVRIie3VT6tpURzG8e+yVwh75+ySgW1iJt6AYuIdMDJibKT2yMRbMFTeixETBhSlDBj4NyDlX0hszujWvXWPe9t16gys6Vq/59Ovnlri+Xzyncfxrelv4EcA8tWlaZpPAKfTiZSS6/XK7XZDCIEQ4o+31WpV/C3jnxtcLhdmsxm73Q6Px4OmaSiKwuPx4H8q/hJwuVzEYjHC4TDT6ZR+v8/xeMTv96OqKkIIHo+HfWCxWLBYLEgkEuTzeQKBAO12m263i2VZ6LqO2+1+uclLQFVVer0etVqN+XxOsVikVCqhKArNZpPxeIyUEp/PZw8wDINUKoWu69TrdcrlMk6nk0qlQiaTYb/fMxwO2W63X2a8bNFqteLj44NkMkk0GqXRaGCaJul0mmw2SyQSodPpMJvN7AFCCNbrNbvdDsMwKBQKjEYjWq0W/X6fdDpNPB5H0zR7gJQSh8PB7XZjMpmgqiqhUIhcLsdoNGIwGLBcLjEMwx7wq4IulwspJefzmclkgqZpRCIRgsEgm82G+/1uD/gdUhQFRVGwLIvD4cDpdMLj8eD1erEs68tZ8f5w3sDPBz4BoP2muWAOuj4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABWklEQVRIie3VzarqMBDA8f8kLbRFSyKKO3dufCcfxZWv5gP4CCJCXYgoitIkNXd1yvV+9N4jHDgLAyGQQH4zMJNIjJGvHOpLb38D3wJIug4Xi0X8qDIReVp/HjFGlsvl7wf/AgAej0e7xhj5AJVS7fwT+l9AnufkeY7WukVCCHjv8d4TQiCE0BlgJ7DZbKjrGq01RVFgjMFaS7/ff8qwC+kEJpMJt9uN0+nEfr9nvV5zOBxI0xRjDIPBAGPME/gpYDweY4yhLEvKsqQoCkSEqqrY7XZUVcX5fKau69eA1WqFcw4RIcsyer0e1lqGwyHWWmazGSLC9Xp9DZhOp4QQuN/vXC4Xjscj2+2WpmkQEZIkIcsyjDHM5/PPAyEEtNYYYxiNRqRpioi0VeSco2kavPevZRBCwDn3tKeUQmvd1n+SJK/3gYi0PQC0TfZrxEr9/cWR94fzBr4/8AOiIZKSGwIHdQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABXklEQVRIie3VTYrCMBjG8X8+moLUWiRF6NYjeDMP4om8hncQrdRWamht2tkVZpgpozAwC99VPkh+izwvEcMw8Jcl//T2N/AvAD21ud1uh77vP60JIb4d73Y7wTc1CWitUUoBMAwDfd/Tdd04/03EJwHnHABKKYwxhGFIHMcAeO/pug7vPd7714C2bSnLEucczjmapsF7z2w2Y7FYMJ/PiaIIY8xrwGazIUkStNb0fY9zjuv1Sp7nXC4XqqrieDwi5c9ZmQT2+z1SSqIoIk1TsiwjyzLW6zUAj8eD+/1OWZavAdZa6rrmdDpxOBw4n89UVUUcx1hrsdaSJMn4Lk8Dq9WKMAwxxhAEAVprvPcURUFRFNxuN5qmGcPwNJDnOQBSSpRSKKUIgoAwDFkul6RpiveepmleA4IgGPPedR1t21LX9YgKIRBCjL3yNACMl0gpx7R87e7J8+8P5w38f+AD992aPz/oOSkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABmklEQVRIie2UsariQBSGv5kMMibRaCPBQlGwsLDRRvBV7pP4AD6D7+FzWNiICIKFmkY0RNBootnistlmb3aVvewu3L+ZYob/O+fMOUckScJnSn6q+xfgnwCorMvhcJi2mJTvsTwej/RMkgQhBFJKRqOReBogpUwN4zjmfr+TJAlSSpRS5PN5hBBcr9fXMojjOI1UKUWxWMSyLLTWAPi+TxAEaRBPA5RSaK0plUporbndbuz3e2azGZ7nYRgGlmVRq9VeAziOw/V6ZT6fs16v8TyPMAxpt9v0+30ajQaO4+D7/oceImtVvL29JUEQoJTCdV16vR7dbhchBL7vs1gsWK1WnE4nxuPx85/carWo1+tUq1XK5TKbzYbJZMJyuWS322GaJpVKhVKp9KFHJsC2bTzPYzqdslqtCMMQrTWu6zIYDLBtG4Aoil4DbLdbjscjuVyOTqdDoVDANE0AwjDkfD4TRRFZZc4EaK1pNpsYhgG8t+3hcCCO4/SNlBIhflr+XwMALpcL8GOCv0/u7yqzi/6E/v9t+gX4+4Bv+GqkXdr63G4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABdUlEQVRIie3VS4rqQBiG4dcqK4QQFBVFcOQls0x1E+7Ghbgbh+7AQW9ARSHiZSApUUIuVWcmpw+09Ak09MB/WvA9VPH9VMVay0+O+NH0N/ArgOqrw9ls9qli1lr+bp0QAiEExhjm83nlv4F/Q4UQKKVwHOcZnGUZxphyN7DWIqV8hiqlyPMcrTVaa+73O8YYWq1WOcD3fQCyLON0OhFFEdfrFaUUjUaD0WhEt9slTdNyQBRFaK2J4xghBEEQMJ1O6ff71Ot1drsdm82GKIrKAa7r0uv1CIKAwWBAURRcLheWyyUfHx88Hg9c16XdbpcDwjDEcRy22y2LxYL1es3hcKDT6RAEAWEY0mw2EeLrtr8EVqsV+/0eYwye5zEejxkOh/i+T57n3G434jgmSZJyQK1WYzKZ4Hnep9Dj8UiSJBhjkFIipSwHKKUA0FpzPp/J8xwAKSXVavXl03wLKIqCNE2x1iKEwHVdgOeSfWcq7w/nDfx+4A/qBKl+urEUygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABtElEQVRIie3VO6rqQByA8W+SePARXwgqAcVGBFHBQlfgBgQLtyG2lm7BDVjY2dhbWFvYpYqCFlqJ8YGGxJBb3VvdE0E4cAqnHeb7McMfRniex08u6UfrH+BXAIrf5mAw+DdiQghc18V1XQKBALFYDMuysCwLSZIYDofif42XNxBCIITg+XziOA6qqpJOp7ndbqzXa2zb9j3vCwgh8DwP27ZxXZd0Ok0mk8EwDJbLJalUilKpRCAQ+Lbh+0R/44qioGkasiwzn8/Zbrd0Oh2q1Sqr1YrD4fAe4DgOoVAITdM4n8/MZjOCwSD9fp98Ps90OmW325HL5d4DotEo8XiczWbDYrGgWCzS6/U4Ho+MRiMulwuVSgVVVd8DgsEguq5jGAatVotut8tqtWI8HpNMJmk2myiKwvV6fQ/YbDacTifa7Ta1Wo3pdMpisaBQKFCr1bAsC9M0/RL+gKqqlMtlEokEk8mE/X5Po9Egm81yvV653+8IIZCk74fRF/j6+sI0TXRd5/F4UK/XCYfDmKaJbdsv4y8BgPv9TiQSQdM0JEnidDrheR6yLL86CoD4fDgf4PcDfwDzqKjMQY+pZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABrUlEQVRIie3Vv64pURTH8e+MIWPMTCkSEaGkVxCm1ChQeg7RegGNF6D3CMILKEQpiPiTIWIUhISEPbc6t7p37o3kJKew2pX1+2Tv7Owlua7Ld5b8rekf4EcAilezXq//fmJCCFzXRVEUNE3j8XhwuVxQVRVZlmm1WtKfMv7rBF/hwWAQwzA4n88sFgtkWcY0Tc/ZfwJCCABM00TTNFarFdPplGQySS6XQ1EU7vf7X+c9r0gIgc/nwzAMXNdlNBpxvV6pVquk02n6/T7j8ZhkMvke4Pf70XWd8/nMZDIhFArRaDTQdZ12u812uyWTyRCNRt8DVFVlv98zm81IpVLUajV2ux3NZpPX60WpVMIwDI7H43vAfr/HcRwsy6JYLDIcDul0OsRiMcrlMkIINpsNz+fzPUAIQaFQIB6P0+12GQwG5PN5stksjuNg2zaSJBEIBN4DIpEIr9eLXq/H4XCgUqmQSCRYr9ecTicCgQB+v98rwhu43+/M53MkScKyLDRNY7lccrvdUFUVn88HgNeX7wkA6LpOOBwGwLZthBBomoYkSZ7BXyV9Fs4H+PnAL8ynqZVjX+7aAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABnklEQVRIie3VvYryQBiG4TsqSkhhohjBQgSJCHoCdpY2dp6AZ+EheDJ2gq0IliIiFiIqmlLTxB9ifma+bqv9XBEWtnDaYZ5r5p13GEVKyW+O2K+mf4A/ASSeTfZ6va8WC8OQeDxONptlu92iKAr5fJ7H4wFAv99Xvst46QRCCKIoIpPJcLlcsG2bSqVCNpvFdd2na18CgiBAVVU0TWOxWJDL5ajVamw2GzzPI5VKvQ9EUQRAoVBgt9th2zbtdpvj8ch+v8c0TRTl2+q8BgRBgGEYxONxJpMJjUYDy7IYjUYYhkE6ncbzvPeAMAxJJBJkMhlmsxlSSrrdLuPxmNPpRLlcxvd9wjB8D4iiCF3Xud1uzOdzOp0OUkqGwyGWZaFpGrfbjVjs/zFPgWQyiaqqrFYrTNOk2WwyGAyQUlKtVrlerwgh3r8DVVVxHIfz+Uyr1eJwOLBcLqnX6wghuN/vxGKx9wHf93Fdl1KpRKFQYDqdous6pml+9f+zcPjhJQsh0DSNVCrFer3GcRyKxSJBEBAEwY+7B1A+H84H+PvAP0xqrBVJAbtnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABmUlEQVRIie3Vz8o5URzH8ffMnDEZykJK0ZSSKWZjZ8OGXIErcSH2LsIF2IylG5ANpTRIjBmUJOY8u9/q9/CknnoWzvbb+bzOv29HkVLym0P91fQP8CcA8azY7Xb/PTEpJclkkiAIOBwOOI5DGIbc73cAer2e8r+MH+0giiIMw0BKyWq1wrIscrkcnudxvV7RNO3buS8BKSWqqpJMJvE8D4BarcZkMsH3fTKZDEJ8fxA/AhKJBJfLhfl8Tr1eRwiB67oUCgVM0+RyubwHSCkRQmAYBtPplGw2S7PZZDAYIITAtm2OxyOPx+N9wDRNfN9nu93S6XSYzWa4rkuj0QDgdDqh6/p7gK7rKIrCYrHAcRxKpRL9fh/LsrBtm81mA/D+JRuGwW63I4oi2u02o9GI5XJJq9UiCALO5/PT1b8E7vc7YRhSLpfRdZ3hcEi1WiWdTrNarRBCoGkaURS9B9xuN1KpFPl8nvF4DEClUmG/33O73dB1/Wn4SwAgHo/jeR7r9ZpisYimaQRBQCwWQ1Vf96ny+XA+wN8HvgCApqYNe4BMdgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABTklEQVRIie2VQW7yMBBGn+2JAQUIiwgRiXNwO3acjWNwAYRsAkqCEttdVO2qhernr9QF3o71Pb2xR6NSSvzm0b+a/gL8CYDcK2632wQQQqAsSw6HA1prNpsN+/2exWKBtZZhGNjtduqrjLsGSik+vrGIcLlcGI1G5HmO9x4RwRhDjPHbjIctijEiImitOZ/PFEVBjJGu6xARHs3RjwEAp9OJqqpo25ZhGLDW/h9AlmUAeO9Zr9c450gpkWXZ3fb8CACQZRm3242+76mqiuPxiDEGay0hBLT+PuYuIKWEUgoRwTlHWZaklPDeM51OiTE+b6CUQmuN957VakXTNFyvV+bzOX3ff975Z4MP/aZpWC6X1HVN13XMZrNPwFMG8D5oIQSKoqCua1JKTCYThmF4DqCUwhhD27aMx2PyPMc5h7UWEXn4wADqtXBegL8PeAPgqKAuM4gtvwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABR0lEQVRIie2VS47CMBBEn+OAMT8rEtlxGM7EQTgcZwBWXoRgEeXTzixGsJoJkdBIs6C3ra6n6lapVd/3/GUlf6r+AfwLQDrU3O/3fd/3TKdTRITT6cRut+N4PFJVFdvtlhACSikOh4P6SWOUA601TdMQY8Q5R1mWiAjGGERkcHY0IITAbDZjPp9TFAXGGLTWvMrRIOAxnKYpt9uN5XLJZDLBe89qtUJE3gMAKKXQWnO9XtlsNgB478myDBEhxvge4FH3+508z6mqirIsybKMuq6/RZLfZV6uKE1T6rpGa41zjsvlwnq9RmtN13WD4qMcJElCXdcYY7DWcj6fyfOcruuegKE1jbpB0zRYa9Fa470nz3PatkVE3ncAICJYa2nblhACWZY9c6HUj/kaB0iS5Bkk5xxFURBjZLFYPA/8CqA+D+cD+P+AL/xfnSU+9ACoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABwElEQVRIie3VW6spYRjA8b9tjBnlMLkgOU5MLoiS4oMpH8GHkm+g5JBSUoyZLMlhaIxM9t2+2svF1Kp14b196v/r6bl4fa/Xi598f360/gF+BSC8G/Z6vZff70fXdVqtFqZp0u/36XQ6DAYDDocD7Xab7XZLt9v1/a/xdgPHcSiXyywWC06nE9VqFcuy2O/35HI5rtcrjuMgSdK3jbfAbrcjGAySyWQYj8ckk0kURWEymVAsFnk8HliWhSzL3oDT6YRt25TLZabTKbIsk8vlmM1mRKNRYrEYX19fhEIhb4AgCJimSTab5fl8slgsaDabmKbJ5XKhUChgGAbBYNAbEI/HWa1WJBIJwuEwo9GIWq2Gbdus12vy+Tz3+x3LsrwBsViM/X6P67pomsZ0OkUURTRNYzgckkqlkCSJ3W7nDZBlGdd1MU2Ter3Ocrnkfr9TrVaZzWZIkkQ8HvcOCIKAKIrouk46nUYURebzOZVKBdu20XUdVVU5HA7eANd1URQFwzCQJIlMJsNoNEJVVfx+/7/7BAIBb8Dz+SQSiXA+n3Ech3w+z2az4Xa70Wg0OB6PGIZBqVT6tuH7fDgf4PcDfwHfjsQEUnj27QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABtklEQVRIie3Vy6qqABSA4V87RGCR2E6NQCIKIro+TOPeo2mv0rRHCYRw0MCUogthg9Qiu9h5gX06IGzYg9Z0wf+xRkt4vV785Ig/Wv8AvwL48245Go1euVyOIAhYr9cMh0MmkwlxHDMYDJhOpyiKgiRJjMdj4bvG2wts2yaXy2EYBqZp8nw+qVarbLdb4jhGURSCIEAQvm3/H0ilUmy3W0qlEtlsFsuyaLVanE4nPM/DMAyCIHiXeA/ouo7rusiyjKZpmKZJpVIhnU7jOA66riMIAlEUJQM0TSMMQ47HI+12G9u2iaKITqfDfD4nn88jSRLn8zkZoCgKoijiOA79fp/9fo/neXS7XVarFVEUoes6vu8nAzKZDPl8Htd1KRQKqKrKbDajXq+TSqVwXRfDMAjDMBlwuVwol8tsNhsAGo0Gs9kMTdNQFIXlcsnX1xfpdDoZcD6fUVWV6/XKbrej2WzieR6Hw4Fer4frusRxjKqqyS/IZDLIssxisaBWq/F4PLBtm06nw/F4xPd9isViMuB2uyGKIrIss16vyWaz6LqOZVkAtNttHMfhfr//syF8Hs4H+P3AX0IauF47P5AZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB3ElEQVRIie3VTYtxARjG8b+nUzSnNDTeamzP2dhNRFlRWFixMUmapZWtj+Ab0Cyk7DQ1NcqGxIq8lGY6SSOlEDWrEysnni8wWZyamsXc27uuX9fi7jZcLhd+cv79aPof8CsA4dqyWCxeAFRV5evri0wmg6Io9Ho9/H4/t7e37Pd7zuczhULB8F3G1QaapmE2mwkEAqxWK15fX/F4PMiyzGQyQdM0LBbL1QZXgcPhQL/f5+7ujmw2y3A4ZDKZEIlEEEWR8XiMyWRCFEV9QDAY5HA4UK1W8Xq9RKNR6vU6+/2eRCLB8XhkNpvhcDj0AQDJZJLPz09eXl5Ip9PIskypVMJoNBIKhVgulyyXS31Au93G6XQSj8d5e3tjNBqRz+cRBIFarYbH4+Hh4YH393d9gKqq9Pt9fD4fXq+XcrmMqqrkcjmm0ynNZpNAIMD9/b0+QJIkFosFHx8fpFIp7HY7z8/PSJLE09MTrVaL+XxOJBLRBwiCgNvtZjAYsF6veXx8ZLfbUalUCIfD+P1+Go0G2+1WH6BpGlarFZvNRrfb5ebmhmQyyWg0otPpEIvFkCSJzWajDwA4nU64XC40TaPb7SLLMsFgkFarhaIoWK1WDIZvjxgAw9/D+QN+P/AfKku6gKxUsQ4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB2UlEQVRIie3VsUsyARjH8W/64iIYOFyI0UXi5HAHxnHLSfNFhEet4eTm3yD+C0F/QQ0OtnhQByq4HCGHi6ARmItwnF6BEIZIcu/2TtELB0FDz/rA78NveHi2giDgOyfyrem/wI8A/ny1rFarAUAikcC2bba3tzEMg36/z9PTE5lMhmg0CkCtVtv6LOPLBkEQ4Hke6XQawzAYj8c8PDygKAqCIOC67n8bfAkcHBzg+z63t7dks1l0XafT6fD8/IymaWw2G+bzeXggFotxdnaG67rU63VOTk7I5/M0Gg0ikQiFQoHFYsFyuQwH3N/fk0gkOD8/x7IsWq0WFxcXxONxrq+vSaVSyLLMdDoNB6zXa9rtNvl8nuPjY25ubvB9n0qlgud5WJZFLpdDFMVwwOHhIZPJhG63y+npKaIocnl5STKZpFQq4TgOw+EQTdPCAYIgIMsyjuMwHo8pl8usViuurq5QVRVN07i7u2M2m4UDPM9jf3+fvb09TNNkvV5TKpV4fHyk2WxSLBbJZrP0er1wwPv7O29vb0iSRDwexzRNdnd30XWdZrPJYDBAkiR2dnbCAQCLxYKPjw8UReH19RXbtjk6OkJVVUajES8vL/+u+bPZ+n04v8DPB/4CZxe5GAAoOSsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACD0lEQVRIie2Uv0tqYRjHP91EQQLFVJKgyMAlRTdTokEpiBAqCiT88Xf4F7i4uOiguGkggpPkoeAsZ/JAYwQHtBxCjFyEssHetjvpAeHGvRf6rs/L58Pzvu/zLAkh+M78+lb6j+CfEBj0itlsVpjNZpxOJ6qq0u/32djYwOv1YjQaeX195e3tDYBMJrM0i6HbgRCCbrfLZDLh7OyMWCzGeDzm5uYGTdOwWq2sra1hMpnmMnQFKysrfHx8UKlUqNVqrK6ukk6n2d3dRdM0ZFnm5eUFh8Mxl7GkN2jlclmEw2Hu7u6oVquMRiMODw85OjpiOp1ye3vL/f09drudYrE484p0BZFIROzs7HB6esr6+jrtdptGo4HBYODk5IRgMMhgMEBRFHK53OKCYrEoZFlmOByyv7/PxcUFn5+fVKtVZFlma2uL4+Njtre32dvbW1zQbDaFxWJBVVUkSWJ5eZnz83Oi0Si9Xo+rqys0TcPn81EoFBYXpFIp4fF4CIfDCCFot9soioLL5eLy8pJAIECn00GWZUql0kyB7hy43W6enp54fHwkEAgQi8UIBoNIkkQ+n8fv93NwcIDf75/L0P2mZrMZj8eDw+FAVVXq9Trv7+8kEgmSySSj0YhWq8V4PJ7L0O1gOp0CYLPZsNlsPD8/c319zebmJqFQiHg8zsPDw+9zs6L7Bn8i//82/RH8fcEXNujWm2rs+xsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACCUlEQVRIie3UsUtqYRjH8a+3DqdwSQRLcJIzOIjTUUHB02BnO2CKS1M4u7fo1NLfEELgIgiCiyIEmS51hEDBFBw8OoTYUpBQHIJzp7tdDwg37r3Qs768vw/v877P67Asi6+sH1+a/g38E8C23WKxWLQA/H4/LpeLfr/PaDRiZ2cHr9eLKIr8eoXn5+eOjQGAp6cndF1HlmUikQiSJPHw8IBhGOzt7eF2uxEEYe1+2xY5nU7S6TSyLNPpdLi8vOT5+ZmjoyOSySQOh4PZbMbLy8vaDNsT9Ho9TNMklUqRSCSoVquUy2UkSULTNI6PjxkOhwwGg7UZDrtJPjs7s7rdLk6nk0wmQzweZzweU6lUmM/nRKNRDg8PEQSBTCbz2zuwBZrNpvX29sb19TX39/dIksTp6Sl+v5+bmxsajQafn59Eo1EuLi42B05OTqxYLEYwGMQwDGq1GoZhoCgK2WyWra0tWq0Wd3d3tNvtzYF8Pm9Np1MODg5QVRWfz4eu69TrdVarFZqmoaoqk8mEXC63OVAqlazX11ceHx9ZLpcEAgEURUEURW5vb+l0Ong8HsLhMIVCYfM5WCwWuFwu4vE4i8WC4XDI1dUVsiyTSCQIhUL0ej0+Pj7WZtgCpmmyXC7Z3d1lf38fr9fLZDJB13VGoxHhcBiPx8P7+/vaDNsW/Yn6/3/Tb+DvAz8BqnfSPS/k/70AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABp0lEQVRIie2UwarqMBCGf4sllCYaKbTZuBa661Lc+DQ+iC+jS1/Bl9C9Impt0WpttWnSu7rd3HsKyj3cc+AMhCyG/N/M5GdaVVXhM8P4VPUfwJcAtJuS0+m00lrDsizYtl0fwzBQliWklPU9mUxaLwOUUjidTpBS1mJlWYIQAtu2wRgDpRSWZb3XQRAE8DwPUkrkeY4sy5CmKZIkweVyQZIk2O12eDwe7wHm8zmUUnAcB57nQQgBIQQ8zwMhBKZpot1uQyn1HmA0GiGOY0RRhPV6jeVyiTiOQQgBpRSMMTDG0O12MRwOXwf4vg/GGEzTrKvVWiMMQ+z3exwOB4RhiPP5/KFGq2kXjcfjyjCMusperwfHceC6LlzXheM44JxDa40gCP7qokbAYrGortcroihCGIY4Ho+IoghFUUBKid9vhRCYzWav23S73YJSin6/D9/3QSkFIQRZluF2u+F+vyNN08YRNQJWq9UfDiGEgDEGzjk6nQ4opeCcvwcYDAbQWkMpBaUUyrJEURTI8xybzQbP5xNSSgghPtRo/IN/Ed9/m/4A/j/gFz2LzEtD/ZHFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABqElEQVRIie3VMYrCQBTG8X8yyUAcMRESiYWFja2dYCFYegjPsHfwJF4mh7CysYlJNKAhjoSY7YSFVVBY2MJpppjh+zHv8RijaRr+cpl/mv4B/gVgPTv8+vpq4jjGNE0sy8K2baSUP3YhBLZts1qtjJeB0WjEZDKhLEvO5zOn04nT6cTlcqEsS+q6pqqq91+w3W5ptVp0Oh08z2MwGOC6Lk3TcL1e0VqjtaYsy/eANE2J4xitNVVVUVUVt9uNbrdLr9cjCAJ838d13YcZxrNJjqKocRznR4nyPGe/35MkCVmWcTwe0VoTRdGvPXgKLBaLpt1uEwQBYRjS7/cJwxDHcZBSIoTAsiy01szn89ebPB6PyfOcNE3ZbDZkWUZRFCil8Dzv3psgCJjP579mPAWm0ymO4yCEwDRNDMOgqiqSJCGOY/b7PVmWkabpw4ynwHq9RghBq9VCKYVSCtd18TwP3/cZDocopciy7D1gNpuhtaYoCoqi4HA4sNvtqOv6fkcIQRiGLJfL1wHDMOh0Ovi+j5QSKSUAWmsul8t9Dq7X6+OMz4fzAf4/8A2Epb+y55y6ewAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB40lEQVRIie3UwUoqcRTH8W+Xv+NfF4IMNcQ0YA3DLEQEa9UqYoja+zQ+Qa8itXHZE4SLIEFmCjERcQaswGkGmcLRdneXoNy490Jn+4ffh3POn7O1XC75zvr1rek/wD8BiFWPl5eXSykl+XyeXC5HNpsliiKCIOD5+ZnX11cymQyWZdFoNLbWBnRd5/39nbe3N56engiCgCRJkFKiqirHx8fs7OwwHo836+D29pY4jvn4+KBYLFIulymVSuzu7iKlZDQaMRgMmE6nmwHb29scHR1hmiaFQoEwDBkOh1xdXeG6LkmSIISgVqttBpycnDCZTLi5ucHzvN8jsm2b09NTyuUyBwcHRFH0ZcbWqlNxcXGxjOMYVVXZ39/n8PCQarWKEILZbIbnedzf3zMajWg2m+sv+ezsjEqlgqZpSCl5fHyk1Wrhui79fh9FUTAMA8uyvsxYCWiaxsPDA9fX13S7XdI0pVAoYNs2juOg6zrZbJYkSTYD2u02vV4PwzCo1+sYhoGmacznc3zfp9Pp4Ps+YRhyfn6+PrC3t4fjOCiKwmw2IwgC2u02Ly8vLBYLFEWhWCximuZmHaRpyt3dHZPJhOl0ihACVVUplUpIKRFCkKYp8/n8y4yVv+hP1P9/TX+Avw98Apdmw5hBy8iGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB40lEQVRIie2UzWriYBSGH+NPhQaMqW2SjdkEFIlCbqALoXfQZb2UbnoHvYHeSMG1m4IpWgqKCQTxLygV24aU+HU1sxodWqbMDPTAWX3wPLzf4ZyUEIKvLOlL6d+Cf0KQ2fd4eXkpUqkUAFEUsdlsfnYul0PTNEqlEm9vb1xdXaU+LPgBW6/XSJKELMuYpolpmhwdHZEkCU9PT/i+/7kEq9WKUqmEbdtomkahUCCKInzfx3VdfN9nuVxSr9c/Jzg/PydJEhaLBff39/R6PYIgIJPJoKoqjuPQaDTYbrc7Gal9m9xqtcR4PGa9XpPP52k0GjiOg2VZHB8fM5vNGA6HPDw8cH19/fEZZLNZms0mtm1jmiavr6/M53Pa7TZ3d3csl0uEEGia9rkEt7e3IkkS+v0+3W6X0WjEdDpF13UqlQq1Wg3Lsjg8POTs7OyXCfYKLi4uhOu6yLKMYRjU63Ucx6FYLCKEYDKZ8Pj4SBAE3NzcfPyLyuUyp6en6LpOoVAgDEMGgwGe5zGdTomiCEVRUFV1J2OvQJZl4jim0+ngeR7Pz8+k02kURaFarXJyckI+n0eSdh+E3y6a67qk02kMw0DXdVRV5eDggJeXFzabDWEYEsfxTsbeGfyJ+v+v6bfg7wveAcc+0Pm8ZAoIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB5ElEQVRIie3VsUsyARjH8e+drwRy04l3LTc4nIiNFgUi4SJCg5vIDU43ReDq0tAW9AccDi62HITTUcOBEUKE2qCTDqbCqS2RQtBQhO9fkJAQNPisD8/vw/Msj7BcLvnNEn81fQP8CeDfqub5+fkSQJIkQqEQo9GIbreLpmmk02lmsxnT6RRBECiVSsKPAVEUkWWZra0tut0uw+GQ3d1dDg4O6PV6XF9fE4lEkGX5+4xVgKqqANzf3zOZTMhkMhweHlKv16lWq+i6TiaTIRAIrAfM53MajQaiKFIoFAiHw1iWxc3NDYZhYJom/X6fu7u7bzNWnqjZbBKNRkmn0zw/P2NZFn6/n9PTU3Rdp1Kp8Pj4SDKZXA9IpVLEYjEeHh64urpiZ2eHYrHI6+srZ2dnvLy8kM/n0XV9PSAajVKr1Wi32xwdHWEYBq1Wi3K5jKqqHB8fEwgEcF2XVCr1c+Dy8hLP8zg5OSEej2PbNo7jsL+/Ty6Xw/M8XNfl6+trvQ1UVSWbzRIMBrm4uGAwGJDL5djb26PT6dBsNlEUhe3t7fUASZIYj8fYts37+zumaaIoCre3tzw9PaFpGrIs8/n5uR4AsFgs0DSNRCIBgOM4vL29EYvF8Pl8fHx8rJwXNg9nA/x94D+aI6yhDB3r2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB0UlEQVRIie3VsWviYBjH8W/OG1t0KSGLKZ2cBANFQ0lAcIxS6NqlS1vqf9AluPsXiJMgCFVBFEotdXJqXRyEEgrpZF8FoaYglBh6w8FNd727QKFD3/XhfT7PD56XV3p7e+Mjz7cP7f4FfArg+3tF27Z/rdjLywvT6ZR4PI5hGAghuL29RZZlIpEIpVJJ+l2Pf0own895enpC0zRM02QymVCr1QCQZTl8At/3EUIQBAG5XA5FUbi8vGQ0GmGaJvv7+4zHY56fn8MBrusiyzKGYQBQqVSYzWYcHR2RTqfpdDp0u110XQ8HJBIJdnd3eXh44OLigmg0im3bxGIxyuUy9/f3WJaFaZrhgFQqxXA45Pr6mkwmw+HhIUIIzs/P8X2fs7MzFEXh5uaGbDb7/0C73cZ1XQ4ODrAsi36/T71eR1VVjo+PeX19pdVqsVgswiXwPI+TkxN2dnaoVqtcXV2Rz+cpFAo4jkOv12NjY+OP0/8V2NvbQ5IkyuUyQghOT0/RNI3BYMDd3R3b29skk0nW63X4BI7jAFAsFtnc3KTZbPL4+Iimaaiqiud5LJfLcAD8fEi6rhMEAY1GA9/3MQyDra0thBCsVqt370tfH84X8PmBHwxgucwdJGv4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB10lEQVRIie3VvWoqARCG4XePByy0MeIPBkQMikGxMC42plgt3NJCUpgr8Qq8E5tgsNFC7UKM2qisKCzCsgRFxCKiWMjGPTdwYkAIpHDaYb6HKYYRTNPkJ+vPj6ZfgF8B/D3VLJfLptVqxeVy0el0sNls5PN5KpUKhmEQj8fZbrcAlEol4X8Z327gcDhYLBas12vS6TSz2QxFUZAkCb/fj6ZpWCyWL+dPAjabDUEQmE6nRKNRvF4vjUaDUCiEKIo0m012ux23t7fnAVdXV2iaxuFwIJvN0uv10DSNYrHIYDBgNBohSRLH4/E84HA4oKoqqVQKi8VCrVYjk8kQCASoVCpEIhHC4TDdbvc8QFEU7HY7d3d3NBoNDMPg8fGRdrvNcrkkl8uh6zq6rp8HvL+/k06n+fj4oNVqUSgUAKhWq9zf3+Nyuej3+3g8nvMAv9/Pzc0N9Xodp9OJLMs8PT3x+fmJLMsoisJ2u8Xn850HiKKIqqqMx2OKxSLz+ZyXlxdkWcY0TYbDIdfX16ciTgOr1YrRaEQsFiORSPD8/Izb7SaZTPL29oYgCDgcDgzD+DLj5CXv93s8Hg92u53X11cmkwkPDw9sNhtUVSUYDPLdPxEuD+cC/H7gHyv6utwJPHGGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB20lEQVRIie3VT4tpARjH8e+5Xc0kxEoTU1IzJaLEYijOZrKSMouxmndi5ZXYjh0LkQ0lw8lCFmYxWEhG/o90jj9x38C9bp2amoVn+/T7fXpWj3A6nfjO+fWt7RfgRwC/zy2TyeRJEAQWiwWr1YpYLIYkSdTrdV5eXhgOhyiKgiAIpFIp4W8d/71gv98znU5xu93sdjve3t4IBoPc3d3RaDTYbDZotdp/5s8CgiAwm80wGAy4XC7K5TIajYZIJEKhUGAymeD3+7m+vlYHbLdblsslPp+Pz89P6vU60WiUw+FALpdDFEWMRiOSJKkDRqMRVqsVi8VCNpvFZrMhiiKvr6/o9XpEUaTZbLJer9UBiqLg8/nodDr0ej0SiQQfHx9UKhXi8TiyLNNsNnE4HOoAp9OJVqulVCrh9XpxOByk02nsdjt+v59KpcLV1RX39/fqAI/HQ6PR4Ovri+fnZ6rVKt1ul6enJwaDAe/v77hcLo7HozpgPp8jSRLhcBiTyUQmk+Hh4QGbzUaxWMRsNnNzc8N0OlUH9Pt9bm9vCQQC5PN5ZFnm8fGRdrvNeDzG6XSiKAqyLKsDACwWC+12m1arRSgUQqfTUavVsNvtGI1GFovF2bxweTgX4OcDfwCkIcF5aAUS0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABh0lEQVRIie3VQY6yQBCG4ReaBkEIGAGN1/BOehXjEdzOlVy6F1SStoEQhMAs/mRW/7hwMsksrG0l35OqVFLGOI78Zpm/mv4G/gRgPWseDodRa83xeGS73fLx8cE4jmw2G/b7Pev1miiKOJ1O7HY7438ZTyeQUtI0DZPJBCEESimiKMI0TbIsYzab4TgOWZZ9m/EUsCyLqqoIgoC+7ynLkjRNUUoB4Ps+dV0jpXwNkFJSliVhGNJ1HWVZslwuud1uGIbBdDpFa43jOK9PUNc1URTRti1VVbFarbher0gpCYIApRSu674GDMNA27aEYUhVVbRtS5qm5HmObdt4nofW+nWgrmssy8L3ffI8Zz6fYxgGRVEQxzFt2/J4PLBt+2eA53mcz2fSNKXve5RSLBYLqqoC/q3yZcC2bVzX5XK5EMcxXddxv99J0/QLEEK8BjRNg2VZSCkpioIkSb6uKUkSyrL8GWCaJkmSkGUZQgjiOEZrzTAMBEFAXdcIITDN72OM98N5A38f+ASHka7vyVKiegAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABg0lEQVRIie3VT66qMByG4RcVKERQhKAsxg24KSdsx7iNswcXIEqMVP4ZoHBmju715mpOcgZ22uR7+jW/tNowDPzkGv1o+gf4FcDk2eZ2ux0OhwPr9Zr7/c5utyOOY/b7PWmastls+Pr6Yj6fE8ex9qeMpw3atgXAtm2klAghEEKQpimmaRIEAafTCcMw/prxFFBKoes6QggulwuLxYKu68iyjCAIKMsSpRSWZb0GtG3LeDx+nDoIApRSSClZLpfkeQ6AEOI1oGkaDMPANE3O5zNhGNI0DVJKVqsVUkqA9xpYloWmaVyvV8IwpK5ryrIkDEOyLGMymbzXwHVdpJSMRiM8zyNJElzXRdd18jzHsiyevWf/bOC6LlmWoes6s9mM4/GI7/sMw0BRFDiO85i2/wa6rsNxnAfgOA5JkuD7Pn3fU5Yl0+mUpmleA/q+x7ZtbrcbQghs2yZNUzzPQyn1PhBFEVmWUVUVURRRVRVFUeB5HnVdo5RCCPH0irTPh/MBfj/wDdZ4wYIGxPjzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB2UlEQVRIie3V3UoqURiA4deNM4E4hpSCSpbmSDLjD4jiWV5QF9AdCF2Il9AVeNqfjE4zkkIOMwXhqBG1knAf7NOtBwNBB63TD96HBd9ihdbrNd95/nxr/Rf4EUB427DT6azD4TAPDw+0220cx+Hy8pKLiwu63S6u63J2dkav1+P8/Dz0v8bWG/i+T6vVYjAY8PLyQrVa5fX1FcdxKJVKzGYz5vM58Xh8Y2MrMBqNkCQJVVW5vb0lkUiQSCS4vr5G0zSEEHiex/7+fjDAdV0WiwW1Wg3DMJBlmaOjIwzDIB6Pk0wmsW2bvb29YIAsy4xGI/L5PF9fX9i2Tb1ex/M8fN9H13Xu7++JxWLBgEKhQL/fJ5PJoCgK/X6fcrnMx8cH4/GYk5MTlsslT09PwYBsNst0OmW1WqFpGsPhEEmSKBQKXF1dcXh4SCQSwTCMYMDu7i5CCCaTCY1GA8uy+Pz8RNM07u7uiEajpFIpTNMMBsiyTCQSwbZtcrkcOzs7WJaFruu8vb3x+PhIpVJhPB4HA4QQZLNZTNNEURQODg64ubmhWCwC/9b4+PgYSZI2Nra+ZCEE6XQa0zQRQqCqKoPBgPf3d5rNJs/Pz4RCIU5PTzc2Qr8fzi/w84G/UXDAuP/ofJgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABvElEQVRIie3VTasxYRjA8f8MaVYMhmhKeVdGI/KhrOyVhW/h+9gzyeJGJoo0ZpKXlJeFs3tW53hq6tRZuLdX/X9dXYtber1e/OaTf7X+Af4EEHw37Pf7r0gkwul0wrZt2u02g8EASZLodDp0u12KxSKaptHr9aTvGm83GA6HqKpKqVRiNBrxeDwolUqs12sejwe5XA7HcZCkb9v/BxRFYT6fo+s6qqoymUyo1WpcLhe22y2GYeA4DrL8c+YtYBgG0+kUVVXRdR3LsshkMiiKghCCbDZLIBDgcrn4AyqVCufzmd1uh2ma2LbN7XajXq8zGo2Ix+PE43E8z/MH5HI5gsEgQgiazSau6+K6LqZpslqtuN1u5PN5HMfxB4TDYVKpFEIINE0jkUhgWRb5fB5ZllksFv/u4As4HA4YhsFyuUSSJMrlMpZlkU6niUajCCHIZDKEQiF/wH6/p1AocL1e2Ww2VKtVXNfF8zwajQaz2QyAcrnsD/A8j1gshqZpjMdjDMPgfr9j2zaNRoPtdsvxeKRYLPoDzucziqKg6zpCCFRVJZlMMplMkGWZVquFZVk8n88fG9Lnw/kAfx/4Arofst/asv+OAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB30lEQVRIie3VT4tpARzG8S9zlc1EymTHQholC8p0ykKSkoUU02wm0pHsvABLL2FWJmrKv9I0nc7KRtEkWSjNllghC0UTFqfcN3CvxampWfhtn3o+Pauf5nw+85On/dH2K/ArgD+XwlKpdAbY7/es12sKhQKDwYBms0k6ncZqtTIej1EUhWKxqPlXx8UFiqJgMBjw+/3M53Pe3t4QBIGHhwfa7Tan0wm73X5xwUVgt9vR7XYxmUyIoshgMODz85OnpycMBgONRgOj0YjFYlEHxONxDocDr6+veL1eIpEIrVaL1WpFLpdju93S6XRwu93qAEVRyGazTKdT3t/feXx8xOFw8PLygl6vJ5lMMhwO6fV66oByuYzFYiGRSCDLMuPxmHw+z83NDdVqFZ/PRygUQpIkdcBms0GSJAKBAIIgUC6X2e/3iKLIZDJBkiSi0Sj39/fqgHA4zGg0ot/vk8lkuLu7o1Kp4HA4eH5+RpZlvr6+yGQy6gCdTofX6+Xj44PZbIYoiiyXS6rVKqFQCEEQaDabLBYLdcDxeMRms2G326nX69ze3pJKpRgOh3S7XWKxGE6nk+l0qg4A+P7+xuVycTqdqNVqeDwegsEgsiwzmUwwm81otf+v0VwfzhX4/cBfvyi85A73ChcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABzElEQVRIie3Vv+t5cRTH8SfJJEk2SYY7CIuBKGX3I8Wg2zUoC/+DyR8hGUiRkgGDwqA+KyVdksEgMrz/Addyv//A5+tTtz71GZz11OtxznKOSdd1frPMv5r+Af4EYHnXbDQaOoDD4WCxWOByuahWq8xmM9brNclkEqvVCkC9Xjd9l/F2A13Xud1ueDweFEXhdDqxWCzIZrNIkoSqqj9u8BYIBoMIIRgMBkiSRC6XYzKZoKoqsiyjaRrn8xmT6dvhfwYsFgvlcpnH48FgMCCTyRCNRun1epjNZmRZ5nq9IoQwBrTbbex2O4qisFwuWS6XKIqCzWaj2WwiSRLpdJrtdmsM0DSN4XBIJBIhlUoxHA4RQlCr1bjdboxGIxKJBOFw2BggyzKqqjKdTikUCni9XprNJk6nk3K5zNfXF5vNhlKpZAwIBAJkMhlWqxXH45Farcbz+aTVahGPx0kmk/T7fa7XqzFgu90Si8UIhUJ0u100TaNSqXA4HJhOpxSLRfx+P/P53BgghOB+v5PP57Hb7XQ6HXw+H7lcjvF4zG63IxKJ4Ha7jQEAl8uF1+uFoijc73dmsxnpdJpEIsF+v0cIgcXy/4Ng+jycD/D3gX/u7LnkugY3iwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACC0lEQVRIie3Uv0tqcRjH8Xd6kUCUyEUQAycho0wHF380RQ6e6OjWv6CLo4NjELS5GAq6CCL4K3EoIY6DgwjCsUU3ySXO0uIgIed7/4Gu4OXGvRd61gc+Lz7D8+wIIfjKMXxp+jfwTwA/Ni1vbm6E1WrF4XDQ7XZRVRW/38/V1RVms5nZbIamaQghyGQyO59lbGyg6zrT6ZTVakUqlSKVSqFpGre3tzw/P+NyufD5fFit1l9m7Gy6g1wuJ1RV5eXlBb/fjyzL7O3toSgKT09P7O7ucn5+jtfrJRgMftpgI1AsFkU4HGY4HFKpVHh/fycajRKLxViv19TrdQaDAU6nk2q1uj0QCATE6ekpiUQCu91Or9ej1WphNBqRZZlQKMRisaDdblMoFLYHSqWS6HQ6aJpGOBwmHo+j6zq1Wg1FUXC5XMiyzOHhIcfHx9sDvV5P7O/v0+/3eXh4wGAwkEgkODs74/X1lWq1ynQ6xev1ks/ntwdCoZCIRCJIkoQQgkajgaIo2O12rq+v8fl8jEYjHh8fKZfL2wPZbFaMx2N0Xefi4oJgMMjb2xvNZpPJZMLJyQmSJDGfz0kmk9sDd3d3wmQyMZ/PmUwm2Gw2Li8v8Xg8qKpKq9XCYrHgdrt/79A+Pj5YLpc4nU4kScJsNpPL5bi/v+fg4IB0Os3R0dGmiM0N/sT8/9/0G/j7wE9Oh+Wdv1EddAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACC0lEQVRIie3UMUtqcRjH8W+XKwUecDggOBQcqiFEBE0IjkMYLnFsqEjoTE6Cb8DNoqG1tyHqVJ1jamnRMV0ScbFB8Dg1NCToJvS/b+AmHLhx74We9c/z+/A88PwXhBB8Zf340vRv4J8Afs57PDs7EwChUAiv10u5XOb29haPx0MgEMDtdvPx8QFALpdbcAwA2LbN/f09sViMeDxOOBzGMAwsy2J5eZnV1VUWFxc/7Z+7IkmS0HUdVVUxDIPz83NeX19JpVKk02mEEFiWxWg0+jRjYd6h7e/vi0gkwu7uLm9vb+TzeZ6fn9nY2ODo6IiVlRUeHx8xTZNKpfLbFc0FcrmcuLu7Q5IkDg8P2draot/vUygUsG2baDSKpmksLS0RjUadA+12W7y/v3N1dUWr1WJ9fR1d11EUhYeHBy4vL5nNZmxvb3N6euoc2NvbE5qmEQ6HGQwGFItFbNtGVVUODg5wuVyYpkm9XqfZbDoHMpmM6HQ6KIpCMplEURSazSalUonJZEIikUDTNF5eXjg+PnYOWJYlbNvm5uaG4XDI5uYmmqbhdrup1WpUq1VkWUZVVbLZrHPg4uJCrK2t4fF46PV6VCoVptMpOzs7xGIxxuMxjUYDSZI4OTlxfmiTyYRut4ssy/j9foLBINVqlevra56enkgkEvh8PqbT6acZcyf4E/X//6bfwN8HfgHi8t0rVO3lWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABxUlEQVRIie2UTariQBRGT8rGF41i1ChGBMeCohtRd+BCXIZjV+FMQVehc0f+VfyLBLRiqDdz9BRs+tHd8C7cUcE53O9WlaG15jtLfCv9R/BPCH69Ouz3+xqgUCg82nEcYrEYt9uN6/X66G63a7wtUEqxXC6Zz+copVBKEUURlmXhOA75fJ58Po9t208Zxqt3MJlMdKVSIQgCLpcLvu/j+z6e5yGlREqJ53n4vs9sNvtygpeCTqejAUqlEuVyGdd1cV2XdDpNIpHANE1M00RrTa1We18wGAz0brfD8zx2ux1SSk6nE/F4nGQySSqVIpVKkclkGA6H7++g0WhgWRaxWAwhBEIItNZIKdlsNmy3WzabDfv9/inj5QStVkt/fHxg2za5XI5sNovjOBQKBYrF4uNmGYZBvV5/P6LpdKoPhwPr9Zr1es1qtUJKyfV6JQxDwjDkfr9TLpcZjUbvRzSfz7Ftm2q1SrPZxLZtTNMkCAJ83+dyuXA+n9lut08ZLwXj8ZgoihBCYBgGQggSicQjIsdxHlH9lqDdbhNFEUopwjBEKUUQBByPRxaLBcfjkfP5TKvVotfrfcl4uYM/Uf//b/oj+PuCT8+q2Q7IpcxAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABr0lEQVRIie2UsariQBSG/yzLREcTkgw6EVuxsQ55g1Q+hY/hq9gKPoithWJnK6iZRI0hMEoSZqvNsqABYS97L9wDfzXw/XPOf2Y0pRQ+sn58KP3b4FMY/Kw7nEwmarPZgBCCdrsNSilarRYopWg2m5V0Xcd0OtWeMbS6NZ3P56rT6eB8PiOOY0RRhCiKkKYp7vc7pJSQUqIsSyyXy6cGtR1st1sYhgHGGFzXxWg0gm3bUEpVcCklsix7yag12O/3OB6PyPMcRVGgKAoopeA4DjjncF0XnHMwxl4yake0Xq8VIQRpmuJ2uyFJEiRJAiEEwjCEEAJCCGRZhtVq9X4GQRAoy7LAOQfnHL1eD5zzKlhCCAghyPMcnue9n4Hv+1XAu90Ol8sFWZaBUgrDMGCaJkzTRLfbhed5Txm1BkEQQNd1aNqfyz0eDwghcDgcEIYhTqcThBAvGbUj8n1fNRoNWJZVyXEcMMYq2baNMAwxHo/fz2CxWKjr9Vq9gTiOkSQJyrKsNgoABoMBZrPZ+xkAQL/fx3A4BKUUlFJomvbXRv3Wq6rt4F/U1/9Nvw3+v8EvDArZXBQwoBgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB90lEQVRIie2UvWoicRRHj+sHkagYIgoDE/xgiAZF05gUQqzSWaXMC/gS6e19CEsRsbezCKIgoxOn+IOKSiBBCMYZkUxeYB0wbNhdyG0vnMP9Xe51WJbFd9avb6X/CP4JgcuuWalUrJOTE0KhEKenp/j9fhaLBaqqMhqNEELgdru5ubnh4eHBcbCgUCiwWq2Yz+d0Oh00TePt7Y3j42MkSeLu7o54PI4Q4msTVKtVXl9f2W63RCIR8vk8yWSSs7MzfD4fk8mE8XjM8/Pz1wSyLHN7e8vFxQXBYJCXlxeEENRqNQaDAev1GofDQT6f38tw2F3y4+OjNZvN6Ha7qKrKcrnEMAwURSGdTpNOp1EUhff3d66urn67A1tBsVi0NpsN4XCYWCzG5eUlmUwGp9OJYRhomkav10MIQbPZPHzJpVKJbDZLKBTC5XKh6zqtVovhcMh4PObo6AhZlslms3sZtgJJkhgOh/T7fVRV5ePjg2AwSCqVolQqEY1G8fl8GIaxl2EbUblctlRV5fz8nFwuRzweR5IkTNNECMHT0xOaprFYLGi324dHlEgkuL+/x+12s1qt0HWder3OdDplt9vh9XqRZZlMJrOXYSvY7XY0Gg10XWe5XOLxeFAUhevrawKBAB6PB9M0MU1zL8M2oj9R//83/RH8fcEniRTOOO5uVAwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB70lEQVRIie2UvWrqABiGn6ipWLARg2iLiNo0CP4FcXQSSgcnb0Dvo6u71+Hq4A04dao/2BoERQiIGosQUVoR0q1TK1hOOedAv/WD9+F9vx/Btm1+shw/qv4L+CcArmPNWq1mC4KAw+HAsixeXl6Yz+eYponb7ebm5gZVVTkcDtzf3wsnA0zTZLFYsFgsEEURWZZJp9NkMhnC4TAA6/WaXq/3PQeTyYRYLEaxWCQajSLLMq+vr4xGI5rNJqPRiNlsRj6f/1JDOHZo3W7X3u/3GIbBcDik2+0ynU5xuVz4/X40TSOfzyMIAuVy+dOIjgKq1aptGAabzYbz83Oy2SyapnF9fU0oFGK5XDIej+n3+9Tr9dNn4HQ6ubu7I5lMEolE2O12mKZJu93m8fGR1WrF4XAgGAx+qXHUwcPDg/329kav1/uIxzRNgsEgqqqSSqVIJpNIkoSmaadHVKlU7E6ng8/nIxwOo2kauVwOWZYBMAyDTqeDrus0Go3TI1IUhVKpxNXVFZIkMZvNGAwG6LrOZDJht9sRCAQ+VvZkgNfrZbvd0mq1eHp6YrPZIIoil5eX3N7eoigKPp+Ps7Oz7wEsy6LVauHxeEilUiQSCeLxOBcXF6xWK5bLJc/Pz1iWRaFQ+FTj6Az+RP3/3/QX8PcB76ygzyCv+0ZtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB+UlEQVRIie3Uv0tycRTH8Xdy20QQAnUSREEuIZaCoOAmgoIgyN1MFBRaGlocdKupJSJc/IEOCnJdFLU5p5BClFaju7QIahCR0XD7CxKSJ57ngc765ft5cTiHs6WqKj9Zmh9N/wX+CUBY93h6eqoCGI1GHA4Hg8GAdruNKIokk0kURWE6naLRaMjn81vfBgRBwGazodPpkGWZ4XBIMBgkHA4zmUxoNpuIosjOzs5mHezv7/P29ka5XGa5XJJIJHC73bTbbfr9Pl6vl0gkwng8/jJj7QwURaFQKKCqKtlsFrvdzsXFBd1ul3g8Tjqd5v7+nqurq806qFar+Hw+JEni6emJQqHA9vY2uVwOq9VKqVRiOBwSCAQ2A+LxOB6Ph8FggCzL2O12Dg8PeX5+5uTkhNlsRiaTwel0fpmxte4WjUYjtVarcXNzQzgcJhaLcXd3R7lcxmg0kkql0Gq11Ot1isXi97fo/PwcRVE4Ojpib2+PVqtFr9fD5/ORTCaZTqcUi0Xe39+/zFgLmEwmJElCr9dzdnbGw8MDBwcH+P1+rq+v6XQ62Gw2dnd3NwN0Oh2Pj49UKhVWqxXHx8cYDAYajQa3t7e4XC7MZjOvr6+bAQCLxQKLxUI0GuXj44PLy0vm8zmhUAhBEHh5eVn7f+2Q/0T9/9f0F/j7wCcD0r5EC7k4YwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB9UlEQVRIie3UQWTycRzH8XdPHUfpuGymdKjTxLQxEZGoVpFYurUOdRhdOk2HaIdNpykxpkvqNCmJGrtEIh3qP3UZo6Ky2GnKrOf0PLfF8szzPOxz/fl9Xr+f389XtFwu+cr8+NL2b+CfACSrFmOx2BJAJBIxnU5ptVrodDp8Ph9PT0+Uy2W2traQSCScnZ2JPg38Sq/XYzAY4HA4ODw8pNFokM1m0Wg0bG5uMplM1rvBYrGg2+0yn88JBAKoVCry+Tz1eh2j0YjH46HdbvP8/LwecH9/j1qtxuv1AnBxccFwOOTk5AS9Xk+hUOD29haDwbAeYDQasdlsdDodMpkMUqmUaDSKTCYjkUjw8PCA2+3GarV+2CFaNSqazeayUqlQLBY5ODjg+PiY8XhMKpXi7e0Nv9/P9vY2pVKJeDz++UdOJpP0+/3fp6xWq+RyOXZ2dggGg7y+vnJ1dcVoNPqwYyUwm80Ih8MolUqur6+5u7vDbrfjdDoRBIGbmxvkcjmnp6frASaTCbFYzPn5OePxmFAoxN7eHoVCgVqtxu7uLi6Xi/l8vh7w8vKCIAgARCIRNjY2SKfTCILA0dER+/v7DIdDHh8fsVgsnwcAFAoFZrOZ9/d3Li8vWSwW+P1+tFotrVaL6XS6cv/KX/Qn8v9P02/g7wM/AcG8xXtxQUugAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB0ElEQVRIie3VQYsxARzH8e88OK7ksFyUk5qDaZSUkuxhWzkYHKS8Bm/AC9hXIO/AhYxwktqabHvDCAc3JcVBakokzHPb0/M4TG3twf/6r9+nX//DXzBNk5+cPz+a/gB+BWC/t3x/fzedTieyLFOpVHh6eqJUKlGtVjmdToTDYQzDAKBcLgv/yrjbQBAEAoEA4/GY1WqFoijM53N0XSeZTOL3+1ksFjgcjv9m3AWen5+x2Wz0+31isRg+nw9VVRFFkXA4TKfT4XA4EI1GrQGiKPL5+cnpdCKXyzEYDFgulxQKBYbDIZPJBEVRuFwu1gDDMNA0jbe3N+x2O6qq8vLygs/no1arIUkSkiRRr9etAa1WC5fLRSKRoN1uc7vdyOfzfHx8sN1uyWazTKdTxuOxNWA0GpHJZNjv9/R6PRRFQRCE7yZer5dut4soitYAWZaRZRlVVXG73by+vtJsNjFN8/smu90OWZatAalUitlsxnA4pFgsstls0DSNdDqNaZr0ej0kSeJ6vVoD1us1X19fSJJEJBKh0Wjg8XiIx+O0221sNht+v5/j8WgNMAwDr9dLMBhE0zR0XSedTrPb7RgMBoRCIc7n870IhMfDeQC/H/gLu3C9a2d1lNoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABzklEQVRIie3Vv2rqABTH8W/CBR0qFbs4RgWzOLqIYin+wZDSqdI+hK+ggw/jILhodKnGKeAQME4KkrTq5BAUmqKUgne7071eCBQ6eNbD+X04ZznC6XTiO0v81vQL8COAX+eazWbzJIoib29vbDYb6vU6w+GQl5cXarUa6/Wa4/GIIAg0Gg3hbxn/3eB4PGLbNqqqcjgcGI/HFAoFZFnGMAw8zyMUCv1z/iwgiiKO4xCNRsnlcmiaRiAQQFEUNE3DdV3u7u4IBoP+gI+PDzabDaqqslqtMAyDh4cHPj8/6fV6lMtlwuEwvV7PHzCbzUilUiSTSdrtNpIkkc/n6XQ6XF9fUyqV0HWd3W7nD3h/f0dVVSzLYrFY8PT0hOM4GIbB4+MjnucxGo0olUr+gGKxSCQSodvtkk6nkWWZVqtFPB4nk8nQ7/e5urryDyiKgq7ruK7L8/Mzk8mE5XJJtVrFcRxM00RRFL6+vvwB2+32zwlubm7odDpks1kSiQTtdptYLEYqlWI+n/sDptMpsViM29tbBoMBnudxf3+PaZq8vr5SqVTY7/e4rusPAJAkCcuyME2TQqFAOBxG0zTS6TSSJGHbNud+inB5OBfg5wO/AfdBxSe/N6fWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABgUlEQVRIie3VwaqqUBTG8b9XM0nSSjdmk16lZ2rWQzRt1DMEvUJv0DMIGVvTHWKKoN5Zo3vOheLAGbSmC74f32Qtre97fnL+/Gj6B/gVgPHd8ng89lEUcTgc2O12bLdb2rZlvV6z2WxYrVYIITidTuz3e+1fGd82sCyLPM9xHAcApRSe59F1HXEcEwQBlmVxPp+/zPgvkGUZvu9T1zVlWRIEAWmaYhgGjuMgpcR13deBNE0RQlDXNUVREAQBSZIwGAxwHIc0TZlOp68Bw+GQ2+2GEIKqqng8Hsznc6SUT0BKyWw2ew1o25ayLPF9n6IoaJoGIQRSSizLwrZtkiR5vUGapui6juu6xHHMZDJB07Rnq7quqaqK0Wj0GpAkCaZpMh6PuVwuCCHouo48z1ksFiil6LoO0zTfB+I4fgJZlhGGIUop+r5/DxgOh9i2zfV6xfM82rZFKUUYhtzv9/caACyXS6IoAiAIAsqypGkaPM9DKYWu6xjG1wdB+zycD/D7gb9WbbqVA1ddCQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABb0lEQVRIie3VPa7iMBiF4ddDIoiI5ITEAQrWwIbYAJugZAmwA1pKNkOfH2Q5QRASN5lq0BQzGQnmSrfA7Sedx0eyPouu6/jK8+NL0z/AtwCcvuF2u+1OpxPr9Zqmadjtduz3ew6HA3mes1qtOB6PLBYLNpuN+FNGb4O2bXEcB9/30Vrj+z6u65LnOePxGKUU5/OZIAj+mtELPB4PRqMRnueRpilKKay13G43ZrMZxhiA9wDP8/A8j6IomE6nWGu5Xq/M53O01ggh/k+DLMtIkoS2bamq6gk4jvM6UNc1UkqEEBhjSJKEuq5pmgalFFprBoMBUsrXgPv9ThzHGGMQQhCGIUVR4Ps+w+EQYwxBENC3z/4J/Lqp4zhIKUnTlMlkAoAxhjiOqev6vQa/A3meP4GyLFFKvQ5Ya4miCK01rusipSTLMsIwpOs6yrJ8r8FyueRyuTyfpbWWqqqIooi2bWmahjAMewHx+XA+wPcHfgLkYLj35WqE6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABw0lEQVRIie3VvWryYBjG8X9fEqQOEcEPRKzWz0CCilAXl24elG6OPQtPwqPwA+OD2gaxNaggKYJINEu6Ob06BAodvNcbrh8PFzfPg+d5/Ob8+9X0O/AnAOnWstPpeLIsY5omr6+vWJZFr9fj7e2NbrfLer2m1WrR6/Vot9sP/8u4+QLbtmk0GgghsG2bSqXC8XjEsiw0TWO322HbNtFo9GrGTWA6nSLLMqVSieFwSCQSIRqNMhgM0HUd13VZrVbE43F/gGVZ7Pd7qtUqQghkWSadTiOEIBwOE4/HEUIQi8X8AY+Pj8xmM3K5HJ7n8fHxQbVaZbvd8v39TblcRgiBoij+gGKxyHA4JJlMoigKhmGg6zqO47BcLlFVlf1+z2az8Qdks1k+Pz9xXRdVVS+dZLNZ+v0+mUyGYDDIYDDwB4TDYc7nM4vFgnq9jmmauK6LpmkYhoGiKCQSCQzD8AcEAoFLD8/Pz0iSxPv7O5qmcTgc+Pr6olarMZ/P/QGO45DL5ZhMJoRCIVKpFKPRiEKhcCk9n88jSdfv9eYln04nnp6eGI1GnM9nisUi4/EYx3F4eXlhu90C0Gw2r2Y83D+cO/D3gR8UucOI3JLSzQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABu0lEQVRIie3Vu6rqQBSA4d8obtFmhARFwUKJ90sUfCkbC3t9DJ/GVwghRRIUUSLxiqIEBQXxVOdUZ1sENuzCaRf8H7OmmNDr9eInj/Sj9Q/wK4DIu+FoNHoJIbhcLsznc3q9HuPxGEmSGAwG9Pt9yuUy6XSa4XAY+l/j7Q0mkwlCCEqlEoZhcL/fKRaLuK7L4/FAVVW22y2S9H3mLZBIJHAch2w2ixAC0zSp1+v4vs9qtULTNNbr9bvEe0DTNEzTRAhBJpPBMAxyuRyxWAzbtsnn80QiEXzfDwY0Gg3O5zPr9RpN03Bdl9vtRqPRQNd1ZFlGlmX2+30woFQqEY1GsSyLTqfDbrfjdDrRarVYLpf/3sTzvGDA39XYto0sy6RSKUzTpFAoIEkS0+mUZrPJZrMJBhwOBzRNYzabEQqFUFUVwzBIp9MIIbAsi3w+z9fXVzBgs9lQqVS4Xq+sVitqtRqHw4Hj8Ui73cZxHCRJolqtBgP2+z2KoqAoCrquU6/Xud/vLBYLOp0OnudxOp2oVCrBgOPxSDweJ5fLYVkWyWQSRVEwTZNwOEy320XXdZ7P57eN0OfD+QC/H/gDlbuzFWgA9oUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB4klEQVRIie3VPUtyARjG8X9yFpcERTBwEwfFSfAlzUGQaLKWs0gg+JJIju7iVwgnETVBUUI4iENLBobpJNqq5GjipMhxMLAv8DwOB4IG7/WC68c13Sf7/Z7fPNWvth+BPwEIh8JsNrsHWK1WfH19kU6n6Xa7PD4+kkqlMJlMvL+/s9vtyGQyJ//qOLjg+/sbjUaDz+djNptRKpXwer1cXFxQqVSQZRmr1XpwwUFgtVrx8vKCTqcjHo/T7/d5e3sjFAqh0WgoFApotVqMRqMyQBRFttst+Xweu93O5eUljUaD+XzO/f09y+USSZJwOp3KgN1uRyKRYDqd0mw2EUURs9lMLpdDrVYTCoXodrs8Pz8rAx4eHjg7O0MURdrtNsPhkLu7OwRBoFQq4Xa7ubq6olarKQMWiwVPT0/4/X7Oz88pFApsNhsikQij0QhJkri+vsZmsykDgsEgvV6P19dXYrEYBoOBYrGI2Wzm9vaWdrvNx8cHyWRSGSAIAh6Ph3q9zmQyIRqNMp/PKZfLBAIBXC4X1WqVz89PZYAsy5hMJiwWC+VymdPTU8LhMIPBgE6nw83NDVarlclkogwAWK/X2O12ZFmmWCzicDgIBAK0Wi3G4zF6vR6V6v81J8eHcwT+PvADB928wxIIPe8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB4ElEQVRIie3VMUsyARjA8X9aTQoitIkRBNbgYuDZItImFgVnICFK5oG0+BHE75DLcYODGBKBKEQJTuHiIJziYYODB3KIfoC76d4v8FZwEDT0rA/8fzzTs2XbNj85rh+t/wG/Atj+almtVm0An8/H29sbe3t7lMtlnp6e6PV6JJNJdnd3sW2bSqWy9b/Gtxcsl0uCwSC5XI6Pjw9eXl5Ip9McHx8zGo2+veBLIBwOs16vaTabHB4ecnl5SbvdZjKZcHt7i2maaJqGy/V55ktgZ2eHu7s7DMOg2Wxyfn5ONBqlXq/jcrkoFArM53NWq5Uz4OHhAa/XSzabpd/v0+/3ubm5wePxUKvVCIVCiKLIYDBwBliWRaPRQBAEUqkUrVaLzWZDqVRiuVzy+PjI2dkZsVjMGVAsFhmPx7TbbURRZH9/H1mW8fv95PN53t/fGQ6HSJLkDIhEIlxfX/P6+oqmadzf32OaJoqicHp6SiKRoNFosFgsnAGDwYBEIsHJyQmyLGOaJsVikel0SrfbJZPJcHR0RLfbdQYYhoGu62SzWfx+P4qicHBwwNXVFc/Pz6iqiiAIBAIBZ4Bt28xmMyzLQpIkdF2n0+lwcXFBPB5HVVXW6zVut/vTxtbfw/kDfj/wD6RZvSw68UUOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACCUlEQVRIie3Uv0vqURzG8XfXICTEKIKkQSdx8AepYINoQ4NDin51EP8HF8XNtpZwCUeXAmnxVyVCiIaIi4KgoujkomA05iaU52536grGjXsv9FkPPK/zHPicDSEEXzk/vjT9G/gngM1VhxcXF0KpVHJ4eMj9/T2dTofj42NCoRBKpZJut8tsNgMgHo9vfJSxsoEQguFwyGKxIBqNEolEeH5+5vz8nFKphFarxW63s7u7+9uMjVV7cHV1JXq9HoPBAKvViiRJ7Ozs8PT0xOPjI3K5HI/Hg81mw2KxfNhgJZBKpYTD4aDVanF7e8vr6ysul4uzszPe3t7IZrPU63XUajWFQmF9wGKxCLPZTCAQ4ODggGq1SrFYRCaTIUkSdrud6XRKPp/n5uZmfeD6+lo8PDzw8vLCyckJXq+X9/d3crkcjUYDjUaDJEno9XoMBsP6QKVSEXt7e9RqtV839/l8OJ1OJpMJmUyG0WjE0dERqVTqc090enqK3+9HCEE+n6dWq6FSqQgGg5jNZtrtNuVymXQ6vT4Qj8dFs9lkuVzidrtxOp3MZjMKhQL9fh+TyYTb7WYymRAOh9cHEomE2NraYjwe02632d/fJxAIYDQa6Xa73N3dsb29jU6n+9yiLRYL5vM5Go2GYDCIQqHg8vKSZDKJWq0mFothMBhWRaxu8Cfm//9Nv4G/D/wE/GjirM3yrVwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACC0lEQVRIie3UT0sqYRTH8a8XN9YQKKHgJgNbuBQcFXNpoobhuElE99LGrSsr8A30LiL800wIgg4jCWkRuFQwRVq0CcRw1UKmN3AVhBv3XuhsH/h9nucczmPQdZ3vrF/fmv4D/BOAcd3h5eWlDuD1erFarciyTL1ex2w24/F4EASB5XIJwPn5uWFjwGAwMJlM0DSNcDhMLBbD7/dTrVZRVRWHw8HBwQEmk2llxtoWbW9vk81mCQQCKIpCqVTi7e2NXC5HPp9nuVyiaRrj8Xj1JdctmiRJuiiKRKNR3t/fub6+pt/v43K5OD09ZW9vj3a7TaVSodPp/LZFa4Fisag3m012dnZIJpP4fD6GwyE3Nze8vr4SDAY5OTlha2sLv9+/OfD09KTPZjMUReHx8RGn00kqlcLhcHB/f8/d3R2fn5+EQqGVQ14LRCIRPZFIIIoiLy8vlMtlptMpgUAASZIwGo00Gg1arRa9Xm9z4OzsTO92uzidTtLpNPv7+zw8PFCtVvn4+CAej3N8fMxgMCCTyWwO9Pt9fTQacXt7y2g0QhRFEokEgiDQbDZpNBrs7u5yeHhIoVDYHLi6utJdLhcWi4Xn52dkWWaxWBAOhzk6OmI+n6OqKoIgcHFxsfmizedzer0eNpsNt9uNx+NBlmVqtRqdTgdJkrDb7SwWi5UZa1/wJ+r//01/gL8PfAHN0d0Z9dAjJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABtElEQVRIie2UvarqQBSFVyZXJMag0Uz+EGsLJZUPoS/kY9jaWfgAloKFrZ34ArYJxgR1VExicE51AgeOgcCRey+cDYspBtbHrL33CJxzvLPIW91/Af8E4E/e5Wg04oIgwDAMmKYJ0zRhGAZEUUQURbjf79k5GAyEwoAoirDb7bDZbJAkCeI4xvP5hKIooJSCUgpN06Bp2ksPIW8Plsslb7VauF6vYIyBMYbT6YQgCOD7fqbz+Yz1ev3tC3IBw+GQA4Bt27BtG5ZlwbIsKIoCSZIyAUCn0ykOGI/H3Pd9BEGQ6Xg8olQqoVqtolKpQJZl1Go1TKfT4j3o9XqQZRmEEBBCIAgCOOc4HA7Y7/eZwjB86ZH7gm63y8vlMlRVRaPRgKqqaDab0HUdmqZB13VQSkEIgeM4xSNarVY8CAJ4ngfXdeG6LnzfRxzHiOMYaZri8XjAsiwsFoviEW23W6iqina7DcdxUK/XIUkSbrdbNlWMMXie99IjFzCfz8E5hyiKWQ9kWQalNFu8z5heVW5Ek8mEp2maLVmSJLhcLgjD8MtU9ft9zGaz4j34ifr/f9NfwN8HfAB+a9vZqNp6RgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABmElEQVRIie2VsarCMBSG/16uFlsV0sS2g+Aqgjh1cBcEn8FH8V0cnXwQpz6AoyJtsLGVahFCc7eAYAsOwh088HMggf9LzjkhhlIKn4yfj7p/Af8C8Fu3uVwu1W63g23baLfbT7IsS6vZbGK1WhmvPIy6Md1sNqrX64Fz/qQsy3C73VAUBfI8h5QSYRi+BNTeIAxDdLtdMMbQ7/cxmUxACIFSCkVR4H6/a0hV1AIOhwOOxyOklCjLUmdKKXzfh+/78DwPjLFKj9oShWGoGo0Grtcr0jRFlmVI0xScc8RxrHOe55UlqgXMZjNFCIHnefq0ruui1WrBNE0tKSWCIHi/B9PpFEmSIEkS7Pd7CCGQ5zksy0Kn09FyXRdBELz0qAXM53OYpvm09ng8wDnH6XRCFEWIogic80qP2hKNx2Nl2zYIISCEwHEcUEpBKQVjDIwxOI4DzjkWi8X7PdhutypJEsRxrJt6uVwgpdQTVZYlhsMh1uv1+z0wDAODwQCj0Ui/YABI0xRCCAghcD6fIYSo9vh+OF/A/wf8AQu73Hd848nNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB7ElEQVRIie2Uz0oqARSHPy9TLgamFiqDgzGME/kHRBFpk7SsQNq3atODtPBlpAeQdq6VCpVhRgcdnAmhmJAWZaOL6QWugnHj3gud7YHv4/wO50TCMOQ769e30n8E/4RAWNes1+thPB5HlmXi8TiSJDGdTrm/v6fX62HbNoIgcHJywvX1dWRjwdnZGb7v47ourVYLwzB4fX1FFEUUReHi4gJd1xmPx1+eAN/3WSwWyLLM4eEh2WyWvb09JEliMpkwHA55fn7+miCVSnF6eko+n2d3d5eXlxccx6HRaNDv93l7ewOgUqmsZETWXXK73Q49z+Pu7g7DMHh6eiIIAtLpNPl8nlwux/7+Pu/v7xwdHf12B2sF1Wo1nM/nJBIJVFWlVCqRzWbZ2toiCAIGgwEPDw84jkOz2dx8ybVajUKhQCwWQxAEbNvm9vYW0zQZjUZsb2+jKArFYnElY60gmUximibdbhfDMACQJIlcLsf5+TmqqiJJEh8fHysZayO6uroKTdPk4OCAYrGIpmkkk0mCIGA0GmFZFoZh8Pj4SKfT2TwiXde5vLxEEARmsxmWZdFoNHBdl8VigSiKaJpGuVxeyVgrWC6X3NzcYFkWnuchiiKZTIbj42N2dnaIRqPM5/OvR/Qn6v//pj+Cvy/4BG/6y0G7uAqCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB+klEQVRIie2U3UoqYRSGH0ehP5h+FJVBJQpjcsSfAa8hD+zEY+lKPPIGupI86BYEz4TPsUQFRZSmqUYwSS0KZh/s0xwwduy9oXW64HnWehff53Ech+8s6VvpP4J/QuBza1YqFUeSfs8wm814fHzENE3u7+/Z3Nzk9PSURCLBx8cH5XLZs7bAsixM08SyLHw+H8FgEF3XSafTHB4e4vF4sG0bIcTXNuh0OpycnJDP5zk6OsLv97NcLun1elSrVbrdLnd3d+RyuZUMj9tDu7m5cV5fXxmNRrTbbYQQDIdDvF4vBwcHZLNZdF1HkiSKxeKnEbkKLi4unNFoxHw+Z3t7m1QqRSqV4vj4mGAwyGQyYTAYYBgGl5eX699AkiTOzs7QNI1oNMp8Pse2bWq1GkIIbNvm/f2dUCi0kuG6Qb1ed97e3jAMg2azyXA45OnpiXA4TDweJ5lMomkae3t7pNPp9SMqlUqOEIL9/X0ikQiZTIZsNovf7wdgPB7TaDS4vb2lWq2uH1E8Huf8/BxFUZBlGdM0MQyDdrtNv99nsVgQCoWIxWIrGa4CWZZ5eXnh+vqaVqvF8/MzGxsbKIpCoVBAVVUCgQBbW1tfE0ynU66urtjZ2UHXdTRNQ1VVdnd3eXh4wLIshBBMJhMymcynDNcb/In6/3/TH8HfF/wC2hjPHQkptW0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB/ElEQVRIie3Uv0tqcRjH8bd6zxjiICg46SAqDjoIR4kmwSFykBwKBMUfgaNL4CAtFQciQnDRrBaHchDkQEtDNKiDoI4KnclFQURoOEvev8ADyY17L/SsX76fFw/Pw6Nbr9d8Z+m/Nf0H+CeAX1qPZ2dnawCbzUYgEOD5+ZlGo4HX6+Xk5IT393cmkwl6vZ5SqaT7MiAIAh6PB5PJRL1e5+3tjf39fQ4ODhgOhzQaDdxuN2azebsOQqEQHx8fXF9fs1gsSKfTBAIBWq0WsiwjiiLRaJTBYLAxQ3MG4/GYi4sLPj8/KRaLuFwubm5uaLfbHB8fk8lkGI1GyLK8XQeVSoXd3V2Ojo6YTqdcXl4iCAKnp6c4HA6q1Sq9Xo9wOLwdkEwmEUWR19dXnp6ecDqdZLNZlssl5+fnzGYzcrkcfr9/Y4ZO6xb1+/313d0d3W6XSCRCLBaj3+9ze3uL1WollUqxs7NDvV7n4eHh61t0dXWFoijk83l8Ph/NZhNZlgmFQqRSKcbjMeVyGVVVN2ZoAhaLhXg8jslkQpIkFEUhkUiwt7fHy8sLj4+PuN1ufD7fdoDRaERRFGq1GqqqUigUsFgs3N/f0+l0CAaD2O12VqvVdgDAYrHA4XBweHiIqqpIksR8PicWi2EwGFgul5r/NYf8J+r/v6Y/wN8HfgNkyL3AEyHMaQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB+ElEQVRIie3UP0iqYRTH8a9/1kJoU4oQXBRaROSdrBaJsEHEFwfBQQcRhHdyEJGGEFqkoSUEGwJBMERpSHQS3QRFI3QQXhAsX4U2xSDvcOeE5Ma9Fzrrw/l9eDiHo1qtVnxnqb81/Qf4JwDtusfz8/MVgFqt5vX1lUajgSAIBINBZFmmVCqxu7uLRqMhmUyqvgyoVL97er0ew+EQn8/H4eEhjUaDu7s7zGYzer2eyWSy2Q8WiwXtdpvlcokkSZhMJnK5HPV6naOjI0RRpN1uM5vNNgMeHx+xWCwEAgEAUqkU4/GYUCiE3W6nXC5TKBRwOBybAU6nE4/HQ6fT4fb2lu3tbeLxODqdjnQ6zfPzM6IocnZ29mmGat2paLVaq4eHB0qlEoIgIIoiiqJwc3PD+/s7wWCQvb09isUil5eXXx/y1dUVg8EAj8fD6ekp1WqVfD7P/v4+4XCY+XxOOp1mNBp9mrEWmE6nSJKE0Wgkm81Sq9VwuVy43W663S6ZTIadnR0SicRmgNPpRKvVkkqlUBSFSCSCzWbj/v6eSqWC1WrF7/czn883A97e3nh6egIgFouxtbXF9fU1vV4Pr9eLw+FAlmX6/T7Hx8dfBwAMBgMnJyd8fHxwcXHBcrkkGo1ycHBAs9nk5eWFdYuydov+RP3/1/QH+PvAL1heyL2mHWuZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB3UlEQVRIie3VPUtyARjG8b9aa5jTIcmlraijokcXMUQaGk5yDrn4MVwd9RP4EWwROr7lpEiJg4hWHERwFQcdFFSQknx5vsDz+MCBoKF7vbivH/d0m3a7Hd855m9t/wV+BHCwL0wmkzubzYbP5yOVSnF0dEQ8HiedTvP5+YnX62U+nwOQSCRMf+v47wUXFxc0m00GgwGqqtLtdnl/f+fm5gaHw0G/3+fw8PCf+3sBu92OxWKhXC4TDAY5PT0ll8txfn6Ox+Ph6emJ5XJJIBAwBoiiSK1W4+Pjg2g0SqPRYDAYcH9/T6fTQdd1FEXh6+vLGDCbzahWq9ze3mKxWMjn8wSDQRwOB9lsFlEUEUWRTCZjDHh4eOD4+JhQKESpVGKz2aCqKs/Pz4zHYxRFQdd1Wq2WMaDdbqOqKrPZjEqlgizLmEwmisUioVAIQRB4fHzE6XQaA9xuNy6XC03TsFqthMNhCoUC2+0WRVF4eXlhMpkgSZIxIBKJ0Ov1eH19JRaLMRqNqNfryLIMQLlcxuPxsF6vjQHD4ZBGo8Hl5SWSJKFpGoIgcH19jaZpmM1mzs7OWC6XxoDFYsHJyQlXV1fU63Xe3t64u7tjOp1SrVbx+/2sVqt9FZh+H84v8POBP+8nujgSW0PXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAB0klEQVRIie3Vy6pxARjG8T++gYmQkYmyJYbKhFISK0oOrRzuwi3I0JUwIpHkMJMByRoJOSQTCckhA8q+ge/z1apde+Cdvr3Pr3f0KF6vFz85yh9N/wC/AvjzbpnL5V5KpZLFYsFqtSKfz9NoNGg0GmQyGTabDff7HYVCQTabVfwt478f3O93ptMpoihyu93odDoIgoDNZqPb7XK9XtFoNP+8fwuoVCpmsxlGoxGv10u1WkWtVhMMBqnVauz3e/x+P2q1Wh5wuVxYr9eIosh6vabb7RKNRnk8HtRqNUKhEDqdjlKpJA8YDoc4HA7sdjvFYhGTyYTH46FcLqPVahEEgWazyeFwkAecTidEUWQ0GjGZTEilUqxWK3q9HolEgsvlQqvVIhKJyAPC4TB6vZ5yuYzT6cRms1EsFjGbzbhcLiqVChqNhmg0Kg+IxWK0222OxyPpdJp+v89isSCZTLJcLhkMBsTjcZ7Ppzxgu93SarUIBAIYDAZKpRJutxuLxUKhUMBqteJ0OpEkSR4wGAz4+vrC5/NRr9c5n89EIhH6/T7z+Zx4PM7xeGS328kDAMxmM5IkMRwOEQQBnU5HpVLB7XZjtVoZj8e86xTFp3A+wO8HvgHYfcVgBRdnUQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABdUlEQVRIie3VParqQBiA4TeXmB+ICSGT+NO6CpfgHsTC7di6AVfhJqys7AUTM5gEcRQJyZwucItzi1wOnMLpZx7emYHP0Frzk+vPj57+AX4/sN/v9Xa71fP5XJ9OJ71er/VqtdLH41EvFgu92Wz0brfTy+Xy26/4T8B1XYqiIAgCAKqqQghB0zSkacpkMsG2bQ6HQ78Cx3GQUpIkCa/Xi+fzSZIkSCkZDAb4vs/1eiUMw36A67p/AUopxuMxeZ5jmmYHCCH6F+R53gGPx4PRaISUEtu2GQ6HZFnWH6jrGqUUcRxzv9+p6xohBFJKHMfB8zyklERR1A+QUmKaJkEQkGUZYRhiGAa32404jrtr8zyvH5BlWfeYl8sFIQRaa8qyZDqdUlUVWmts2+4PWJaF7/ukaUoURbRtS1EUHdC2LZZl/R/geV5X0LZtV1CWJU3T9C8AmM1mnM9nAJIkQSnF+/0miiLKssQwDEzT/Ha/8Rk4H+D3A1+T28Ce+aI6rwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABbElEQVRIie3VzY3CMBAF4JfICnKwSEJkW6SL3KiFGx3QAhJ9cKQEiqABOsAYO0SQHwUO3tOi3cOiVVZIe2AKmM9vZiR7zjm8svyXdn8D/x9YrVYuz3O32Wzcer12eZ673W7nFouFm81mbrvduvl87pbL5Y+n+BS43W4IggCMMRhjwBgDIQRaa4RhCM459vs90jTtl6BpGoRhCEopDocDpJToug7X6xWTyQRFUcD3/b8BlFKEYQitNYQQD0BKCWPM34HhcAhKKZRS3xJkWQZrLQghGI/H/YCqqhBFETzPg7UWnHO0bYumaSCEgDEGhBDEcdwfEELgfD7D930kSQKtNRhjGAwGsNYiSZJnLX4HfI4iiiIopR4zL4oCUkpUVdUf+LrM0WiE4/H4eHVZlhBC9AfqugbnHMYYBEGAOI6hlHoAnwnquu4HTKdTnE4nXC4XZFmG+/2OsiyRpim6rkPbtkjT9CngvT+cN/D/gQ+Bl7gxqB5mOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 13.68x13.68 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gabor_test(sigma, theta, Lambda, psi, gamma, kernel_size, cos):\n",
    "        sigma_x = sigma\n",
    "        # sigma_y = float(sigma) / gamma\n",
    "        sigma_y = sigma / gamma\n",
    "\n",
    "        # Bounding box\n",
    "        half_size = (kernel_size - 1) // 2\n",
    "        ymin, xmin = -half_size, -half_size\n",
    "        ymax, xmax = half_size, half_size\n",
    "    #     (y, x) = np.meshgrid(np.arange(ymin, ymax + 1), np.arange(xmin, xmax + 1))\n",
    "        y, x = torch.meshgrid([torch.arange(ymin, ymax+1), torch.arange(xmin,xmax+1)])\n",
    "\n",
    "        if cos:\n",
    "            gb = torch.exp(-.5 * (x**2 / sigma_x**2 + y**2 / sigma_y**2)) * torch.cos(2 * np.pi / Lambda * x + psi)\n",
    "        else:\n",
    "            gb = torch.exp(-.5 * (x**2 / sigma_x**2 + y**2 / sigma_y**2)) * torch.sin(2 * np.pi / Lambda * x + psi)\n",
    "\n",
    "        # Rotation\n",
    "        degrees = theta * 180 / np.pi\n",
    "        gb = FF.apply_rotation(gb, {'degrees': torch.tensor(degrees)}, {'interpolation': torch.tensor([1]), 'align_corners': torch.tensor(True)})\n",
    "        gb = gb.squeeze()\n",
    "        return gb\n",
    "\n",
    "\n",
    "pretrain_gabor_model = torch.load('pretrain_gabor.pt')\n",
    "filter_cos = pretrain_gabor_model['filter_cos']\n",
    "filter_sin = pretrain_gabor_model['filter_sin']\n",
    "bias1 = pretrain_gabor_model['bias1']\n",
    "bias2 = pretrain_gabor_model['bias2']\n",
    "weights = pretrain_gabor_model['weights']\n",
    "w = pretrain_gabor_model['w']\n",
    "b = pretrain_gabor_model['b']\n",
    "sigma = pretrain_gabor_model['sigma']\n",
    "theta = pretrain_gabor_model['theta'][0]\n",
    "Lambda = pretrain_gabor_model['Lambda']\n",
    "psi = pretrain_gabor_model['psi']\n",
    "gamma = pretrain_gabor_model['gamma']\n",
    "for scale in range(6):\n",
    "    sigma1 = sigma *(2.1**scale)\n",
    "    for ori in range(8):\n",
    "        theta1 = theta+ori*np.pi/8\n",
    "        gb = gabor_test(sigma1, theta1, Lambda, psi, gamma, 19, True)\n",
    "        figure, b = plt.subplots()\n",
    "        figure.set_size_inches(0.19, 0.19)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(gb, cmap='gray')\n",
    "        plt.savefig(\"./filter/%s_scale_%.2fdeg_%.2f.png\" % ('cos',sigma1,theta1), dpi=100,pad_inches=0.0,bbox_inches='tight')\n",
    "        gb = gabor_test(sigma1, theta1, Lambda, psi, gamma, 19, False)\n",
    "        figure, b = plt.subplots()\n",
    "        figure.set_size_inches(0.19, 0.19)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(gb, cmap='gray')\n",
    "        plt.savefig(\"./filter/%s_scale_%.2fdeg_%.2f.png\" % ('sin',sigma1,theta1), dpi=100,pad_inches=0.0,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc(x): #x as a picture\n",
    "    x_cos = F.conv2d(x, filter_cos, bias=bias1)\n",
    "    x_sin = F.conv2d(x, filter_sin, bias=bias2)\n",
    "    x_comb = torch.cat((x_cos, x_sin), 2)\n",
    "\n",
    "    x_cos = x_cos.view(len(x), 1, 1, 24)\n",
    "    x_sin = x_sin.view(len(x), 1, 1, 24)\n",
    "    weighted_cos = (torch.matmul(x_cos, weights)).view(len(x), 1)\n",
    "    weighted_sin = (torch.matmul(x_sin, weights)).view(len(x), 1)\n",
    "\n",
    "    numerator = torch.norm(torch.cat([weighted_cos, weighted_sin], 1), dim=1)\n",
    "#         print(\"numerator\", numerator.size())\n",
    "    x_comb_norm = torch.norm(x_comb, dim=2)\n",
    "    x_comb_norm = x_comb_norm.view(len(x), 1, 24)\n",
    "#         print(\"x_comb_norm\", x_comb_norm.size())\n",
    "    denominator = torch.matmul(x_comb_norm, torch.abs(weights))\n",
    "    denominator = denominator.view(len(x))\n",
    "#         print(\"size:\", numerator.size(), denominator.size())\n",
    "    pc = numerator / denominator                \n",
    "#     return torch.sigmoid(w * pc + b)\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/71432 (0%)]\tLoss: 0.628911\n",
      "Train Epoch: 1 [6400/71432 (9%)]\tLoss: 0.295053\n",
      "Train Epoch: 1 [12800/71432 (18%)]\tLoss: 0.262737\n",
      "Train Epoch: 1 [19200/71432 (27%)]\tLoss: 0.184690\n",
      "Train Epoch: 1 [25600/71432 (36%)]\tLoss: 0.136053\n",
      "Train Epoch: 1 [32000/71432 (45%)]\tLoss: 0.114957\n",
      "Train Epoch: 1 [38400/71432 (54%)]\tLoss: 0.085034\n",
      "Train Epoch: 1 [44800/71432 (63%)]\tLoss: 0.084477\n",
      "Train Epoch: 1 [51200/71432 (72%)]\tLoss: 0.152466\n",
      "Train Epoch: 1 [57600/71432 (81%)]\tLoss: 0.074285\n",
      "Train Epoch: 1 [64000/71432 (90%)]\tLoss: 0.152649\n",
      "Train Epoch: 1 [70400/71432 (98%)]\tLoss: 0.175278\n",
      "\n",
      "Test set: Average loss: 0.1372, Accuracy: 7698/8141 (95%), Positive accuracy: 3737/4091 (91%), Negative accuracy: 3961/4050 (98%), f1 score: 0.9464\n",
      "\n",
      "Train Epoch: 2 [0/71432 (0%)]\tLoss: 0.093958\n",
      "Train Epoch: 2 [6400/71432 (9%)]\tLoss: 0.198442\n",
      "Train Epoch: 2 [12800/71432 (18%)]\tLoss: 0.119478\n",
      "Train Epoch: 2 [19200/71432 (27%)]\tLoss: 0.054716\n",
      "Train Epoch: 2 [25600/71432 (36%)]\tLoss: 0.035890\n",
      "Train Epoch: 2 [32000/71432 (45%)]\tLoss: 0.093249\n",
      "Train Epoch: 2 [38400/71432 (54%)]\tLoss: 0.084860\n",
      "Train Epoch: 2 [44800/71432 (63%)]\tLoss: 0.115456\n",
      "Train Epoch: 2 [51200/71432 (72%)]\tLoss: 0.027868\n",
      "Train Epoch: 2 [57600/71432 (81%)]\tLoss: 0.156310\n",
      "Train Epoch: 2 [64000/71432 (90%)]\tLoss: 0.171898\n",
      "Train Epoch: 2 [70400/71432 (98%)]\tLoss: 0.129014\n",
      "\n",
      "Test set: Average loss: 0.1187, Accuracy: 7776/8141 (96%), Positive accuracy: 3939/4091 (96%), Negative accuracy: 3837/4050 (95%), f1 score: 0.9553\n",
      "\n",
      "Train Epoch: 3 [0/71432 (0%)]\tLoss: 0.123147\n",
      "Train Epoch: 3 [6400/71432 (9%)]\tLoss: 0.064148\n",
      "Train Epoch: 3 [12800/71432 (18%)]\tLoss: 0.057013\n",
      "Train Epoch: 3 [19200/71432 (27%)]\tLoss: 0.051640\n",
      "Train Epoch: 3 [25600/71432 (36%)]\tLoss: 0.142964\n",
      "Train Epoch: 3 [32000/71432 (45%)]\tLoss: 0.052249\n",
      "Train Epoch: 3 [38400/71432 (54%)]\tLoss: 0.142406\n",
      "Train Epoch: 3 [44800/71432 (63%)]\tLoss: 0.224097\n",
      "Train Epoch: 3 [51200/71432 (72%)]\tLoss: 0.108885\n",
      "Train Epoch: 3 [57600/71432 (81%)]\tLoss: 0.118777\n",
      "Train Epoch: 3 [64000/71432 (90%)]\tLoss: 0.142764\n",
      "Train Epoch: 3 [70400/71432 (98%)]\tLoss: 0.078584\n",
      "\n",
      "Test set: Average loss: 0.1099, Accuracy: 7779/8141 (96%), Positive accuracy: 3859/4091 (94%), Negative accuracy: 3920/4050 (97%), f1 score: 0.9556\n",
      "\n",
      "Train Epoch: 4 [0/71432 (0%)]\tLoss: 0.072330\n",
      "Train Epoch: 4 [6400/71432 (9%)]\tLoss: 0.105764\n",
      "Train Epoch: 4 [12800/71432 (18%)]\tLoss: 0.096189\n",
      "Train Epoch: 4 [19200/71432 (27%)]\tLoss: 0.050050\n",
      "Train Epoch: 4 [25600/71432 (36%)]\tLoss: 0.083667\n",
      "Train Epoch: 4 [32000/71432 (45%)]\tLoss: 0.070155\n",
      "Train Epoch: 4 [38400/71432 (54%)]\tLoss: 0.088110\n",
      "Train Epoch: 4 [44800/71432 (63%)]\tLoss: 0.072408\n",
      "Train Epoch: 4 [51200/71432 (72%)]\tLoss: 0.095049\n",
      "Train Epoch: 4 [57600/71432 (81%)]\tLoss: 0.139827\n",
      "Train Epoch: 4 [64000/71432 (90%)]\tLoss: 0.208677\n",
      "Train Epoch: 4 [70400/71432 (98%)]\tLoss: 0.063020\n",
      "\n",
      "Test set: Average loss: 0.1061, Accuracy: 7800/8141 (96%), Positive accuracy: 3949/4091 (97%), Negative accuracy: 3851/4050 (95%), f1 score: 0.9582\n",
      "\n",
      "Train Epoch: 5 [0/71432 (0%)]\tLoss: 0.171333\n",
      "Train Epoch: 5 [6400/71432 (9%)]\tLoss: 0.141677\n",
      "Train Epoch: 5 [12800/71432 (18%)]\tLoss: 0.096760\n",
      "Train Epoch: 5 [19200/71432 (27%)]\tLoss: 0.040107\n",
      "Train Epoch: 5 [25600/71432 (36%)]\tLoss: 0.160389\n",
      "Train Epoch: 5 [32000/71432 (45%)]\tLoss: 0.081809\n",
      "Train Epoch: 5 [38400/71432 (54%)]\tLoss: 0.217380\n",
      "Train Epoch: 5 [44800/71432 (63%)]\tLoss: 0.066390\n",
      "Train Epoch: 5 [51200/71432 (72%)]\tLoss: 0.257002\n",
      "Train Epoch: 5 [57600/71432 (81%)]\tLoss: 0.068168\n",
      "Train Epoch: 5 [64000/71432 (90%)]\tLoss: 0.178072\n",
      "Train Epoch: 5 [70400/71432 (98%)]\tLoss: 0.091437\n",
      "\n",
      "Test set: Average loss: 0.1243, Accuracy: 7736/8141 (95%), Positive accuracy: 3764/4091 (92%), Negative accuracy: 3972/4050 (98%), f1 score: 0.9510\n",
      "\n",
      "Train Epoch: 6 [0/71432 (0%)]\tLoss: 0.093696\n",
      "Train Epoch: 6 [6400/71432 (9%)]\tLoss: 0.089412\n",
      "Train Epoch: 6 [12800/71432 (18%)]\tLoss: 0.053104\n",
      "Train Epoch: 6 [19200/71432 (27%)]\tLoss: 0.128564\n",
      "Train Epoch: 6 [25600/71432 (36%)]\tLoss: 0.061691\n",
      "Train Epoch: 6 [32000/71432 (45%)]\tLoss: 0.049206\n",
      "Train Epoch: 6 [38400/71432 (54%)]\tLoss: 0.093124\n",
      "Train Epoch: 6 [44800/71432 (63%)]\tLoss: 0.045356\n",
      "Train Epoch: 6 [51200/71432 (72%)]\tLoss: 0.117486\n",
      "Train Epoch: 6 [57600/71432 (81%)]\tLoss: 0.031344\n",
      "Train Epoch: 6 [64000/71432 (90%)]\tLoss: 0.096738\n",
      "Train Epoch: 6 [70400/71432 (98%)]\tLoss: 0.054412\n",
      "\n",
      "Test set: Average loss: 0.1038, Accuracy: 7798/8141 (96%), Positive accuracy: 3859/4091 (94%), Negative accuracy: 3939/4050 (97%), f1 score: 0.9580\n",
      "\n",
      "Train Epoch: 7 [0/71432 (0%)]\tLoss: 0.050202\n",
      "Train Epoch: 7 [6400/71432 (9%)]\tLoss: 0.164157\n",
      "Train Epoch: 7 [12800/71432 (18%)]\tLoss: 0.064871\n",
      "Train Epoch: 7 [19200/71432 (27%)]\tLoss: 0.126301\n",
      "Train Epoch: 7 [25600/71432 (36%)]\tLoss: 0.122827\n",
      "Train Epoch: 7 [32000/71432 (45%)]\tLoss: 0.087962\n",
      "Train Epoch: 7 [38400/71432 (54%)]\tLoss: 0.092184\n",
      "Train Epoch: 7 [44800/71432 (63%)]\tLoss: 0.067493\n",
      "Train Epoch: 7 [51200/71432 (72%)]\tLoss: 0.045258\n",
      "Train Epoch: 7 [57600/71432 (81%)]\tLoss: 0.087943\n",
      "Train Epoch: 7 [64000/71432 (90%)]\tLoss: 0.140034\n",
      "Train Epoch: 7 [70400/71432 (98%)]\tLoss: 0.050738\n",
      "\n",
      "Test set: Average loss: 0.1047, Accuracy: 7799/8141 (96%), Positive accuracy: 3864/4091 (94%), Negative accuracy: 3935/4050 (97%), f1 score: 0.9581\n",
      "\n",
      "Train Epoch: 8 [0/71432 (0%)]\tLoss: 0.183605\n",
      "Train Epoch: 8 [6400/71432 (9%)]\tLoss: 0.054177\n",
      "Train Epoch: 8 [12800/71432 (18%)]\tLoss: 0.045105\n",
      "Train Epoch: 8 [19200/71432 (27%)]\tLoss: 0.042597\n",
      "Train Epoch: 8 [25600/71432 (36%)]\tLoss: 0.053316\n",
      "Train Epoch: 8 [32000/71432 (45%)]\tLoss: 0.167242\n",
      "Train Epoch: 8 [38400/71432 (54%)]\tLoss: 0.154776\n",
      "Train Epoch: 8 [44800/71432 (63%)]\tLoss: 0.103956\n",
      "Train Epoch: 8 [51200/71432 (72%)]\tLoss: 0.119752\n",
      "Train Epoch: 8 [57600/71432 (81%)]\tLoss: 0.041732\n",
      "Train Epoch: 8 [64000/71432 (90%)]\tLoss: 0.074255\n",
      "Train Epoch: 8 [70400/71432 (98%)]\tLoss: 0.052287\n",
      "\n",
      "Test set: Average loss: 0.1018, Accuracy: 7808/8141 (96%), Positive accuracy: 3873/4091 (95%), Negative accuracy: 3935/4050 (97%), f1 score: 0.9592\n",
      "\n",
      "Train Epoch: 9 [0/71432 (0%)]\tLoss: 0.080405\n",
      "Train Epoch: 9 [6400/71432 (9%)]\tLoss: 0.222797\n",
      "Train Epoch: 9 [12800/71432 (18%)]\tLoss: 0.141201\n",
      "Train Epoch: 9 [19200/71432 (27%)]\tLoss: 0.082919\n",
      "Train Epoch: 9 [25600/71432 (36%)]\tLoss: 0.081865\n",
      "Train Epoch: 9 [32000/71432 (45%)]\tLoss: 0.049721\n",
      "Train Epoch: 9 [38400/71432 (54%)]\tLoss: 0.061186\n",
      "Train Epoch: 9 [44800/71432 (63%)]\tLoss: 0.032972\n",
      "Train Epoch: 9 [51200/71432 (72%)]\tLoss: 0.188676\n",
      "Train Epoch: 9 [57600/71432 (81%)]\tLoss: 0.127291\n",
      "Train Epoch: 9 [64000/71432 (90%)]\tLoss: 0.099966\n",
      "Train Epoch: 9 [70400/71432 (98%)]\tLoss: 0.034310\n",
      "\n",
      "Test set: Average loss: 0.1023, Accuracy: 7811/8141 (96%), Positive accuracy: 3874/4091 (95%), Negative accuracy: 3937/4050 (97%), f1 score: 0.9596\n",
      "\n",
      "Train Epoch: 10 [0/71432 (0%)]\tLoss: 0.072068\n",
      "Train Epoch: 10 [6400/71432 (9%)]\tLoss: 0.096265\n",
      "Train Epoch: 10 [12800/71432 (18%)]\tLoss: 0.096871\n",
      "Train Epoch: 10 [19200/71432 (27%)]\tLoss: 0.074134\n",
      "Train Epoch: 10 [25600/71432 (36%)]\tLoss: 0.187330\n",
      "Train Epoch: 10 [32000/71432 (45%)]\tLoss: 0.177263\n",
      "Train Epoch: 10 [38400/71432 (54%)]\tLoss: 0.079541\n",
      "Train Epoch: 10 [44800/71432 (63%)]\tLoss: 0.098711\n",
      "Train Epoch: 10 [51200/71432 (72%)]\tLoss: 0.055075\n",
      "Train Epoch: 10 [57600/71432 (81%)]\tLoss: 0.120037\n",
      "Train Epoch: 10 [64000/71432 (90%)]\tLoss: 0.077059\n",
      "Train Epoch: 10 [70400/71432 (98%)]\tLoss: 0.095046\n",
      "\n",
      "Test set: Average loss: 0.1002, Accuracy: 7813/8141 (96%), Positive accuracy: 3877/4091 (95%), Negative accuracy: 3936/4050 (97%), f1 score: 0.9598\n",
      "\n",
      "Train Epoch: 11 [0/71432 (0%)]\tLoss: 0.080987\n",
      "Train Epoch: 11 [6400/71432 (9%)]\tLoss: 0.082783\n",
      "Train Epoch: 11 [12800/71432 (18%)]\tLoss: 0.093649\n",
      "Train Epoch: 11 [19200/71432 (27%)]\tLoss: 0.080262\n",
      "Train Epoch: 11 [25600/71432 (36%)]\tLoss: 0.023133\n",
      "Train Epoch: 11 [32000/71432 (45%)]\tLoss: 0.088302\n",
      "Train Epoch: 11 [38400/71432 (54%)]\tLoss: 0.074070\n",
      "Train Epoch: 11 [44800/71432 (63%)]\tLoss: 0.163287\n",
      "Train Epoch: 11 [51200/71432 (72%)]\tLoss: 0.120242\n",
      "Train Epoch: 11 [57600/71432 (81%)]\tLoss: 0.184463\n",
      "Train Epoch: 11 [64000/71432 (90%)]\tLoss: 0.147104\n",
      "Train Epoch: 11 [70400/71432 (98%)]\tLoss: 0.116054\n",
      "\n",
      "Test set: Average loss: 0.0991, Accuracy: 7815/8141 (96%), Positive accuracy: 3883/4091 (95%), Negative accuracy: 3932/4050 (97%), f1 score: 0.9600\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [0/71432 (0%)]\tLoss: 0.083781\n",
      "Train Epoch: 12 [6400/71432 (9%)]\tLoss: 0.094759\n",
      "Train Epoch: 12 [12800/71432 (18%)]\tLoss: 0.058378\n",
      "Train Epoch: 12 [19200/71432 (27%)]\tLoss: 0.080779\n",
      "Train Epoch: 12 [25600/71432 (36%)]\tLoss: 0.067572\n",
      "Train Epoch: 12 [32000/71432 (45%)]\tLoss: 0.132605\n",
      "Train Epoch: 12 [38400/71432 (54%)]\tLoss: 0.112927\n",
      "Train Epoch: 12 [44800/71432 (63%)]\tLoss: 0.069503\n",
      "Train Epoch: 12 [51200/71432 (72%)]\tLoss: 0.078576\n",
      "Train Epoch: 12 [57600/71432 (81%)]\tLoss: 0.150780\n",
      "Train Epoch: 12 [64000/71432 (90%)]\tLoss: 0.130739\n",
      "Train Epoch: 12 [70400/71432 (98%)]\tLoss: 0.043161\n",
      "\n",
      "Test set: Average loss: 0.0995, Accuracy: 7814/8141 (96%), Positive accuracy: 3884/4091 (95%), Negative accuracy: 3930/4050 (97%), f1 score: 0.9599\n",
      "\n",
      "Train Epoch: 13 [0/71432 (0%)]\tLoss: 0.122115\n",
      "Train Epoch: 13 [6400/71432 (9%)]\tLoss: 0.129866\n",
      "Train Epoch: 13 [12800/71432 (18%)]\tLoss: 0.059112\n",
      "Train Epoch: 13 [19200/71432 (27%)]\tLoss: 0.114963\n",
      "Train Epoch: 13 [25600/71432 (36%)]\tLoss: 0.106861\n",
      "Train Epoch: 13 [32000/71432 (45%)]\tLoss: 0.131254\n",
      "Train Epoch: 13 [38400/71432 (54%)]\tLoss: 0.069102\n",
      "Train Epoch: 13 [44800/71432 (63%)]\tLoss: 0.086838\n",
      "Train Epoch: 13 [51200/71432 (72%)]\tLoss: 0.166093\n",
      "Train Epoch: 13 [57600/71432 (81%)]\tLoss: 0.053013\n",
      "Train Epoch: 13 [64000/71432 (90%)]\tLoss: 0.056699\n",
      "Train Epoch: 13 [70400/71432 (98%)]\tLoss: 0.119297\n",
      "\n",
      "Test set: Average loss: 0.0987, Accuracy: 7815/8141 (96%), Positive accuracy: 3884/4091 (95%), Negative accuracy: 3931/4050 (97%), f1 score: 0.9600\n",
      "\n",
      "Train Epoch: 14 [0/71432 (0%)]\tLoss: 0.114278\n",
      "Train Epoch: 14 [6400/71432 (9%)]\tLoss: 0.107919\n",
      "Train Epoch: 14 [12800/71432 (18%)]\tLoss: 0.129818\n",
      "Train Epoch: 14 [19200/71432 (27%)]\tLoss: 0.039510\n",
      "Train Epoch: 14 [25600/71432 (36%)]\tLoss: 0.042833\n",
      "Train Epoch: 14 [32000/71432 (45%)]\tLoss: 0.193245\n",
      "Train Epoch: 14 [38400/71432 (54%)]\tLoss: 0.106232\n",
      "Train Epoch: 14 [44800/71432 (63%)]\tLoss: 0.115025\n",
      "Train Epoch: 14 [51200/71432 (72%)]\tLoss: 0.165810\n",
      "Train Epoch: 14 [57600/71432 (81%)]\tLoss: 0.107673\n",
      "Train Epoch: 14 [64000/71432 (90%)]\tLoss: 0.101006\n",
      "Train Epoch: 14 [70400/71432 (98%)]\tLoss: 0.035910\n",
      "\n",
      "Test set: Average loss: 0.1013, Accuracy: 7809/8141 (96%), Positive accuracy: 3872/4091 (95%), Negative accuracy: 3937/4050 (97%), f1 score: 0.9593\n",
      "\n",
      "Train Epoch: 15 [0/71432 (0%)]\tLoss: 0.065083\n",
      "Train Epoch: 15 [6400/71432 (9%)]\tLoss: 0.093368\n",
      "Train Epoch: 15 [12800/71432 (18%)]\tLoss: 0.083543\n",
      "Train Epoch: 15 [19200/71432 (27%)]\tLoss: 0.068609\n",
      "Train Epoch: 15 [25600/71432 (36%)]\tLoss: 0.100358\n",
      "Train Epoch: 15 [32000/71432 (45%)]\tLoss: 0.042973\n",
      "Train Epoch: 15 [38400/71432 (54%)]\tLoss: 0.046921\n",
      "Train Epoch: 15 [44800/71432 (63%)]\tLoss: 0.115877\n",
      "Train Epoch: 15 [51200/71432 (72%)]\tLoss: 0.143330\n",
      "Train Epoch: 15 [57600/71432 (81%)]\tLoss: 0.102971\n",
      "Train Epoch: 15 [64000/71432 (90%)]\tLoss: 0.125153\n",
      "Train Epoch: 15 [70400/71432 (98%)]\tLoss: 0.069280\n",
      "\n",
      "Test set: Average loss: 0.0983, Accuracy: 7815/8141 (96%), Positive accuracy: 3889/4091 (95%), Negative accuracy: 3926/4050 (97%), f1 score: 0.9600\n",
      "\n",
      "Train Epoch: 16 [0/71432 (0%)]\tLoss: 0.047638\n",
      "Train Epoch: 16 [6400/71432 (9%)]\tLoss: 0.061732\n",
      "Train Epoch: 16 [12800/71432 (18%)]\tLoss: 0.047918\n",
      "Train Epoch: 16 [19200/71432 (27%)]\tLoss: 0.181946\n",
      "Train Epoch: 16 [25600/71432 (36%)]\tLoss: 0.058396\n",
      "Train Epoch: 16 [32000/71432 (45%)]\tLoss: 0.086347\n",
      "Train Epoch: 16 [38400/71432 (54%)]\tLoss: 0.104609\n",
      "Train Epoch: 16 [44800/71432 (63%)]\tLoss: 0.114340\n",
      "Train Epoch: 16 [51200/71432 (72%)]\tLoss: 0.083960\n",
      "Train Epoch: 16 [57600/71432 (81%)]\tLoss: 0.147891\n",
      "Train Epoch: 16 [64000/71432 (90%)]\tLoss: 0.144431\n",
      "Train Epoch: 16 [70400/71432 (98%)]\tLoss: 0.091007\n",
      "\n",
      "Test set: Average loss: 0.0997, Accuracy: 7817/8141 (96%), Positive accuracy: 3883/4091 (95%), Negative accuracy: 3934/4050 (97%), f1 score: 0.9603\n",
      "\n",
      "Train Epoch: 17 [0/71432 (0%)]\tLoss: 0.048393\n",
      "Train Epoch: 17 [6400/71432 (9%)]\tLoss: 0.147594\n",
      "Train Epoch: 17 [12800/71432 (18%)]\tLoss: 0.050461\n",
      "Train Epoch: 17 [19200/71432 (27%)]\tLoss: 0.093160\n",
      "Train Epoch: 17 [25600/71432 (36%)]\tLoss: 0.074909\n",
      "Train Epoch: 17 [32000/71432 (45%)]\tLoss: 0.102596\n",
      "Train Epoch: 17 [38400/71432 (54%)]\tLoss: 0.038498\n",
      "Train Epoch: 17 [44800/71432 (63%)]\tLoss: 0.105612\n",
      "Train Epoch: 17 [51200/71432 (72%)]\tLoss: 0.224443\n",
      "Train Epoch: 17 [57600/71432 (81%)]\tLoss: 0.341571\n",
      "Train Epoch: 17 [64000/71432 (90%)]\tLoss: 0.148425\n",
      "Train Epoch: 17 [70400/71432 (98%)]\tLoss: 0.069144\n",
      "\n",
      "Test set: Average loss: 0.0997, Accuracy: 7816/8141 (96%), Positive accuracy: 3883/4091 (95%), Negative accuracy: 3933/4050 (97%), f1 score: 0.9601\n",
      "\n",
      "Train Epoch: 18 [0/71432 (0%)]\tLoss: 0.305515\n",
      "Train Epoch: 18 [6400/71432 (9%)]\tLoss: 0.033998\n",
      "Train Epoch: 18 [12800/71432 (18%)]\tLoss: 0.101539\n",
      "Train Epoch: 18 [19200/71432 (27%)]\tLoss: 0.059131\n",
      "Train Epoch: 18 [25600/71432 (36%)]\tLoss: 0.060571\n",
      "Train Epoch: 18 [32000/71432 (45%)]\tLoss: 0.136280\n",
      "Train Epoch: 18 [38400/71432 (54%)]\tLoss: 0.018740\n",
      "Train Epoch: 18 [44800/71432 (63%)]\tLoss: 0.091572\n",
      "Train Epoch: 18 [51200/71432 (72%)]\tLoss: 0.180748\n",
      "Train Epoch: 18 [57600/71432 (81%)]\tLoss: 0.141591\n",
      "Train Epoch: 18 [64000/71432 (90%)]\tLoss: 0.080921\n",
      "Train Epoch: 18 [70400/71432 (98%)]\tLoss: 0.101809\n",
      "\n",
      "Test set: Average loss: 0.0995, Accuracy: 7816/8141 (96%), Positive accuracy: 3883/4091 (95%), Negative accuracy: 3933/4050 (97%), f1 score: 0.9601\n",
      "\n",
      "Train Epoch: 19 [0/71432 (0%)]\tLoss: 0.130183\n",
      "Train Epoch: 19 [6400/71432 (9%)]\tLoss: 0.097520\n",
      "Train Epoch: 19 [12800/71432 (18%)]\tLoss: 0.036642\n",
      "Train Epoch: 19 [19200/71432 (27%)]\tLoss: 0.296278\n",
      "Train Epoch: 19 [25600/71432 (36%)]\tLoss: 0.103613\n",
      "Train Epoch: 19 [32000/71432 (45%)]\tLoss: 0.123244\n",
      "Train Epoch: 19 [38400/71432 (54%)]\tLoss: 0.048501\n",
      "Train Epoch: 19 [44800/71432 (63%)]\tLoss: 0.162154\n",
      "Train Epoch: 19 [51200/71432 (72%)]\tLoss: 0.117224\n",
      "Train Epoch: 19 [57600/71432 (81%)]\tLoss: 0.064882\n",
      "Train Epoch: 19 [64000/71432 (90%)]\tLoss: 0.055350\n",
      "Train Epoch: 19 [70400/71432 (98%)]\tLoss: 0.062591\n",
      "\n",
      "Test set: Average loss: 0.0991, Accuracy: 7816/8141 (96%), Positive accuracy: 3885/4091 (95%), Negative accuracy: 3931/4050 (97%), f1 score: 0.9601\n",
      "\n",
      "Train Epoch: 20 [0/71432 (0%)]\tLoss: 0.135870\n",
      "Train Epoch: 20 [6400/71432 (9%)]\tLoss: 0.080118\n",
      "Train Epoch: 20 [12800/71432 (18%)]\tLoss: 0.196047\n",
      "Train Epoch: 20 [19200/71432 (27%)]\tLoss: 0.054095\n",
      "Train Epoch: 20 [25600/71432 (36%)]\tLoss: 0.171291\n",
      "Train Epoch: 20 [32000/71432 (45%)]\tLoss: 0.126950\n",
      "Train Epoch: 20 [38400/71432 (54%)]\tLoss: 0.061922\n",
      "Train Epoch: 20 [44800/71432 (63%)]\tLoss: 0.043741\n",
      "Train Epoch: 20 [51200/71432 (72%)]\tLoss: 0.117166\n",
      "Train Epoch: 20 [57600/71432 (81%)]\tLoss: 0.059137\n",
      "Train Epoch: 20 [64000/71432 (90%)]\tLoss: 0.066605\n",
      "Train Epoch: 20 [70400/71432 (98%)]\tLoss: 0.093014\n",
      "\n",
      "Test set: Average loss: 0.0997, Accuracy: 7815/8141 (96%), Positive accuracy: 3882/4091 (95%), Negative accuracy: 3933/4050 (97%), f1 score: 0.9600\n",
      "\n",
      "[7698, 7776, 7779, 7800, 7736, 7798, 7799, 7808, 7811, 7813, 7815, 7814, 7815, 7809, 7815, 7817, 7816, 7816, 7816, 7815]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwU5Z3H8c9vhnO4QVQYUFCJcooKREUFNMu5AYm3oIJZMVGS+MrqSjaJq258aaJJWBAj6CIGBMSLmEgWzEZF14sjgCAihygzIDfIMZzz7B9PD9PT9Mz0zPR091R9369Xvbq6qrr7maL51tNPPfWUOecQEZHgykp3AUREpHop6EVEAk5BLyIScAp6EZGAU9CLiARcrXQXINYpp5zi2rVrl+5iiIjUKEuWLNnhnGsZb13GBX27du1YvHhxuoshIlKjmNmXpa1T042ISMAp6EVEAk5BLyIScBnXRi8iwXX06FHy8vI4dOhQuotSY9WrV482bdpQu3bthF+joBeRlMnLy6NRo0a0a9cOM0t3cWoc5xw7d+4kLy+P9u3bJ/w6Nd2ISMocOnSIFi1aKOQrycxo0aJFhX8RKehFJKUU8lVTmf0XmKDfswceeggWLUp3SUREMktggh7gwQdh4cJ0l0JEMtWePXt46qmnKvXawYMHs2fPnoS3f/DBB3niiScq9VnJFpigb9IEcnIgPz/dJRGRTFVW0B87dqzM186bN4+mTZtWR7GqXWCC3gxycxX0IlK6cePGsX79erp37859993H22+/zeWXX87QoUPp1KkTAFdffTUXXXQRnTt3ZsqUKSde265dO3bs2MHGjRvp2LEjd9xxB507d6Z///4UFBSU+bnLli3j4osvplu3bgwfPpzdu3cDMGHCBDp16kS3bt248cYbAXjnnXfo3r073bt354ILLmDfvn1V/rsD1b1SQS9Sc9xzDyxbltz37N4dxo8vff1jjz3GypUrWRb54LfffpulS5eycuXKE90Vp06dSvPmzSkoKKBnz55cc801tGjRosT7rF27llmzZvHMM89w/fXX88orrzBy5MhSP/fWW29l4sSJ9OnThwceeICHHnqI8ePH89hjj/HFF19Qt27dE81CTzzxBJMmTaJ3797s37+fevXqVXGvBKhGD9C6tYJeRCqmV69eJfqkT5gwgfPPP5+LL76YTZs2sXbt2pNe0759e7p37w7ARRddxMaNG0t9/71797Jnzx769OkDwG233cbCyMnEbt26MWLECGbMmEGtWr7e3bt3b376058yYcIE9uzZc2J5VQSuRr95Mzjnm3JEJHOVVfNOpQYNGpyYf/vtt/nb3/7GBx98QE5ODn379o3bZ71u3bon5rOzs8ttuinNG2+8wcKFC/nzn//MI488wieffMK4ceMYMmQI8+bNo3fv3syfP5/zzjuvUu9fJFA1+txcOHIEdu5Md0lEJBM1atSozDbvvXv30qxZM3Jycvjss8/48MMPq/yZTZo0oVmzZrz77rsATJ8+nT59+lBYWMimTZvo168fv/71r9m7dy/79+9n/fr1dO3alfvvv5+ePXvy2WefVbkMgavRg2++OeWU9JZFRDJPixYt6N27N126dGHQoEEMGTKkxPqBAwfy9NNP07FjR84991wuvvjipHzu888/zw9+8AMOHjzIWWedxXPPPcfx48cZOXIke/fuxTnHj3/8Y5o2bcovf/lL3nrrLbKysujcuTODBg2q8uebcy4Jf0by9OjRw1X2xiMffACXXgpvvAGDBye5YCJSZatXr6Zjx47pLkaNF28/mtkS51yPeNsHrukGdEJWRCRaoIK+VSt/ElZBLyJSLFBBX7s2nHqqgl5EJFqggh7Ul15EJFbggr6oL72IiHiBDHrV6EVEigUy6HfsgMOH010SEck0VRmmGGD8+PEcPHgw7rq+fftS2a7h1S2QQQ9qvhGRk1Vn0GeywAa9mm9EJFbsMMUAjz/+OD179qRbt278x3/8BwAHDhxgyJAhnH/++XTp0oUXX3yRCRMmsHnzZvr160e/fv3K/JxZs2bRtWtXunTpwv333w/A8ePHGTVqFF26dKFr1678/ve/B+IPVZxsgRoCART0IjVGGsYpjh2meMGCBaxdu5aPP/4Y5xxDhw5l4cKFbN++ndatW/PGG28AfgycJk2a8Lvf/Y633nqLU8oYY2Xz5s3cf//9LFmyhGbNmtG/f3/mzp1L27Ztyc/PZ+XKlQAnhiWON1RxsqlGLyKhtWDBAhYsWMAFF1zAhRdeyGeffcbatWvp2rUrb775Jvfffz/vvvsuTZo0Sfg9Fy1aRN++fWnZsiW1atVixIgRLFy4kLPOOosNGzbwox/9iP/5n/+hcePGQPyhipMtcDX6pk2hXj0FvUjGy4Bxip1z/OxnP+POO+88ad3SpUuZN28ev/jFL7jqqqt44IEHqvRZzZo1Y/ny5cyfP5+nn36aOXPmMHXq1LhDFSc78ANXoy+6paBOxopIrNhhigcMGMDUqVPZv38/APn5+Wzbto3NmzeTk5PDyJEjue+++1i6dGnc18fTq1cv3nnnHXbs2MHx48eZNWsWffr0YceOHRQWFnLNNdfwq1/9iqVLl5Y6VHGyBa5GD+pLLyLxxQ5T/Pjjj7N69WouueQSABo2bMiMGTNYt24d9913H1lZWdSuXZs//OEPAIwZM4aBAwfSunVr3nrrrbif0apVKx577DH69euHc44hQ4YwbNgwli9fzujRoyksLATg0UcfLXWo4mQL1DDFRW6+GT76CNavT1KhRCQpNExxclTLMMVmNtDM1pjZOjMbF2f9FWa21MyOmdm1Met+Y2arzGy1mU0wq/6b/BXV6DPsGCYikhblBr2ZZQOTgEFAJ+AmM+sUs9lXwChgZsxrLwV6A92ALkBPoE+VS12O3Fx/ZeyuXdX9SSIimS+RGn0vYJ1zboNz7ggwGxgWvYFzbqNzbgVQGPNaB9QD6gB1gdrA1iqXuhzqYimSuTKtubimqcz+SyToc4FNUc/zIssSKdAHwFvAlsg03zm3OnY7MxtjZovNbPH27dsTeeuyC6ygF8lI9erVY+fOnQr7SnLOsXPnTurVq1eh11VrrxszOwfoCLSJLHrTzC53zr0bvZ1zbgowBfzJ2Kp+buvW/lFBL5JZ2rRpQ15eHsmo0IVVvXr1aNOmTfkbRkkk6POBtlHP20SWJWI48KFzbj+Amf0VuAR4t8xXVZGCXiQz1a5dm/bt26e7GKGTSNPNIqCDmbU3szrAjcDrCb7/V0AfM6tlZrXxJ2JParpJtjp1oGVLXTQlIgIJBL1z7hgwFpiPD+k5zrlVZvawmQ0FMLOeZpYHXAdMNrNVkZe/DKwHPgGWA8udc3+uhr/jJLpoSkTES6iN3jk3D5gXs+yBqPlFFLfDR29zHDh5EIkUUNCLiHiBG+umiIJeRMQLdNBv365bCoqIBDroAbZsSW85RETSLfBBr+YbEQm7wAa9+tKLiHiBDfqiGr360otI2AU26Js3h7p1VaMXEQls0BfdUlBBLyJhF9igBwW9iAgo6EVEAi8UQa+hr0UkzAIf9IcOwe7d6S6JiEj6BDro1ZdeRCTgQa++9CIiIQl61ehFJMwCHfRquhERCXjQ160Lp5yioBeRcAt00IP60ouIKOhFRAJOQS8iEnCBD/rWrWHbNjhyJN0lERFJj8AHvW4pKCJhF5qg10VTIhJWoQl6tdOLSFgp6EVEAi7wQd+ihW4pKCLhFvigN/M9bxT0IhJWgQ96UF96EQm3UAS9avQiEmahCHrdUlBEwiw0QV9QAHv3prskIiKpF5qgBzXfiEg4JRT0ZjbQzNaY2TozGxdn/RVmttTMjpnZtTHrzjCzBWa22sw+NbN2ySl64hT0IhJm5Qa9mWUDk4BBQCfgJjPrFLPZV8AoYGact/gj8LhzriPQC9hWlQJXhoJeRMKsVgLb9ALWOec2AJjZbGAY8GnRBs65jZF1hdEvjBwQajnn3oxstz85xa4Y3VJQRMIskaabXGBT1PO8yLJEfAvYY2avmtk/zOzxyC+ElKpXz18hq6AXkTCq7pOxtYDLgXuBnsBZ+CaeEsxsjJktNrPF27dvr5aCqC+9iIRVIkGfD7SNet4msiwRecAy59wG59wxYC5wYexGzrkpzrkezrkeLVu2TPCtK0ZXx4pIWCUS9IuADmbW3szqADcCryf4/ouApmZWlN5XEtW2n0q5uRqTXkTCqdygj9TExwLzgdXAHOfcKjN72MyGAphZTzPLA64DJpvZqshrj+Obbf7XzD4BDHimev6UsuXmwtatcPRoOj5dRCR9Eul1g3NuHjAvZtkDUfOL8E068V77JtCtCmVMitxcPwTC119D27blby8iEhShuDIW1JdeRMJLQS8iEnAKehGRgAtN0LdoAbVrK+hFJHxCE/RZWbpoSkTCKTRBD7poSkTCKXRBr4umRCRsQhf0uqWgiIRN6IL+wAH45pt0l0REJHVCF/SgdnoRCRcFvYhIwIUq6HWnKREJo1AFvWr0IhJGoQr6+vWhWTMFvYiES6iCHtSXXkTCJ5RBrxq9iISJgl5EJOBCGfRbt8KxY+kuiYhIaoQy6AsL/S0FRUTCIJRBD2q+EZHwCF3Q66IpEQmb0AW9avQiEjahC/qWLf0tBdWXXkTCInRBn5UFrVqpRi8i4RG6oAf1pReRcFHQi4gEnIJeRCTgQhv0+/frloIiEg6hDHr1pReRMAll0KsvvYiEiYJeRCTgQh30umhKRMIgoaA3s4FmtsbM1pnZuDjrrzCzpWZ2zMyujbO+sZnlmdmTySh0VeXkQNOmqtGLSDiUG/Rmlg1MAgYBnYCbzKxTzGZfAaOAmaW8zX8CCytfzORTF0sRCYtEavS9gHXOuQ3OuSPAbGBY9AbOuY3OuRVAYeyLzewi4DRgQRLKmzQKehEJi0SCPhfYFPU8L7KsXGaWBfwWuLfiRateCnoRCYvqPhl7FzDPOZdX1kZmNsbMFpvZ4u3bt1dzkbzWrf1dpnRLQREJuloJbJMPtI163iayLBGXAJeb2V1AQ6COme13zpU4oeucmwJMAejRo4dL8L2rpOiWglu3FvfCEREJokSCfhHQwcza4wP+RuDmRN7cOTeiaN7MRgE9YkM+XaL70ivoRSTIym26cc4dA8YC84HVwBzn3Coze9jMhgKYWU8zywOuAyab2arqLHQyqC+9iIRFIjV6nHPzgHkxyx6Iml+Eb9Ip6z2mAdMqXMJEFRTA7Nlw5ZVw5pnlbq6rY0UkLIJzZezOnXDHHfDUUwltfuqpUKuWgl5Egi84Qd+mDQwfDs8+62v35dAtBUUkLIIT9ABjx8KuXb4JJwHqSy8iYRCsoL/iCujSBSZOBFd+L83WrRX0IhJ8wQp6M1+r/8c/4MMPy91cNXoRCYNgBT3AiBHQpAk8Wf5Ambm5sG+fn0REgip4Qd+wIYweDS+95Mc4KIP60otIGAQv6AHuuguOHoVnnilzM/WlF5EwCGbQd+gAAwfC00/7wC+Fgl5EwiCYQQ/+pOzmzTB3bqmbKOhFJAyCG/QDB8JZZ5V5UrZBA3/eVkEvIkEW3KDPzvZt9QsXwooVpW6mvvQiEnTBDXrwvW/q14dJk0rdRH3pRSTogh30zZv7fvUzZsDu3XE3UdCLSNAFO+gB7r4bDh6EadPirs7N9d3tjx9PbbFERFIl+EHfvTtcdplvviksPGl1bq4P+W3b0lA2EZEUCH7Qg+9quX49zJ9/0ip1sRSRoAtH0A8f7gefj9PVUkEvIkEXjqCvUwfuvBP++ldYt67EKgW9iARdOIIeYMwY37f+D38osfjUU/1iBb2IBFV4gr5VK7jmGpg6FQ4cOLE4OxtOP11BLyLBFZ6gB39Sds8emDmzxGL1pReRIAtX0PfuDeef70/KRt1qUEEvIkEWrqAvutXgihXw3nsnFufm6uYjIhJc4Qp6gJtvhqZNS3S1zM2FvXtLNN2LiARG+II+Jwe+/3149dUT7TXqYikiQRa+oAf44Q/9uAdTpgABCfrDh/05iNmz010SEckw4Qz6s8+GwYNh8mQ4coTWrf3iGh30L74I778Pv/lNuksiIhkmnEEP/qTs1q3wyis1v0bvHEycCFlZ8I9/wPLl6S6RiGSQ8AZ9//5wzjnw5JM0agSNGtXgoP/oI1i8GB56yA/38Nxz6S6RiGSQ8AZ9VpYfq/7992Hp0prdl37CBGjcGO65B4YNgxdegCNH0l0qEckQ4Q16gFGjfC+cSZNqbl/6zZvhpZfg9tuhYUN/+8QdO+Avf0l3yUQkQ4Q76Js2hVtugZkz+VaLnTWzRj95su9BdPfd/nn//v6O52q+EZGIhILezAaa2RozW2dm4+Ksv8LMlprZMTO7Nmp5dzP7wMxWmdkKM7shmYVPirvvhkOH+O72qWzZEvcmVJnryBEf9IMH+/MN4Edpu/VWPyTz11+nt3wikhHKDXozywYmAYOATsBNZtYpZrOvgFHAzJjlB4FbnXOdgYHAeDNrWtVCJ1XXrtCnD71XPEXhseM165aCL73kew796Ecll48e7Wv506enp1wiklESqdH3AtY55zY4544As4Fh0Rs45zY651YAhTHLP3fOrY3Mbwa2AS2TUvJkGjuWxjs3Mph5Nav5ZsIEOPdc+Kd/Krn8W9+CSy/1zTdRg7eJSDglEvS5wKao53mRZRViZr2AOsD6OOvGmNliM1u8ffv2ir511Q0bxpGWuYzlyZoT9B99BB9/7K8HyIrzzzhqFKxe7bcRkVBLyclYM2sFTAdGO+dOagV3zk1xzvVwzvVo2TINFf7atTk06gcMYAFLZ62pGe30Eyf6zv+33RZ//Q03QP36OikrIgkFfT7QNup5m8iyhJhZY+AN4OfOuQ8rVrzUafTTOzicXZ/Bs2/h5mEH+OabdJeoDF9/DXPm+Lb4Ro3ib9O4sb+j1uzZUFCQ2vKJSEZJJOgXAR3MrL2Z1QFuBF5P5M0j278G/NE593Lli1n97PTTqPPyLHrYEkb+5UYu7XWMNWvSXapSTJkCR48Wd6kszejRfvzl115LTblEJCOVG/TOuWPAWGA+sBqY45xbZWYPm9lQADPraWZ5wHXAZDNbFXn59cAVwCgzWxaZulfLX5IEdvUwsiY9yT/zF+778m569XSZd93RkSP+BueDBvmTrmXp2xfatVPzjUjI1UpkI+fcPGBezLIHouYX4Zt0Yl83A5hRxTKm1g9/CJs2cdujj3KgWVuGDv0FDz0EP/95/HOeKffKK77pJrZLZTxZWb4N/+GH4auv4Iwzqr98IpJxMiG6Ms8jj8DIkdy15Zc8ffE0HnjAN3fv25fuguG7VHboAAMGJLb9bbf5LpbPP1+95RKRjKWgj8cM/vu/4Tvf4Y5Fd/DqnfP585/h29+Gzz9PY7kWL4YPPyy9S2U87dtDv34wbVoNu+xXRJJFQV+aOnXglVewzp0Z/sK1fDBpKdu3Q8+eaRwvbOJEP3DZqFEVe93o0bBhA7z7brUUS0Qym4K+LI0bw7x50Lw5PR8cwj9e28g558DQofCf/5niCvK2bb6r5KhRvlwVcc01vhumTsqKhJKCvjytW/sBwg4dos2/DOS9P+1kxAhS324/ZYrvcTN2bMVfm5PjL6B66aUMOdEgIqmkoE9Ep07w+uuwcSP1bxjKHycXMH48qWu3P3rUd6kcMMCPbVMZo0fDwYM+7EUkVBT0ibr8cpgxAz74ABs5gp+MPc6bb5KadvtXX/U3GEmkS2VpLrnEHyTUfCMSOgr6irj2Wvjd7/yVpvfcQ7++jsWLqf52+wkT4Oyz/UVSlWXm2/ffew/Wrk1a0UQk8ynoK+qee+CnP4Unn4QnnuDMM312FrXbX3IJvPNOEj9v6VJ/X9uKdKksza23+veYNi0pRRORmkFBXxmPP+5Pbv7bv8HMmdSvD3/8o78mafNmP/LAkCGwYkUSPmviRGjQwLexV1Xr1r6d//nn/Y1JRCQUFPSVkZXlw7JPH98c8ve/Y+YrzJ9/7o8DH3wA3bv7C1O//LKSn7N9O8ya5d+kSZPklH30aMjPh7/9LTnvJyIZT0FfWXXrwty5fmCx4cNPVN/r14d774X1632Ff84cv8m//ivs3FnBz3jmGTh8uHJdKkszdCg0b66TsiIhoqCviqZNfR/7Ro38Dbo3Fd+Iq1kzeOwxX8MfORLGj4ezzoJHH/W9HMt19Cg89ZS/TWDHjskrc926cPPN/iC1e3fy3ldEMpaCvqratvVhv2+f7xUTcyvEtm39sDkrVvi2+3//dz8m2TPPwLFjZbzv3Lm+iaUqXSpLM3q0/6Uwa1by31tEMo6CPhm6dvVdLj//3Ffbx407KfA7d4Y//ckPN9OuHYwZA126+JfFvX/3xIn+vQYPTn55L7gAunVT841ISCjok+XKK2HZMvjud+E3v/Fpfu+9fuz4KJdd5rtjzp3ru7Z/73tw6aUx440tW+YX3H03ZGcnv6xmvla/eDGsXJn89xeRjKKgT6ZOnWDmTPj0Uz8Qzu9/74cJ/slPfDNMhBkMGwaffALPPuvvCXLFFf4YsXgxvjafkwO33159ZR0xAmrVUq1eJAQU9NXhvPN8x/o1a+Cmm2DSJN8Mc/fdPtUjatWC73/fX6j62GO+Ej+w5w4OP/cCGy67leONmlZfGVu29EeWGTP8iV8RCSwFfXU65xyYOtUn+ahR/gzsOefAnXfCxo0nNsvJgfvv98eAuUOepa47zHcXjOWcc/yIC3v3VlP5Ro/2wx/Pm1f+tiJSYynoU6F9e5g8Gdatgzvu8EMQdOjgq/Pr1p3YrHHOMS5b8RTuyqv41audadvW979v29aPvLBhQ5LLNWgQnHaamm9EAk5Bn0pnnOGbcTZsgLvu8u35557rL6lds8Z3y9m0Cfvxjxg+HBYu9G32w4b5l51zDieWx+2pU1G1asEtt8Abb/iavYgEkrmkJEby9OjRwy1evDjdxUiNLVvgiSf8WPOHDkGLFv5WgevWndTbZvNmH/ZPPw27dsGFF/pa/g03+LseVtqnn/q+n7/9rR+sTURqJDNb4pzrEW+davTp1KqVD9iNG/14CYcP+z74cbpUtm4NjzziL76dPBkKCvwPgXbt/PIdOypZhk6doFcv33yTYQd9EUkOBX0mOPVU3+1m715/orYMOTn+YquVK/0FuV27wi9+4dvxx4yBJUsqkdejR/s3XLKk8n+DiGQsNd0EwKpV8F//BdOn+xagc8/1w9mMGOHvV1KuPXv8r4vbb/ftQ4nat893FSqa8vL85b5Dh/rR3UQkZcpqulHQB8ju3fDyy/DCC8U3P/n2t33g33CD/+FQqptv9j8RtmyBevX8rbK+/toH+JdfFod59Hxpg6I1buzvxnXLLf5KsKreMEVEyqWgD6FNm/yYZS+84AdUy872A2GOGAFXX+3P+Zbw5pvQv78fRP+bb/wbxF5I1aSJ7zl05pn+MXb+tNN8l6Dp0+GVV2D/fr98xAgf+skchVNESlDQh9zKlT7wZ870FfGcHN9lc8QIn+21a+Nr8MOH+0HzYwO8aKrIzU8OHvTdRadPhwUL/B2tLrrIB/5NN5Xz80JEKkpBL4DP8vff96E/Z47vpnnKKXD99T70L7nEj8OTdFu3+p8X06f7e+BmZ/tbGo4c6Y84OTnV8KEi4aKgl5McOQLz5/vQ/9Of/Enc9u19Db9XLz917FgNg2d++qkfX2fGDN881KiRHwDullv8gP2x7fmHD/uTxXv3+sfoKXbZN9/4s88DBvjbPOoAIiGioJcy7dvnx8WfPdvX+IvG1mnQAHr0KA7+Xr18N86k1PoLC4vb8196yReiTRvIzS0Z4ocOlf0+2dn+Tl9Nm/oTD2vW+NfUqQOXX+5Dv39/P/5+tfxcEckMCnpJWGGhH4Pt44+Lp2XL/C8A8Odbo4O/Z09/28QqKSiA11+HF1/0bftNmhSHd9OmJZ/HrsvJKRnghw75YUDnz/fnBj75xC8//XQf+P37+7PSOkdQUl6ev0lCkyb+JgkNGqS7RFJBVQ56MxsI/BeQDTzrnHssZv0VwHigG3Cjc+7lqHW3Ab+IPP2Vc+75sj5LQZ95Dh/2PXeiw/+zz4rXd+hQHPydOvkmoDPOiJzkTbfNm33gL1jgexYVXUJ84YU+9AcM8Hd+qdI4EjVUXp7vj/vSS/6nXJGGDeG66/yIq5dfrl9CNUSVgt7MsoHPgX8C8oBFwE3OuU+jtmkHNAbuBV4vCnozaw4sBnoADlgCXOScK/Wu1Ar6mmHvXj/gWlHwf/SR74JfJCvLt8S0b++H4m/fvuR0+ulp6F5fWOhPBi9Y4Gv877/vb9zboAH06+eDv2dPX+CWLYMZcPn5xeH+f//nl51/vg/2a6/1t8B87jl/tn7/fr8vRo3y422ceWZaiy5lq2rQXwI86JwbEHn+MwDn3KNxtp0G/CUq6G8C+jrn7ow8nwy87Zwr9a7UCvqaKz/fN/t88UXJacOGkgcB8NdktWt38gHgjDN8M/1pp1XPXRRL+OYbePvt4maeqCGjadDAh1zsdPbZPvDq1avmwiVRfr6/rmHOnOJw79bNd7e67jr41rdOfs2BA/Dqq35I7b//3S+78ko/XMb3vpf8E9179vhLvHft8mN4VHaqVcv/25Q11a/vH+vUSf/B3Dn/Pdy1y3dtzsryvzYroapBfy0w0Dn3L5HntwDfds6NjbPtNEoG/b1APefcryLPfwkUOOeeiHndGGAMwBlnnHHRl19+WbG/UDJeQYG/qDb2IFB0INizp+T22dm+1p+bW/Z00oVfVfHFF7B6Naxf7wsVPR08WLydmf/weAeCOnX8ieX9+/1j9Hwijw0b+p9Cbdr4M9+x861a+TArz+bNJcPdOT8wUlG4n3tu4vtl40Z/x7Rp0/w+atTIv8/o0b7ZqyJheeiQb/f75BN/gUfR46ZNib9HMsUeBHJy/N/XsGHlHo8f94FdFNzlze/a5X9VFunVy/88roSMD/poqtGH0549PkPy8nwFND+/5Hx+fvw7bTVu7DO3qMNOy5bQvLk/Qdy8+cnzjRpVohLnnB+vP94BYMOGEvcDLlP9+ieHQ+z8vn3+D8/L8+EXfYABX+Nr1Un0jQEAAAf/SURBVKr4ABB9EGjdGpYv9+H+3nvF4X7ddX4677wK/uExCgv9+z73nG/6OXDA3yShqGmnbdvibY8f9/smNtDXrvXrwJ/E6djRj4/Utat/PO00/zeaVW46ftwfTKKngoKTl8VbX1Dgp9IOwuX1ACtLTo4fhrx585KPsctyc/2FhZWgphsJhAMHSgZ/7IEgP9+faz18uPT3yM72wR/vQNCsmT9wNGiQ2HTi/G1Bga/1btjgwzBeba9hw4q3RTnnj4DRwV80X/R80ya/Y6J16VJcc69quJdm/37/i2HaNN/8ZQbf+Y4/2Kxc6a+XKCjw25r5XztFYV702KFDhpyxT9DRo35fl/WrLDs7fpCnoKmvqkFfC38y9iogH38y9mbn3Ko4206jZNA3x5+ALWp0Woo/GburtM9T0EtVFRQU/yrevTv+Y7xle/ZUbIjn2rVPDv+cnOKpfv2Kz9er59+3Th0/Fc0XPWZnx/wiKWrjLQr/M85I/ZhCGzb4pp3p0/0vkNhA79RJ3TVTIBndKwfju09mA1Odc4+Y2cPAYufc62bWE3gNaAYcAr52znWOvPZ24N8jb/WIc67MG5Qq6CVdCgt9ha0q08GDxVNBQcnnVfnlX8Qs8QNBZVo+6tQpebCKnmIPZPGmWrWKz4tCxefNfMtNdrafEpmPfg6+9aawMP5jeeuysor3Z+y+rV3b/33pPn9bGl0wJZIBCgt92Mc7EBQU+APFkSPF09GjJR/LW3bkSHHzd2U7rRw5UvLgFD0VFqZ3/2WKeAeAosfsbL+fivZnRecvusiPFl4ZZQV9AqfvRSQZsrKKa741TXkHgYMH/YGq6EBT9AuhovPOnVzzLms+9nn0L4LSHstaV1h48sE03mNp64p+FRSVo+jvSmTezHcxrg4KehEplxnUreunKg95ISmnW/+IiAScgl5EJOAU9CIiAaegFxEJOAW9iEjAKehFRAJOQS8iEnAKehGRgMu4IRDMbDtQlQHpTwF2JKk41UHlqxqVr2pUvqrJ5PKd6ZxrGW9FxgV9VZnZ4tLGe8gEKl/VqHxVo/JVTaaXrzRquhERCTgFvYhIwAUx6KekuwDlUPmqRuWrGpWvajK9fHEFro1eRERKCmKNXkREoijoRUQCrkYGvZkNNLM1ZrbOzMbFWV/XzF6MrP/IzNqlsGxtzewtM/vUzFaZ2U/ibNPXzPaa2bLI9ECqyhdVho1m9knk80+6d6N5EyL7cIWZXRjvfaqpbOdG7ZtlZvaNmd0Ts01K96GZTTWzbWa2MmpZczN708zWRh7j3pLDzG6LbLPWzG5LYfkeN7PPIv9+r5lZ01JeW+Z3oRrL96CZ5Uf9Gw4u5bVl/n+vxvK9GFW2jWa2rJTXVvv+qzLnXI2a8DcoXw+cBdQBlgOdYra5C3g6Mn8j8GIKy9cKuDAy3wj4PE75+gJ/SfN+3AicUsb6wcBfAQMuBj5K47/31/iLQdK2D4ErgAuBlVHLfgOMi8yPA34d53XNgQ2Rx2aR+WYpKl9/oFZk/tfxypfId6Eay/cgcG8C//5l/n+vrvLFrP8t8EC69l9Vp5pYo+8FrHPObXDOHQFmA8NithkGPB+Zfxm4yiw19253zm1xzi2NzO8DVgO5qfjsJBsG/NF5HwJNzaxVGspxFbDeOVeVq6WrzDm3ENgVszj6e/Y8cHWclw4A3nTO7XLO7QbeBAamonzOuQXOuWORpx8CbZL9uYkqZf8lIpH/71VWVvki2XE9MCvZn5sqNTHoc4FNUc/zODlIT2wT+aLvBVqkpHRRIk1GFwAfxVl9iZktN7O/mlnnlBbMc8ACM1tiZmPirE9kP6fCjZT+Hyzd+/A059yWyPzXwGlxtsmU/Xg7/hdaPOV9F6rT2EjT0tRSmr4yYf9dDmx1zq0tZX06919CamLQ1whm1hB4BbjHOfdNzOql+KaI84GJwNxUlw+4zDl3ITAIuNvMrkhDGcpkZnWAocBLcVZnwj48wfnf8BnZV9nMfg4cA14oZZN0fRf+AJwNdAe24JtHMtFNlF2bz/j/SzUx6POBtlHP20SWxd3GzGoBTYCdKSmd/8za+JB/wTn3aux659w3zrn9kfl5QG0zOyVV5Yt8bn7kcRvwGv4ncrRE9nN1GwQsdc5tjV2RCfsQ2FrUnBV53BZnm7TuRzMbBfwzMCJyMDpJAt+FauGc2+qcO+6cKwSeKeVz073/agHfA14sbZt07b+KqIlBvwjoYGbtIzW+G4HXY7Z5HSjq3XAt8PfSvuTJFmnP+29gtXPud6Vsc3rROQMz64X/d0jlgaiBmTUqmseftFsZs9nrwK2R3jcXA3ujmilSpdSaVLr3YUT09+w24E9xtpkP9DezZpGmif6RZdXOzAYC/wYMdc4dLGWbRL4L1VW+6HM+w0v53ET+v1en7wCfOefy4q1M5/6rkHSfDa7MhO8R8jn+bPzPI8sexn+hAerhf+6vAz4Gzkph2S7D/4RfASyLTIOBHwA/iGwzFliF70HwIXBpivffWZHPXh4pR9E+jC6jAZMi+/gToEeKy9gAH9xNopalbR/iDzhbgKP4duLv48/7/C+wFvgb0DyybQ/g2ajX3h75Lq4DRqewfOvw7dtF38OinmitgXllfRdSVL7pke/WCnx4t4otX+T5Sf/fU1G+yPJpRd+5qG1Tvv+qOmkIBBGRgKuJTTciIlIBCnoRkYBT0IuIBJyCXkQk4BT0IiIBp6AXEQk4Bb2ISMD9P59T+84jNaDpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class CNNTrainedGabor(nn.Module):\n",
    "    def __init__(self, pretrain_gabor_model):\n",
    "        super(CNNTrainedGabor, self).__init__()\n",
    "        self.pretrain_gabor_model = pretrain_gabor_model\n",
    "        self.conv1 = nn.Conv2d(1, 4, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 3, 1)\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "#         self.dropout2 = nn.Dropout2d(0.5)\n",
    "        # self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc1 = nn.Linear(72,15)\n",
    "#         self.fc2 = nn.Linear(128, 2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(16, 2)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        pc = self.pc(x).view(x.shape[0],1)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "#         x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "#         x = self.dropout2(x)\n",
    "#         print(pc.shape, x.shape)\n",
    "        x = torch.cat((pc, x), 1)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    def pc(self, x):\n",
    "        filter_cos = self.pretrain_gabor_model['filter_cos']\n",
    "        filter_sin = self.pretrain_gabor_model['filter_sin']\n",
    "        bias1 = self.pretrain_gabor_model['bias1']\n",
    "        bias2 = self.pretrain_gabor_model['bias2']\n",
    "        weights = self.pretrain_gabor_model['weights']\n",
    "        w = self.pretrain_gabor_model['w']\n",
    "        b = self.pretrain_gabor_model['b']\n",
    "        \n",
    "        x_cos = F.conv2d(x, filter_cos, bias=bias1)\n",
    "        x_sin = F.conv2d(x, filter_sin, bias=bias2)\n",
    "        x_comb = torch.cat((x_cos, x_sin), 2)\n",
    "\n",
    "        x_cos = x_cos.view(len(x), 1, 1, 48)\n",
    "        x_sin = x_sin.view(len(x), 1, 1, 48)\n",
    "        weighted_cos = (torch.matmul(x_cos, weights)).view(len(x), 1)\n",
    "        weighted_sin = (torch.matmul(x_sin, weights)).view(len(x), 1)\n",
    "\n",
    "        numerator = torch.norm(torch.cat([weighted_cos, weighted_sin], 1), dim=1)\n",
    "    #         print(\"numerator\", numerator.size())\n",
    "        x_comb_norm = torch.norm(x_comb, dim=2)\n",
    "        x_comb_norm = x_comb_norm.view(len(x), 1, 48)\n",
    "    #         print(\"x_comb_norm\", x_comb_norm.size())\n",
    "        denominator = torch.matmul(x_comb_norm, torch.abs(weights))\n",
    "        denominator = denominator.view(len(x))\n",
    "    #         print(\"size:\", numerator.size(), denominator.size())\n",
    "        pc = numerator / denominator                \n",
    "        return torch.sigmoid(w * pc + b)\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "    train_loss = sum(loss_list)/len(loss_list)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "# def test(model, device, test_loader):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             output = model(data)\n",
    "#             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "#             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "#             correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "\n",
    "#     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         test_loss, correct, len(test_loader.dataset),\n",
    "#         100. * correct / len(test_loader.dataset)))\n",
    "def test(model, device, test_loader,epoch,train_loss):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    result= [[0,0], [0,0]] \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            cmat = confusion_matrix(target.view_as(pred), pred, labels=[0, 1]) \n",
    "            result = [[result[i][j] + cmat[i][j]  for j in range(len(result[0]))] for i in range(len(result))] \n",
    "            \n",
    "            # Store wrongly predicted images\n",
    "            if epoch == 7:\n",
    "                wrong_idx = (pred != target.view_as(pred)).nonzero()[:, 0]\n",
    "                wrong_samples = data[wrong_idx]\n",
    "                wrong_preds = pred[wrong_idx]\n",
    "                actual_preds = target.view_as(pred)[wrong_idx]\n",
    "                for i in range(len(wrong_idx)):\n",
    "                    sample = wrong_samples[i]\n",
    "                    wrong_pred = wrong_preds[i]\n",
    "                    actual_pred = actual_preds[i]\n",
    "                    sample = sample * 255.\n",
    "                    sample = sample.byte()\n",
    "                    img = TF.to_pil_image(sample)\n",
    "                    img.save('./wrong-mix/epoch{}_batch{}_idx{}_actual{}.png'.format(\n",
    "                        epoch,batch_idx,wrong_idx[i], actual_pred.item()))\n",
    "                    num = batch_idx * 64 + wrong_idx[i]\n",
    "                    img_ori = origin_dataset[num][0].numpy()\n",
    "                    plt.imsave('./wrong-mix/epoch{}_batch{}_idx{}_actual{}_ori.png'.format(\n",
    "                    epoch,batch_idx,wrong_idx[i], actual_pred.item()), img_ori[0], cmap = 'gray')\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    precision = result[1][1]/(result[1][1]+result[0][1])\n",
    "    recall = result[0][0]/(result[0][0]+result[1][0])\n",
    "    f1score = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Positive accuracy: {}/{} ({:.0f}%), Negative accuracy: {}/{} ({:.0f}%), f1 score: {:.4f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset), \n",
    "        result[1][1],result[1][1]+result[1][0],100. * result[1][1]/(result[1][1]+result[1][0]),\n",
    "        result[0][0],result[0][0]+result[0][1],100. * result[0][0]/(result[0][0]+result[0][1]),f1score))\n",
    "    return test_loss, correct\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=100, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'batch_size': args.batch_size}\n",
    "    if use_cuda:\n",
    "        kwargs.update({'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True},\n",
    "                     )\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    model = CNNTrainedGabor(pretrain_gabor_model).to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_accuracy_list = []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_result = test(model, device, test_loader,epoch,train_loss)\n",
    "        test_loss_list.append(test_result[0])\n",
    "        test_accuracy_list.append(test_result[1])\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "    plt.plot(train_loss_list, color='blue',label='train loss')  \n",
    "    plt.plot(test_loss_list, color='red',label='test loss')  \n",
    "    plt.legend()\n",
    "    print(test_accuracy_list)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9808aeb850>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [7542, 7698, 7836, 7804, 7859, 7892, 7934, 7897, 7900, 7914, 7902, 7895, 7909, 7910, 7901, 7910, 7908, 7910, 7909, 7910]\n",
    "plt.plot(x, color='blue',label='Accuracy')  \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/71432 (0%)]\tLoss: 0.723132\n",
      "Train Epoch: 1 [6400/71432 (9%)]\tLoss: 0.181368\n",
      "Train Epoch: 1 [12800/71432 (18%)]\tLoss: 0.164086\n",
      "Train Epoch: 1 [19200/71432 (27%)]\tLoss: 0.299986\n",
      "Train Epoch: 1 [25600/71432 (36%)]\tLoss: 0.116206\n",
      "Train Epoch: 1 [32000/71432 (45%)]\tLoss: 0.142081\n",
      "Train Epoch: 1 [38400/71432 (54%)]\tLoss: 0.098413\n",
      "Train Epoch: 1 [44800/71432 (63%)]\tLoss: 0.165801\n",
      "Train Epoch: 1 [51200/71432 (72%)]\tLoss: 0.157562\n",
      "Train Epoch: 1 [57600/71432 (81%)]\tLoss: 0.093857\n",
      "Train Epoch: 1 [64000/71432 (90%)]\tLoss: 0.143686\n",
      "Train Epoch: 1 [70400/71432 (98%)]\tLoss: 0.099544\n",
      "\n",
      "Test set: Average loss: 0.1291, Accuracy: 7689/8141 (94%), Positive accuracy: 3794/4091 (93%), Negative accuracy: 3895/4050 (96%), f1 score: 0.9447\n",
      "\n",
      "Train Epoch: 2 [0/71432 (0%)]\tLoss: 0.172338\n",
      "Train Epoch: 2 [6400/71432 (9%)]\tLoss: 0.093837\n",
      "Train Epoch: 2 [12800/71432 (18%)]\tLoss: 0.047705\n",
      "Train Epoch: 2 [19200/71432 (27%)]\tLoss: 0.102612\n",
      "Train Epoch: 2 [25600/71432 (36%)]\tLoss: 0.120843\n",
      "Train Epoch: 2 [32000/71432 (45%)]\tLoss: 0.053516\n",
      "Train Epoch: 2 [38400/71432 (54%)]\tLoss: 0.099203\n",
      "Train Epoch: 2 [44800/71432 (63%)]\tLoss: 0.140209\n",
      "Train Epoch: 2 [51200/71432 (72%)]\tLoss: 0.076230\n",
      "Train Epoch: 2 [57600/71432 (81%)]\tLoss: 0.057741\n",
      "Train Epoch: 2 [64000/71432 (90%)]\tLoss: 0.089737\n",
      "Train Epoch: 2 [70400/71432 (98%)]\tLoss: 0.108630\n",
      "\n",
      "Test set: Average loss: 0.1351, Accuracy: 7686/8141 (94%), Positive accuracy: 3955/4091 (97%), Negative accuracy: 3731/4050 (92%), f1 score: 0.9447\n",
      "\n",
      "Train Epoch: 3 [0/71432 (0%)]\tLoss: 0.161428\n",
      "Train Epoch: 3 [6400/71432 (9%)]\tLoss: 0.076482\n",
      "Train Epoch: 3 [12800/71432 (18%)]\tLoss: 0.081578\n",
      "Train Epoch: 3 [19200/71432 (27%)]\tLoss: 0.129778\n",
      "Train Epoch: 3 [25600/71432 (36%)]\tLoss: 0.104735\n",
      "Train Epoch: 3 [32000/71432 (45%)]\tLoss: 0.041063\n",
      "Train Epoch: 3 [38400/71432 (54%)]\tLoss: 0.110686\n",
      "Train Epoch: 3 [44800/71432 (63%)]\tLoss: 0.086706\n",
      "Train Epoch: 3 [51200/71432 (72%)]\tLoss: 0.107425\n",
      "Train Epoch: 3 [57600/71432 (81%)]\tLoss: 0.130207\n",
      "Train Epoch: 3 [64000/71432 (90%)]\tLoss: 0.112593\n",
      "Train Epoch: 3 [70400/71432 (98%)]\tLoss: 0.083332\n",
      "\n",
      "Test set: Average loss: 0.1083, Accuracy: 7771/8141 (95%), Positive accuracy: 3877/4091 (95%), Negative accuracy: 3894/4050 (96%), f1 score: 0.9546\n",
      "\n",
      "Train Epoch: 4 [0/71432 (0%)]\tLoss: 0.057797\n",
      "Train Epoch: 4 [6400/71432 (9%)]\tLoss: 0.101003\n",
      "Train Epoch: 4 [12800/71432 (18%)]\tLoss: 0.108890\n",
      "Train Epoch: 4 [19200/71432 (27%)]\tLoss: 0.166451\n",
      "Train Epoch: 4 [25600/71432 (36%)]\tLoss: 0.055861\n",
      "Train Epoch: 4 [32000/71432 (45%)]\tLoss: 0.050115\n",
      "Train Epoch: 4 [38400/71432 (54%)]\tLoss: 0.012048\n",
      "Train Epoch: 4 [44800/71432 (63%)]\tLoss: 0.053490\n",
      "Train Epoch: 4 [51200/71432 (72%)]\tLoss: 0.052022\n",
      "Train Epoch: 4 [57600/71432 (81%)]\tLoss: 0.042562\n",
      "Train Epoch: 4 [64000/71432 (90%)]\tLoss: 0.061815\n",
      "Train Epoch: 4 [70400/71432 (98%)]\tLoss: 0.117868\n",
      "\n",
      "Test set: Average loss: 0.1151, Accuracy: 7779/8141 (96%), Positive accuracy: 3799/4091 (93%), Negative accuracy: 3980/4050 (98%), f1 score: 0.9561\n",
      "\n",
      "Train Epoch: 5 [0/71432 (0%)]\tLoss: 0.141853\n",
      "Train Epoch: 5 [6400/71432 (9%)]\tLoss: 0.039673\n",
      "Train Epoch: 5 [12800/71432 (18%)]\tLoss: 0.056235\n",
      "Train Epoch: 5 [19200/71432 (27%)]\tLoss: 0.059102\n",
      "Train Epoch: 5 [25600/71432 (36%)]\tLoss: 0.058170\n",
      "Train Epoch: 5 [32000/71432 (45%)]\tLoss: 0.129347\n",
      "Train Epoch: 5 [38400/71432 (54%)]\tLoss: 0.112623\n",
      "Train Epoch: 5 [44800/71432 (63%)]\tLoss: 0.027189\n",
      "Train Epoch: 5 [51200/71432 (72%)]\tLoss: 0.053099\n",
      "Train Epoch: 5 [57600/71432 (81%)]\tLoss: 0.075908\n",
      "Train Epoch: 5 [64000/71432 (90%)]\tLoss: 0.149355\n",
      "Train Epoch: 5 [70400/71432 (98%)]\tLoss: 0.028692\n",
      "\n",
      "Test set: Average loss: 0.0977, Accuracy: 7816/8141 (96%), Positive accuracy: 3867/4091 (95%), Negative accuracy: 3949/4050 (98%), f1 score: 0.9602\n",
      "\n",
      "Train Epoch: 6 [0/71432 (0%)]\tLoss: 0.128027\n",
      "Train Epoch: 6 [6400/71432 (9%)]\tLoss: 0.064306\n",
      "Train Epoch: 6 [12800/71432 (18%)]\tLoss: 0.054031\n",
      "Train Epoch: 6 [19200/71432 (27%)]\tLoss: 0.085537\n",
      "Train Epoch: 6 [25600/71432 (36%)]\tLoss: 0.065953\n",
      "Train Epoch: 6 [32000/71432 (45%)]\tLoss: 0.101847\n",
      "Train Epoch: 6 [38400/71432 (54%)]\tLoss: 0.108428\n",
      "Train Epoch: 6 [44800/71432 (63%)]\tLoss: 0.088830\n",
      "Train Epoch: 6 [51200/71432 (72%)]\tLoss: 0.048968\n",
      "Train Epoch: 6 [57600/71432 (81%)]\tLoss: 0.087203\n",
      "Train Epoch: 6 [64000/71432 (90%)]\tLoss: 0.068856\n",
      "Train Epoch: 6 [70400/71432 (98%)]\tLoss: 0.059755\n",
      "\n",
      "Test set: Average loss: 0.0876, Accuracy: 7863/8141 (97%), Positive accuracy: 3896/4091 (95%), Negative accuracy: 3967/4050 (98%), f1 score: 0.9660\n",
      "\n",
      "Train Epoch: 7 [0/71432 (0%)]\tLoss: 0.040751\n",
      "Train Epoch: 7 [6400/71432 (9%)]\tLoss: 0.042035\n",
      "Train Epoch: 7 [12800/71432 (18%)]\tLoss: 0.044927\n",
      "Train Epoch: 7 [19200/71432 (27%)]\tLoss: 0.124835\n",
      "Train Epoch: 7 [25600/71432 (36%)]\tLoss: 0.054911\n",
      "Train Epoch: 7 [32000/71432 (45%)]\tLoss: 0.078016\n",
      "Train Epoch: 7 [38400/71432 (54%)]\tLoss: 0.053384\n",
      "Train Epoch: 7 [44800/71432 (63%)]\tLoss: 0.051804\n",
      "Train Epoch: 7 [51200/71432 (72%)]\tLoss: 0.080125\n",
      "Train Epoch: 7 [57600/71432 (81%)]\tLoss: 0.054541\n",
      "Train Epoch: 7 [64000/71432 (90%)]\tLoss: 0.015894\n",
      "Train Epoch: 7 [70400/71432 (98%)]\tLoss: 0.117955\n",
      "\n",
      "Test set: Average loss: 0.0851, Accuracy: 7865/8141 (97%), Positive accuracy: 3930/4091 (96%), Negative accuracy: 3935/4050 (97%), f1 score: 0.9661\n",
      "\n",
      "Train Epoch: 8 [0/71432 (0%)]\tLoss: 0.059791\n",
      "Train Epoch: 8 [6400/71432 (9%)]\tLoss: 0.095556\n",
      "Train Epoch: 8 [12800/71432 (18%)]\tLoss: 0.060632\n",
      "Train Epoch: 8 [19200/71432 (27%)]\tLoss: 0.054446\n",
      "Train Epoch: 8 [25600/71432 (36%)]\tLoss: 0.044928\n",
      "Train Epoch: 8 [32000/71432 (45%)]\tLoss: 0.122400\n",
      "Train Epoch: 8 [38400/71432 (54%)]\tLoss: 0.092461\n",
      "Train Epoch: 8 [44800/71432 (63%)]\tLoss: 0.044232\n",
      "Train Epoch: 8 [51200/71432 (72%)]\tLoss: 0.091392\n",
      "Train Epoch: 8 [57600/71432 (81%)]\tLoss: 0.100112\n",
      "Train Epoch: 8 [64000/71432 (90%)]\tLoss: 0.053660\n",
      "Train Epoch: 8 [70400/71432 (98%)]\tLoss: 0.120569\n",
      "\n",
      "Test set: Average loss: 0.0871, Accuracy: 7860/8141 (97%), Positive accuracy: 3899/4091 (95%), Negative accuracy: 3961/4050 (98%), f1 score: 0.9656\n",
      "\n",
      "Train Epoch: 9 [0/71432 (0%)]\tLoss: 0.062622\n",
      "Train Epoch: 9 [6400/71432 (9%)]\tLoss: 0.161946\n",
      "Train Epoch: 9 [12800/71432 (18%)]\tLoss: 0.011290\n",
      "Train Epoch: 9 [19200/71432 (27%)]\tLoss: 0.084209\n",
      "Train Epoch: 9 [25600/71432 (36%)]\tLoss: 0.052149\n",
      "Train Epoch: 9 [32000/71432 (45%)]\tLoss: 0.090034\n",
      "Train Epoch: 9 [38400/71432 (54%)]\tLoss: 0.062903\n",
      "Train Epoch: 9 [44800/71432 (63%)]\tLoss: 0.203646\n",
      "Train Epoch: 9 [51200/71432 (72%)]\tLoss: 0.030909\n",
      "Train Epoch: 9 [57600/71432 (81%)]\tLoss: 0.046202\n",
      "Train Epoch: 9 [64000/71432 (90%)]\tLoss: 0.121775\n",
      "Train Epoch: 9 [70400/71432 (98%)]\tLoss: 0.083456\n",
      "\n",
      "Test set: Average loss: 0.0874, Accuracy: 7862/8141 (97%), Positive accuracy: 3895/4091 (95%), Negative accuracy: 3967/4050 (98%), f1 score: 0.9658\n",
      "\n",
      "Train Epoch: 10 [0/71432 (0%)]\tLoss: 0.059940\n",
      "Train Epoch: 10 [6400/71432 (9%)]\tLoss: 0.208733\n",
      "Train Epoch: 10 [12800/71432 (18%)]\tLoss: 0.095850\n",
      "Train Epoch: 10 [19200/71432 (27%)]\tLoss: 0.073014\n",
      "Train Epoch: 10 [25600/71432 (36%)]\tLoss: 0.068109\n",
      "Train Epoch: 10 [32000/71432 (45%)]\tLoss: 0.200666\n",
      "Train Epoch: 10 [38400/71432 (54%)]\tLoss: 0.048412\n",
      "Train Epoch: 10 [44800/71432 (63%)]\tLoss: 0.045530\n",
      "Train Epoch: 10 [51200/71432 (72%)]\tLoss: 0.019100\n",
      "Train Epoch: 10 [57600/71432 (81%)]\tLoss: 0.022705\n",
      "Train Epoch: 10 [64000/71432 (90%)]\tLoss: 0.070446\n",
      "Train Epoch: 10 [70400/71432 (98%)]\tLoss: 0.067664\n",
      "\n",
      "Test set: Average loss: 0.0834, Accuracy: 7883/8141 (97%), Positive accuracy: 3912/4091 (96%), Negative accuracy: 3971/4050 (98%), f1 score: 0.9684\n",
      "\n",
      "Train Epoch: 11 [0/71432 (0%)]\tLoss: 0.153892\n",
      "Train Epoch: 11 [6400/71432 (9%)]\tLoss: 0.096314\n",
      "Train Epoch: 11 [12800/71432 (18%)]\tLoss: 0.043817\n",
      "Train Epoch: 11 [19200/71432 (27%)]\tLoss: 0.063673\n",
      "Train Epoch: 11 [25600/71432 (36%)]\tLoss: 0.115786\n",
      "Train Epoch: 11 [32000/71432 (45%)]\tLoss: 0.008975\n",
      "Train Epoch: 11 [38400/71432 (54%)]\tLoss: 0.195075\n",
      "Train Epoch: 11 [44800/71432 (63%)]\tLoss: 0.115904\n",
      "Train Epoch: 11 [51200/71432 (72%)]\tLoss: 0.119196\n",
      "Train Epoch: 11 [57600/71432 (81%)]\tLoss: 0.043590\n",
      "Train Epoch: 11 [64000/71432 (90%)]\tLoss: 0.012064\n",
      "Train Epoch: 11 [70400/71432 (98%)]\tLoss: 0.042925\n",
      "\n",
      "Test set: Average loss: 0.0845, Accuracy: 7887/8141 (97%), Positive accuracy: 3925/4091 (96%), Negative accuracy: 3962/4050 (98%), f1 score: 0.9688\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [0/71432 (0%)]\tLoss: 0.067205\n",
      "Train Epoch: 12 [6400/71432 (9%)]\tLoss: 0.158040\n",
      "Train Epoch: 12 [12800/71432 (18%)]\tLoss: 0.067922\n",
      "Train Epoch: 12 [19200/71432 (27%)]\tLoss: 0.045800\n",
      "Train Epoch: 12 [25600/71432 (36%)]\tLoss: 0.114880\n",
      "Train Epoch: 12 [32000/71432 (45%)]\tLoss: 0.177880\n",
      "Train Epoch: 12 [38400/71432 (54%)]\tLoss: 0.051337\n",
      "Train Epoch: 12 [44800/71432 (63%)]\tLoss: 0.042167\n",
      "Train Epoch: 12 [51200/71432 (72%)]\tLoss: 0.074252\n",
      "Train Epoch: 12 [57600/71432 (81%)]\tLoss: 0.121884\n",
      "Train Epoch: 12 [64000/71432 (90%)]\tLoss: 0.085744\n",
      "Train Epoch: 12 [70400/71432 (98%)]\tLoss: 0.107819\n",
      "\n",
      "Test set: Average loss: 0.0838, Accuracy: 7880/8141 (97%), Positive accuracy: 3920/4091 (96%), Negative accuracy: 3960/4050 (98%), f1 score: 0.9680\n",
      "\n",
      "Train Epoch: 13 [0/71432 (0%)]\tLoss: 0.098931\n",
      "Train Epoch: 13 [6400/71432 (9%)]\tLoss: 0.127376\n",
      "Train Epoch: 13 [12800/71432 (18%)]\tLoss: 0.050912\n",
      "Train Epoch: 13 [19200/71432 (27%)]\tLoss: 0.069185\n",
      "Train Epoch: 13 [25600/71432 (36%)]\tLoss: 0.030214\n",
      "Train Epoch: 13 [32000/71432 (45%)]\tLoss: 0.067661\n",
      "Train Epoch: 13 [38400/71432 (54%)]\tLoss: 0.017610\n",
      "Train Epoch: 13 [44800/71432 (63%)]\tLoss: 0.067203\n",
      "Train Epoch: 13 [51200/71432 (72%)]\tLoss: 0.059360\n",
      "Train Epoch: 13 [57600/71432 (81%)]\tLoss: 0.058473\n",
      "Train Epoch: 13 [64000/71432 (90%)]\tLoss: 0.077697\n",
      "Train Epoch: 13 [70400/71432 (98%)]\tLoss: 0.037194\n",
      "\n",
      "Test set: Average loss: 0.0839, Accuracy: 7881/8141 (97%), Positive accuracy: 3909/4091 (96%), Negative accuracy: 3972/4050 (98%), f1 score: 0.9682\n",
      "\n",
      "Train Epoch: 14 [0/71432 (0%)]\tLoss: 0.039094\n",
      "Train Epoch: 14 [6400/71432 (9%)]\tLoss: 0.058109\n",
      "Train Epoch: 14 [12800/71432 (18%)]\tLoss: 0.014546\n",
      "Train Epoch: 14 [19200/71432 (27%)]\tLoss: 0.134238\n",
      "Train Epoch: 14 [25600/71432 (36%)]\tLoss: 0.196273\n",
      "Train Epoch: 14 [32000/71432 (45%)]\tLoss: 0.082800\n",
      "Train Epoch: 14 [38400/71432 (54%)]\tLoss: 0.057058\n",
      "Train Epoch: 14 [44800/71432 (63%)]\tLoss: 0.049863\n",
      "Train Epoch: 14 [51200/71432 (72%)]\tLoss: 0.059140\n",
      "Train Epoch: 14 [57600/71432 (81%)]\tLoss: 0.106783\n",
      "Train Epoch: 14 [64000/71432 (90%)]\tLoss: 0.064519\n",
      "Train Epoch: 14 [70400/71432 (98%)]\tLoss: 0.029841\n",
      "\n",
      "Test set: Average loss: 0.0834, Accuracy: 7879/8141 (97%), Positive accuracy: 3915/4091 (96%), Negative accuracy: 3964/4050 (98%), f1 score: 0.9679\n",
      "\n",
      "Train Epoch: 15 [0/71432 (0%)]\tLoss: 0.049567\n",
      "Train Epoch: 15 [6400/71432 (9%)]\tLoss: 0.146748\n",
      "Train Epoch: 15 [12800/71432 (18%)]\tLoss: 0.016687\n",
      "Train Epoch: 15 [19200/71432 (27%)]\tLoss: 0.027481\n",
      "Train Epoch: 15 [25600/71432 (36%)]\tLoss: 0.100509\n",
      "Train Epoch: 15 [32000/71432 (45%)]\tLoss: 0.060045\n",
      "Train Epoch: 15 [38400/71432 (54%)]\tLoss: 0.044702\n",
      "Train Epoch: 15 [44800/71432 (63%)]\tLoss: 0.135751\n",
      "Train Epoch: 15 [51200/71432 (72%)]\tLoss: 0.158854\n",
      "Train Epoch: 15 [57600/71432 (81%)]\tLoss: 0.095332\n",
      "Train Epoch: 15 [64000/71432 (90%)]\tLoss: 0.086078\n",
      "Train Epoch: 15 [70400/71432 (98%)]\tLoss: 0.098147\n",
      "\n",
      "Test set: Average loss: 0.0838, Accuracy: 7879/8141 (97%), Positive accuracy: 3916/4091 (96%), Negative accuracy: 3963/4050 (98%), f1 score: 0.9679\n",
      "\n",
      "Train Epoch: 16 [0/71432 (0%)]\tLoss: 0.015018\n",
      "Train Epoch: 16 [6400/71432 (9%)]\tLoss: 0.049293\n",
      "Train Epoch: 16 [12800/71432 (18%)]\tLoss: 0.022479\n",
      "Train Epoch: 16 [19200/71432 (27%)]\tLoss: 0.043834\n",
      "Train Epoch: 16 [25600/71432 (36%)]\tLoss: 0.130988\n",
      "Train Epoch: 16 [32000/71432 (45%)]\tLoss: 0.090611\n",
      "Train Epoch: 16 [38400/71432 (54%)]\tLoss: 0.036616\n",
      "Train Epoch: 16 [44800/71432 (63%)]\tLoss: 0.021027\n",
      "Train Epoch: 16 [51200/71432 (72%)]\tLoss: 0.090138\n",
      "Train Epoch: 16 [57600/71432 (81%)]\tLoss: 0.147830\n",
      "Train Epoch: 16 [64000/71432 (90%)]\tLoss: 0.141443\n",
      "Train Epoch: 16 [70400/71432 (98%)]\tLoss: 0.098716\n",
      "\n",
      "Test set: Average loss: 0.0831, Accuracy: 7882/8141 (97%), Positive accuracy: 3912/4091 (96%), Negative accuracy: 3970/4050 (98%), f1 score: 0.9683\n",
      "\n",
      "Train Epoch: 17 [0/71432 (0%)]\tLoss: 0.071339\n",
      "Train Epoch: 17 [6400/71432 (9%)]\tLoss: 0.062115\n",
      "Train Epoch: 17 [12800/71432 (18%)]\tLoss: 0.025618\n",
      "Train Epoch: 17 [19200/71432 (27%)]\tLoss: 0.030233\n",
      "Train Epoch: 17 [25600/71432 (36%)]\tLoss: 0.073048\n",
      "Train Epoch: 17 [32000/71432 (45%)]\tLoss: 0.083643\n",
      "Train Epoch: 17 [38400/71432 (54%)]\tLoss: 0.108850\n",
      "Train Epoch: 17 [44800/71432 (63%)]\tLoss: 0.047220\n",
      "Train Epoch: 17 [51200/71432 (72%)]\tLoss: 0.130635\n",
      "Train Epoch: 17 [57600/71432 (81%)]\tLoss: 0.052141\n",
      "Train Epoch: 17 [64000/71432 (90%)]\tLoss: 0.107931\n",
      "Train Epoch: 17 [70400/71432 (98%)]\tLoss: 0.072241\n",
      "\n",
      "Test set: Average loss: 0.0832, Accuracy: 7879/8141 (97%), Positive accuracy: 3918/4091 (96%), Negative accuracy: 3961/4050 (98%), f1 score: 0.9679\n",
      "\n",
      "Train Epoch: 18 [0/71432 (0%)]\tLoss: 0.062748\n",
      "Train Epoch: 18 [6400/71432 (9%)]\tLoss: 0.032877\n",
      "Train Epoch: 18 [12800/71432 (18%)]\tLoss: 0.064217\n",
      "Train Epoch: 18 [19200/71432 (27%)]\tLoss: 0.099729\n",
      "Train Epoch: 18 [25600/71432 (36%)]\tLoss: 0.025871\n",
      "Train Epoch: 18 [32000/71432 (45%)]\tLoss: 0.059450\n",
      "Train Epoch: 18 [38400/71432 (54%)]\tLoss: 0.103248\n",
      "Train Epoch: 18 [44800/71432 (63%)]\tLoss: 0.075347\n",
      "Train Epoch: 18 [51200/71432 (72%)]\tLoss: 0.097110\n",
      "Train Epoch: 18 [57600/71432 (81%)]\tLoss: 0.138375\n",
      "Train Epoch: 18 [64000/71432 (90%)]\tLoss: 0.279850\n",
      "Train Epoch: 18 [70400/71432 (98%)]\tLoss: 0.133631\n",
      "\n",
      "Test set: Average loss: 0.0838, Accuracy: 7881/8141 (97%), Positive accuracy: 3913/4091 (96%), Negative accuracy: 3968/4050 (98%), f1 score: 0.9681\n",
      "\n",
      "Train Epoch: 19 [0/71432 (0%)]\tLoss: 0.043196\n",
      "Train Epoch: 19 [6400/71432 (9%)]\tLoss: 0.018599\n",
      "Train Epoch: 19 [12800/71432 (18%)]\tLoss: 0.107174\n",
      "Train Epoch: 19 [19200/71432 (27%)]\tLoss: 0.097892\n",
      "Train Epoch: 19 [25600/71432 (36%)]\tLoss: 0.192776\n",
      "Train Epoch: 19 [32000/71432 (45%)]\tLoss: 0.052790\n",
      "Train Epoch: 19 [38400/71432 (54%)]\tLoss: 0.028024\n",
      "Train Epoch: 19 [44800/71432 (63%)]\tLoss: 0.037966\n",
      "Train Epoch: 19 [51200/71432 (72%)]\tLoss: 0.056427\n",
      "Train Epoch: 19 [57600/71432 (81%)]\tLoss: 0.086358\n",
      "Train Epoch: 19 [64000/71432 (90%)]\tLoss: 0.076566\n",
      "Train Epoch: 19 [70400/71432 (98%)]\tLoss: 0.068317\n",
      "\n",
      "Test set: Average loss: 0.0836, Accuracy: 7885/8141 (97%), Positive accuracy: 3913/4091 (96%), Negative accuracy: 3972/4050 (98%), f1 score: 0.9686\n",
      "\n",
      "Train Epoch: 20 [0/71432 (0%)]\tLoss: 0.035243\n",
      "Train Epoch: 20 [6400/71432 (9%)]\tLoss: 0.076107\n",
      "Train Epoch: 20 [12800/71432 (18%)]\tLoss: 0.128075\n",
      "Train Epoch: 20 [19200/71432 (27%)]\tLoss: 0.078648\n",
      "Train Epoch: 20 [25600/71432 (36%)]\tLoss: 0.175563\n",
      "Train Epoch: 20 [32000/71432 (45%)]\tLoss: 0.052084\n",
      "Train Epoch: 20 [38400/71432 (54%)]\tLoss: 0.069332\n",
      "Train Epoch: 20 [44800/71432 (63%)]\tLoss: 0.074219\n",
      "Train Epoch: 20 [51200/71432 (72%)]\tLoss: 0.057262\n",
      "Train Epoch: 20 [57600/71432 (81%)]\tLoss: 0.047674\n",
      "Train Epoch: 20 [64000/71432 (90%)]\tLoss: 0.049745\n",
      "Train Epoch: 20 [70400/71432 (98%)]\tLoss: 0.083092\n",
      "\n",
      "Test set: Average loss: 0.0835, Accuracy: 7881/8141 (97%), Positive accuracy: 3914/4091 (96%), Negative accuracy: 3967/4050 (98%), f1 score: 0.9681\n",
      "\n",
      "[7689, 7686, 7771, 7779, 7816, 7863, 7865, 7860, 7862, 7883, 7887, 7880, 7881, 7879, 7879, 7882, 7879, 7881, 7885, 7881]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CNNTrainedGabor(nn.Module):\n",
    "    def __init__(self, pretrain_gabor_model):\n",
    "        super(CNNTrainedGabor, self).__init__()\n",
    "        self.pretrain_gabor_model = pretrain_gabor_model\n",
    "        self.conv1 = nn.Conv2d(1, 4, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 5, 1)\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "#         self.dropout2 = nn.Dropout2d(0.5)\n",
    "        # self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc1 = nn.Linear(225, 128)\n",
    "#         self.fc2 = nn.Linear(128, 2)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 16)\n",
    "        self.fc4 = nn.Linear(16, 6)\n",
    "        self.fc5 = nn.Linear(6, 2)\n",
    "#         self.fc_gabor1 = nn.Linear(96, 36)\n",
    "#         self.fc_gabor2 = nn.Linear(36, 6)\n",
    "\n",
    "        \n",
    "   \n",
    "    def forward(self, x):\n",
    "        pc, x_cos, x_sin = self.pc(x)\n",
    "        pc = pc.view(x.shape[0],1)\n",
    "#         print(x_cos.shape,x_sin.shape)\n",
    "        cos, sin = self.mean_std(x_cos,x_sin)\n",
    "#         print(cos.shape,sin.shape)\n",
    "        comb = torch.cat((cos,sin,pc),1)\n",
    "#         x_comb = torch.cat((x_cos, x_sin), 3)\n",
    "#         x_comb_fc = self.fc_gabor1(x_comb)\n",
    "#         x_comb_fc = self.fc_gabor2(x_comb_fc)\n",
    "#         x_comb_fc = torch.squeeze(x_comb_fc)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "# #         x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        comb_cnn = torch.cat((comb,x),1)\n",
    "        x = self.fc1(comb_cnn)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x)\n",
    "\n",
    "#         x = self.dropout2(x)\n",
    "#         print(pc.shape, x.shape)\n",
    "#         print(x_comb_fc.shape, x.shape)\n",
    "#         x = torch.cat((x_comb_fc,pc, x), 1)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def pc(self, x):\n",
    "        filter_cos = self.pretrain_gabor_model['filter_cos']\n",
    "        filter_sin = self.pretrain_gabor_model['filter_sin']\n",
    "        bias1 = self.pretrain_gabor_model['bias1']\n",
    "        bias2 = self.pretrain_gabor_model['bias2']\n",
    "        weights = self.pretrain_gabor_model['weights']\n",
    "        w = self.pretrain_gabor_model['w']\n",
    "        b = self.pretrain_gabor_model['b']\n",
    "        \n",
    "        x_cos = F.conv2d(x, filter_cos, bias=bias1)\n",
    "        x_sin = F.conv2d(x, filter_sin, bias=bias2)\n",
    "        x_comb = torch.cat((x_cos, x_sin), 2)\n",
    "\n",
    "        x_cos = x_cos.view(len(x), 1, 1, 48)\n",
    "        x_sin = x_sin.view(len(x), 1, 1, 48)\n",
    "        weighted_cos = (torch.matmul(x_cos, weights)).view(len(x), 1)\n",
    "        weighted_sin = (torch.matmul(x_sin, weights)).view(len(x), 1)\n",
    "\n",
    "        numerator = torch.norm(torch.cat([weighted_cos, weighted_sin], 1), dim=1)\n",
    "    #         print(\"numerator\", numerator.size())\n",
    "        x_comb_norm = torch.norm(x_comb, dim=2)\n",
    "        x_comb_norm = x_comb_norm.view(len(x), 1, 48)\n",
    "    #         print(\"x_comb_norm\", x_comb_norm.size())\n",
    "        denominator = torch.matmul(x_comb_norm, torch.abs(weights))\n",
    "        denominator = denominator.view(len(x))\n",
    "    #         print(\"size:\", numerator.size(), denominator.size())\n",
    "        pc = numerator / denominator                \n",
    "        return torch.sigmoid(w * pc + b),x_cos, x_sin\n",
    "    \n",
    "    def mean_std(self, x_cos, x_sin):\n",
    "        cos_mean = torch.FloatTensor([])\n",
    "        cos_std = torch.FloatTensor([])\n",
    "        sin_mean = torch.FloatTensor([])\n",
    "        sin_std = torch.FloatTensor([])\n",
    "        for i in range(6):\n",
    "            cos_curr = torch.FloatTensor([])\n",
    "            sin_curr = torch.FloatTensor([])\n",
    "            for j in range(i,48,6):\n",
    "                cos_curr = torch.cat((cos_curr,x_cos[:,:,:,j]),1)\n",
    "                sin_curr = torch.cat((sin_curr,x_sin[:,:,:,j]),1)\n",
    "            cos_mean = torch.cat((cos_mean,torch.mean(cos_curr,1)),1)\n",
    "            cos_std = torch.cat((cos_std,torch.std(cos_curr,1)),1)\n",
    "            sin_mean = torch.cat((sin_mean,torch.mean(sin_curr,1)),1)\n",
    "            sin_std = torch.cat((sin_std,torch.std(sin_curr,1)),1)\n",
    "#         print(cos_mean.shape,cos_std.shape,\"mean_std\")\n",
    "        cos = torch.cat((cos_mean,cos_std),1)\n",
    "        sin = torch.cat((sin_mean,sin_std),1)\n",
    "        return cos, sin\n",
    "            \n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "    train_loss = sum(loss_list)/len(loss_list)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "# def test(model, device, test_loader):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             output = model(data)\n",
    "#             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "#             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "#             correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "\n",
    "#     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         test_loss, correct, len(test_loader.dataset),\n",
    "#         100. * correct / len(test_loader.dataset)))\n",
    "def test(model, device, test_loader,epoch,train_loss):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    result= [[0,0], [0,0]] \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            cmat = confusion_matrix(target.view_as(pred), pred, labels=[0, 1]) \n",
    "            result = [[result[i][j] + cmat[i][j]  for j in range(len(result[0]))] for i in range(len(result))] \n",
    "            \n",
    "            # Store wrongly predicted images\n",
    "            if epoch == 11:\n",
    "                wrong_idx = (pred != target.view_as(pred)).nonzero()[:, 0]\n",
    "                wrong_samples = data[wrong_idx]\n",
    "                wrong_preds = pred[wrong_idx]\n",
    "                actual_preds = target.view_as(pred)[wrong_idx]\n",
    "                for i in range(len(wrong_idx)):\n",
    "                    sample = wrong_samples[i]\n",
    "                    wrong_pred = wrong_preds[i]\n",
    "                    actual_pred = actual_preds[i]\n",
    "                    sample = sample * 255.\n",
    "                    sample = sample.byte()\n",
    "                    img = TF.to_pil_image(sample)\n",
    "                    img.save('./wrong-final/batch{}_idx{}_actual{}.png'.format(\n",
    "                        batch_idx,wrong_idx[i], actual_pred.item()))\n",
    "                    num = batch_idx * 64 + wrong_idx[i]\n",
    "                    img_ori = origin_dataset[num][0].numpy()\n",
    "                    plt.imsave('./wrong-final/batch{}_idx{}_actual{}_ori.png'.format(\n",
    "                        batch_idx,wrong_idx[i], actual_pred.item()), img_ori[0], cmap = 'gray')\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    precision = result[1][1]/(result[1][1]+result[0][1])\n",
    "    recall = result[0][0]/(result[0][0]+result[1][0])\n",
    "    f1score = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Positive accuracy: {}/{} ({:.0f}%), Negative accuracy: {}/{} ({:.0f}%), f1 score: {:.4f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset), \n",
    "        result[1][1],result[1][1]+result[1][0],100. * result[1][1]/(result[1][1]+result[1][0]),\n",
    "        result[0][0],result[0][0]+result[0][1],100. * result[0][0]/(result[0][0]+result[0][1]),f1score))\n",
    "    return test_loss, correct\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=100, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'batch_size': args.batch_size}\n",
    "    if use_cuda:\n",
    "        kwargs.update({'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True},\n",
    "                     )\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    model = CNNTrainedGabor(pretrain_gabor_model).to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_accuracy_list = []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_result = test(model, device, test_loader,epoch,train_loss)\n",
    "        test_loss_list.append(test_result[0])\n",
    "        test_accuracy_list.append(test_result[1])\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "    plt.plot(train_loss_list, color='blue',label='train loss')  \n",
    "    plt.plot(test_loss_list, color='red',label='test loss')  \n",
    "    plt.legend()\n",
    "    print(test_accuracy_list)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 9 is out of bounds for axis 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8465256dd25e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./wrong-gabor/1_true0__pc0.3958_ori.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mchange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchange\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 9 is out of bounds for axis 0 with size 4"
     ]
    }
   ],
   "source": [
    "img = plt.imread(\"./wrong-gabor/1_true0__pc0.3958_ori.png\")\n",
    "change = np.transpose(img)\n",
    "plt.plot(change[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(\"./wrong-gabor\"):\n",
    "    try:\n",
    "        img = plt.imread(os.path.join(\"./wrong-gabor\",filename))\n",
    "        change = np.transpose(img)\n",
    "        num = filename.split(\"_\")\n",
    "        plt.plot(change[9])\n",
    "        plt.savefig(\"./wrong-gabor/{}_y.png\".format(num[0]))\n",
    "        plt.close() \n",
    "        plt.plot(img[9])\n",
    "        plt.savefig(\"./wrong-gabor/{}_x.png\".format(num[0]))\n",
    "        plt.close() \n",
    "    except:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x384 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = pretrain_gabor_model['weights']\n",
    "weights = torch.squeeze(weights)\n",
    "big = []\n",
    "for i in range(8):\n",
    "    curr = []\n",
    "    for j in range(6):\n",
    "        curr.append(np.ones((10,10))*weights[i*6+j].item())\n",
    "    big.append(curr)\n",
    "output = np.block(big)\n",
    "plt.matshow(output);\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
