{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim  \n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import argparse\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from skimage.util import random_noise\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "import kornia.augmentation.functional as FF\n",
    "import statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./\"\n",
    "def default_loader(path):\n",
    "    return Image.open(path).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self,root, datatxt, transform=None, target_transform=None,loader=default_loader):\n",
    "        super(MyDataset,self).__init__()\n",
    "        fh = open(root + datatxt, 'r') \n",
    "        imgs = []     \n",
    "        data = []\n",
    "        label = []\n",
    "        for line in fh:                \n",
    "            line = line.rstrip()       \n",
    "            data.append(line)\n",
    "        for line in range(len(data)-1):\n",
    "            words = data[line].split()  \n",
    "            imgs.append((words[0])) \n",
    "            label.append(int(words[1]))\n",
    "            \n",
    "        \n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.label = torch.LongTensor(label)\n",
    " \n",
    "    # def __getitem__(self, index):    \n",
    "    def __getitem__(self, idx):    \n",
    "        image = Image.open(str(self.imgs[idx]))\n",
    "        # image = image.convert('RGB')\n",
    "        image = image.convert('L')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        label = self.label[idx]\n",
    "        return image, label\n",
    "    def __len__(self): \n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_trainData: 71432\n",
      "num_of_trainNewData: 89827\n",
      "num_of_testData: 8141\n",
      "num_of_testNewData: 10299\n",
      "num_of_originData: 8141\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(root='./',datatxt='train.txt', transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#         AddGaussianNoise(0., 0.05)\n",
    "        ]))\n",
    "trainNew_dataset = MyDataset(root='./',datatxt='train-new.txt', transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#         AddGaussianNoise(0., 0.05)\n",
    "        ]))\n",
    "test_dataset = MyDataset(root='./',datatxt='test.txt', transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#         AddGaussianNoise(0., 0.05)\n",
    "        ]))\n",
    "testNew_dataset = MyDataset(root='./',datatxt='test-new.txt', transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#         AddGaussianNoise(0., 0.05)\n",
    "        ]))\n",
    "origin_dataset = MyDataset(root='./',datatxt='origin.txt', transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#         AddGaussianNoise(0., 0.05)\n",
    "        ]))\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True,num_workers=1)\n",
    "trainNew_loader = DataLoader(dataset=trainNew_dataset, batch_size=64, shuffle=True,num_workers=1)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False,num_workers=1)\n",
    "testNew_loader = DataLoader(dataset=testNew_dataset, batch_size=64, shuffle=False,num_workers=1)\n",
    "origin_loader = DataLoader(dataset=origin_dataset, batch_size=64, shuffle=False,num_workers=1)\n",
    "print('num_of_trainData:', len(train_dataset))\n",
    "print('num_of_trainNewData:', len(trainNew_dataset))\n",
    "print('num_of_testData:', len(test_dataset))\n",
    "print('num_of_testNewData:', len(testNew_dataset))\n",
    "print('num_of_originData:', len(origin_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label： tensor(0) shape: (1, 19, 19)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "idx = 11\n",
    "img = test_dataset[idx][0].numpy()\n",
    "plt.imshow(img[0], cmap = 'gray')\n",
    "plt.imsave('test.png', img[0], cmap = 'gray')\n",
    "# figure, b = plt.subplots()\n",
    "# figure.set_size_inches(0.19, 0.19)\n",
    "plt.axis('off') \n",
    "print('label：',train_dataset[idx][1], 'shape:', img.shape)\n",
    "print(type(img[0]))\n",
    "matplotlib.image.imsave('name.png',img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-998d1f4958c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnegative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m71432\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mnegative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5f69636893af>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# def __getitem__(self, index):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m# image = image.convert('RGB')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2850\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2852\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "positive = 0\n",
    "negative = 0\n",
    "for idx in range(1,71432):\n",
    "    if train_dataset[idx][1].item() == 0:\n",
    "        negative = negative+1\n",
    "    else:\n",
    "        positive = positive+1\n",
    "print('negative: ', negative)\n",
    "print('positive: ', positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaborConvPC(nn.Module):\n",
    "#     def __init__(self, kernel_size, in_channels, num_orientations, num_scales):\n",
    "#         super(GaborConvPC, self).__init__()\n",
    "#         self.sigma1, self.theta1, self.Lambda1, self.psi1, self.gamma1, self.bias1, self.weights1, self.w, self.b = self.generate_parameters(num_orientations*num_scales, in_channels)\n",
    "#         self.sigma2, self.theta2, self.Lambda2, self.psi2, self.gamma2, self.bias2, self.weights2, self.w, self.b = self.generate_parameters(num_orientations*num_scales, in_channels)\n",
    "#         # self.filter1 = self.whole_filter(in_channels, channel1, kernel_size, self.sigma1, self.theta1, self.Lambda1, self.psi1, self.gamma1)\n",
    "#         self.filter_cos = self.whole_filter(in_channels, num_orientations, num_scales, kernel_size, self.sigma1, self.theta1, self.Lambda1, self.psi1, self.gamma1, True)\n",
    "#         self.filter_sin = self.whole_filter(in_channels, num_orientations, num_scales, kernel_size, self.sigma1, self.theta1, self.Lambda1, self.psi1, self.gamma1, False)       \n",
    "        \n",
    "#         self.fc1 = nn.Linear(1*1*48, 24)\n",
    "#         self.fc2 = nn.Linear(24, 2)\n",
    "\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x_cos = F.conv2d(x, self.filter_cos, bias=self.bias1)\n",
    "#         x_sin = F.conv2d(x, self.filter_sin, bias=self.bias2)\n",
    "#         x_comb = torch.cat((x_cos, x_sin), 2)\n",
    "\n",
    "#         x_cos = x_cos.view(len(x), 1, 1, 48)\n",
    "#         x_sin = x_sin.view(len(x), 1, 1, 48)\n",
    "#         weighted_cos = (torch.matmul(x_cos, self.weights1)).view(len(x), 1)\n",
    "#         weighted_sin = (torch.matmul(x_sin, self.weights1)).view(len(x), 1)\n",
    "\n",
    "#         numerator = torch.norm(torch.cat([weighted_cos, weighted_sin], 1), dim=1)\n",
    "# #         print(\"numerator\", numerator.size())\n",
    "#         x_comb_norm = torch.norm(x_comb, dim=2)\n",
    "#         x_comb_norm = x_comb_norm.view(len(x), 1, 48)\n",
    "# #         print(\"x_comb_norm\", x_comb_norm.size())\n",
    "#         denominator = torch.matmul(x_comb_norm, torch.abs(self.weights1))\n",
    "#         denominator = denominator.view(len(x))\n",
    "\n",
    "#         pc = numerator / denominator                \n",
    "#         return torch.sigmoid(self.w * pc + self.b)\n",
    "\n",
    "\n",
    "#     def generate_parameters(self, dim_out, dim_in):\n",
    "#         sigma = nn.Parameter(torch.randn(1, 1))\n",
    "#         theta = nn.Parameter(torch.randn(1, 1))\n",
    "#         Lambda = nn.Parameter(torch.randn(1, 1))\n",
    "#         psi = nn.Parameter(torch.randn(1, 1))\n",
    "#         gamma = nn.Parameter(torch.randn(1, 1))\n",
    "#         bias = nn.Parameter(torch.randn(dim_out))\n",
    "#         weights = nn.Parameter(torch.randn(1, 48, 1))\n",
    "#         w = nn.Parameter(torch.randn(1, 1))\n",
    "#         b = nn.Parameter(torch.randn(1, 1))\n",
    "#         return sigma, theta, Lambda, psi, gamma, bias, weights, w, b\n",
    "    def __init__(self, kernel_size, in_channels, num_orientations, num_scales):\n",
    "        super(GaborConvPC, self).__init__()\n",
    "        self.sigma, self.theta, self.Lambda, self.psi, self.gamma, self.bias1, self.bias2, self.weights, self.w, self.b = self.generate_parameters(num_orientations*num_scales, in_channels)\n",
    "        self.filter_cos = self.whole_filter(in_channels, num_orientations, num_scales, kernel_size, self.sigma, self.theta, self.Lambda, self.psi, self.gamma, True)\n",
    "        self.filter_sin = self.whole_filter(in_channels, num_orientations, num_scales, kernel_size, self.sigma, self.theta, self.Lambda, self.psi, self.gamma, False)\n",
    "\n",
    "        self.fc1 = nn.Linear(1*1*48, 24)\n",
    "        self.fc2 = nn.Linear(24, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cos = F.conv2d(x, self.filter_cos, bias=self.bias1)\n",
    "        x_sin = F.conv2d(x, self.filter_sin, bias=self.bias2)\n",
    "        x_comb = torch.cat((x_cos, x_sin), 2)\n",
    "        x_cos = x_cos.view(len(x), 1, 1, 48)\n",
    "        x_sin = x_sin.view(len(x), 1, 1, 48)\n",
    "        weighted_cos = (torch.matmul(x_cos, self.weights)).view(len(x), 1)\n",
    "        weighted_sin = (torch.matmul(x_sin, self.weights)).view(len(x), 1)\n",
    "\n",
    "        numerator = torch.norm(torch.cat([weighted_cos, weighted_sin], 1), dim=1)\n",
    "        x_comb_norm = torch.norm(x_comb, dim=2)\n",
    "        x_comb_norm = x_comb_norm.view(len(x), 1, 48)\n",
    "        denominator = torch.matmul(x_comb_norm, torch.abs(self.weights))\n",
    "        denominator = denominator.view(len(x))\n",
    "        pc = numerator / denominator                \n",
    "        return torch.sigmoid(self.w * pc + self.b),pc\n",
    "\n",
    "\n",
    "    def generate_parameters(self, dim_out, dim_in):\n",
    "        sigma = nn.Parameter(torch.randn(1, 1))\n",
    "        theta = nn.Parameter(torch.randn(1, 1))\n",
    "        Lambda = nn.Parameter(torch.randn(1, 1))\n",
    "        psi = nn.Parameter(torch.randn(1, 1))\n",
    "        gamma = nn.Parameter(torch.randn(1, 1))\n",
    "        bias1 = nn.Parameter(torch.randn(dim_out))\n",
    "        bias2 = nn.Parameter(torch.randn(dim_out))\n",
    "        weights = nn.Parameter(torch.randn(1, 48, 1))\n",
    "        w = nn.Parameter(torch.randn(1, 1))\n",
    "        b = nn.Parameter(torch.randn(1, 1))\n",
    "        return sigma, theta, Lambda, psi, gamma, bias1, bias2, weights, w, b\n",
    "\n",
    "\n",
    "    def whole_filter(self, in_channels, num_orientations, num_scales, kernel_size, sigma, theta, Lambda, psi, gamma, cos):\n",
    "        result = torch.zeros(num_orientations*num_scales, in_channels, kernel_size, kernel_size) # \\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW\n",
    "        for i in range(num_orientations):\n",
    "            for j in range(num_scales):\n",
    "                result[i*num_scales + j] = self.one_filter(in_channels, kernel_size, sigma[0]*(2.1**j), theta[0]+i*np.pi/num_orientations, Lambda[0], psi[0], gamma[0], cos)\n",
    "        return nn.Parameter(result)\n",
    "\n",
    "\n",
    "    def one_filter(self, in_channels, kernel_size, sigma, theta, Lambda, psi, gamma, cos):\n",
    "        result = torch.zeros(in_channels, kernel_size, kernel_size)\n",
    "        for i in range(in_channels):\n",
    "            result[i] = self.gabor_fn(sigma, theta, Lambda, psi, gamma, kernel_size, cos)\n",
    "        return nn.Parameter(result)\n",
    "\n",
    "\n",
    "    def gabor_fn(self, sigma, theta, Lambda, psi, gamma, kernel_size, cos):\n",
    "        sigma_x = sigma\n",
    "        # sigma_y = float(sigma) / gamma\n",
    "        sigma_y = sigma / gamma\n",
    "\n",
    "        # Bounding box\n",
    "        half_size = (kernel_size - 1) // 2\n",
    "        ymin, xmin = -half_size, -half_size\n",
    "        ymax, xmax = half_size, half_size\n",
    "    #     (y, x) = np.meshgrid(np.arange(ymin, ymax + 1), np.arange(xmin, xmax + 1))\n",
    "        y, x = torch.meshgrid([torch.arange(ymin, ymax+1), torch.arange(xmin,xmax+1)])\n",
    "\n",
    "        if cos:\n",
    "            gb = torch.exp(-.5 * (x**2 / sigma_x**2 + y**2 / sigma_y**2)) * torch.cos(2 * np.pi / Lambda * x + psi)\n",
    "        else:\n",
    "            gb = torch.exp(-.5 * (x**2 / sigma_x**2 + y**2 / sigma_y**2)) * torch.sin(2 * np.pi / Lambda * x + psi)\n",
    "\n",
    "        # Rotation\n",
    "        degrees = theta * 180 / np.pi\n",
    "        gb = FF.apply_rotation(gb, {'degrees': torch.tensor(degrees)}, {'interpolation': torch.tensor([1]), 'align_corners': torch.tensor(True)})\n",
    "        gb = gb.squeeze()\n",
    "        return gb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> torch.Size([48])\n",
      "<class 'torch.Tensor'> torch.Size([48])\n",
      "<class 'torch.Tensor'> torch.Size([1, 48, 1])\n",
      "<class 'torch.Tensor'> torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> torch.Size([48, 1, 19, 19])\n",
      "<class 'torch.Tensor'> torch.Size([48, 1, 19, 19])\n",
      "<class 'torch.Tensor'> torch.Size([24, 48])\n",
      "<class 'torch.Tensor'> torch.Size([24])\n",
      "<class 'torch.Tensor'> torch.Size([2, 24])\n",
      "<class 'torch.Tensor'> torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/71432 (0%)]\tLoss: 48.603153\n",
      "Train Epoch: 1 [6400/71432 (9%)]\tLoss: 35.009838\n",
      "Train Epoch: 1 [12800/71432 (18%)]\tLoss: 20.915850\n",
      "Train Epoch: 1 [19200/71432 (27%)]\tLoss: 18.323053\n",
      "Train Epoch: 1 [25600/71432 (36%)]\tLoss: 14.125262\n",
      "Train Epoch: 1 [32000/71432 (45%)]\tLoss: 12.656539\n",
      "Train Epoch: 1 [38400/71432 (54%)]\tLoss: 17.845667\n",
      "Train Epoch: 1 [44800/71432 (63%)]\tLoss: 16.184490\n",
      "Train Epoch: 1 [51200/71432 (72%)]\tLoss: 18.283365\n",
      "Train Epoch: 1 [57600/71432 (81%)]\tLoss: 15.357291\n",
      "Train Epoch: 1 [64000/71432 (90%)]\tLoss: 9.544128\n",
      "Train Epoch: 1 [70400/71432 (98%)]\tLoss: 12.535745\n",
      "\n",
      "Test set: Average loss: 0.2239, Accuracy: 7435/8141 (91%), Positive accuracy: 3558/4091 (87%), Negative accuracy: 3877/4050 (96%), train loss: 0.3036\n",
      "\n",
      "Train Epoch: 2 [0/71432 (0%)]\tLoss: 14.142234\n",
      "Train Epoch: 2 [6400/71432 (9%)]\tLoss: 17.528366\n",
      "Train Epoch: 2 [12800/71432 (18%)]\tLoss: 19.495396\n",
      "Train Epoch: 2 [19200/71432 (27%)]\tLoss: 20.777678\n",
      "Train Epoch: 2 [25600/71432 (36%)]\tLoss: 9.979182\n",
      "Train Epoch: 2 [32000/71432 (45%)]\tLoss: 11.304832\n",
      "Train Epoch: 2 [38400/71432 (54%)]\tLoss: 12.378254\n",
      "Train Epoch: 2 [44800/71432 (63%)]\tLoss: 20.189289\n",
      "Train Epoch: 2 [51200/71432 (72%)]\tLoss: 8.403931\n",
      "Train Epoch: 2 [57600/71432 (81%)]\tLoss: 14.401905\n",
      "Train Epoch: 2 [64000/71432 (90%)]\tLoss: 10.057652\n",
      "Train Epoch: 2 [70400/71432 (98%)]\tLoss: 9.452750\n",
      "\n",
      "Test set: Average loss: 0.2018, Accuracy: 7527/8141 (92%), Positive accuracy: 3703/4091 (91%), Negative accuracy: 3824/4050 (94%), train loss: 0.2141\n",
      "\n",
      "Train Epoch: 3 [0/71432 (0%)]\tLoss: 17.356974\n",
      "Train Epoch: 3 [6400/71432 (9%)]\tLoss: 26.465014\n",
      "Train Epoch: 3 [12800/71432 (18%)]\tLoss: 18.550686\n",
      "Train Epoch: 3 [19200/71432 (27%)]\tLoss: 6.265908\n",
      "Train Epoch: 3 [25600/71432 (36%)]\tLoss: 15.318336\n",
      "Train Epoch: 3 [32000/71432 (45%)]\tLoss: 21.957382\n",
      "Train Epoch: 3 [38400/71432 (54%)]\tLoss: 10.599272\n",
      "Train Epoch: 3 [44800/71432 (63%)]\tLoss: 9.167578\n",
      "Train Epoch: 3 [51200/71432 (72%)]\tLoss: 15.493443\n",
      "Train Epoch: 3 [57600/71432 (81%)]\tLoss: 7.803523\n",
      "Train Epoch: 3 [64000/71432 (90%)]\tLoss: 13.470689\n",
      "Train Epoch: 3 [70400/71432 (98%)]\tLoss: 18.927956\n",
      "\n",
      "Test set: Average loss: 0.1916, Accuracy: 7538/8141 (93%), Positive accuracy: 3716/4091 (91%), Negative accuracy: 3822/4050 (94%), train loss: 0.1986\n",
      "\n",
      "Train Epoch: 4 [0/71432 (0%)]\tLoss: 4.525685\n",
      "Train Epoch: 4 [6400/71432 (9%)]\tLoss: 11.413948\n",
      "Train Epoch: 4 [12800/71432 (18%)]\tLoss: 10.337744\n",
      "Train Epoch: 4 [19200/71432 (27%)]\tLoss: 13.829277\n",
      "Train Epoch: 4 [25600/71432 (36%)]\tLoss: 10.648000\n",
      "Train Epoch: 4 [32000/71432 (45%)]\tLoss: 9.183097\n",
      "Train Epoch: 4 [38400/71432 (54%)]\tLoss: 8.687589\n",
      "Train Epoch: 4 [44800/71432 (63%)]\tLoss: 7.233505\n",
      "Train Epoch: 4 [51200/71432 (72%)]\tLoss: 7.977865\n",
      "Train Epoch: 4 [57600/71432 (81%)]\tLoss: 12.913610\n",
      "Train Epoch: 4 [64000/71432 (90%)]\tLoss: 7.191594\n",
      "Train Epoch: 4 [70400/71432 (98%)]\tLoss: 15.340242\n",
      "\n",
      "Test set: Average loss: 0.1946, Accuracy: 7568/8141 (93%), Positive accuracy: 3730/4091 (91%), Negative accuracy: 3838/4050 (95%), train loss: 0.1911\n",
      "\n",
      "Train Epoch: 5 [0/71432 (0%)]\tLoss: 11.228333\n",
      "Train Epoch: 5 [6400/71432 (9%)]\tLoss: 8.029409\n",
      "Train Epoch: 5 [12800/71432 (18%)]\tLoss: 7.156737\n",
      "Train Epoch: 5 [19200/71432 (27%)]\tLoss: 11.244967\n",
      "Train Epoch: 5 [25600/71432 (36%)]\tLoss: 22.286898\n",
      "Train Epoch: 5 [32000/71432 (45%)]\tLoss: 12.147220\n",
      "Train Epoch: 5 [38400/71432 (54%)]\tLoss: 10.312797\n",
      "Train Epoch: 5 [44800/71432 (63%)]\tLoss: 15.073043\n",
      "Train Epoch: 5 [51200/71432 (72%)]\tLoss: 11.375280\n",
      "Train Epoch: 5 [57600/71432 (81%)]\tLoss: 9.181170\n",
      "Train Epoch: 5 [64000/71432 (90%)]\tLoss: 30.953157\n",
      "Train Epoch: 5 [70400/71432 (98%)]\tLoss: 7.275300\n",
      "\n",
      "Test set: Average loss: 0.1780, Accuracy: 7587/8141 (93%), Positive accuracy: 3758/4091 (92%), Negative accuracy: 3829/4050 (95%), train loss: 0.1867\n",
      "\n",
      "Train Epoch: 6 [0/71432 (0%)]\tLoss: 12.656019\n",
      "Train Epoch: 6 [6400/71432 (9%)]\tLoss: 12.505353\n",
      "Train Epoch: 6 [12800/71432 (18%)]\tLoss: 15.405834\n",
      "Train Epoch: 6 [19200/71432 (27%)]\tLoss: 11.881155\n",
      "Train Epoch: 6 [25600/71432 (36%)]\tLoss: 11.198609\n",
      "Train Epoch: 6 [32000/71432 (45%)]\tLoss: 19.696980\n",
      "Train Epoch: 6 [38400/71432 (54%)]\tLoss: 11.070728\n",
      "Train Epoch: 6 [44800/71432 (63%)]\tLoss: 15.453032\n",
      "Train Epoch: 6 [51200/71432 (72%)]\tLoss: 8.923892\n",
      "Train Epoch: 6 [57600/71432 (81%)]\tLoss: 8.488992\n",
      "Train Epoch: 6 [64000/71432 (90%)]\tLoss: 18.881777\n",
      "Train Epoch: 6 [70400/71432 (98%)]\tLoss: 12.786618\n",
      "\n",
      "Test set: Average loss: 0.1751, Accuracy: 7611/8141 (93%), Positive accuracy: 3765/4091 (92%), Negative accuracy: 3846/4050 (95%), train loss: 0.1793\n",
      "\n",
      "Train Epoch: 7 [0/71432 (0%)]\tLoss: 8.927872\n",
      "Train Epoch: 7 [6400/71432 (9%)]\tLoss: 12.002681\n",
      "Train Epoch: 7 [12800/71432 (18%)]\tLoss: 16.686029\n",
      "Train Epoch: 7 [19200/71432 (27%)]\tLoss: 7.779758\n",
      "Train Epoch: 7 [25600/71432 (36%)]\tLoss: 5.392008\n",
      "Train Epoch: 7 [32000/71432 (45%)]\tLoss: 10.716162\n",
      "Train Epoch: 7 [38400/71432 (54%)]\tLoss: 16.224810\n",
      "Train Epoch: 7 [44800/71432 (63%)]\tLoss: 6.802735\n",
      "Train Epoch: 7 [51200/71432 (72%)]\tLoss: 11.243481\n",
      "Train Epoch: 7 [57600/71432 (81%)]\tLoss: 14.110431\n",
      "Train Epoch: 7 [64000/71432 (90%)]\tLoss: 15.860814\n",
      "Train Epoch: 7 [70400/71432 (98%)]\tLoss: 8.977641\n",
      "\n",
      "Test set: Average loss: 0.1761, Accuracy: 7627/8141 (94%), Positive accuracy: 3845/4091 (94%), Negative accuracy: 3782/4050 (93%), train loss: 0.1785\n",
      "\n",
      "Train Epoch: 8 [0/71432 (0%)]\tLoss: 5.503403\n",
      "Train Epoch: 8 [6400/71432 (9%)]\tLoss: 7.995509\n",
      "Train Epoch: 8 [12800/71432 (18%)]\tLoss: 14.070179\n",
      "Train Epoch: 8 [19200/71432 (27%)]\tLoss: 9.854252\n",
      "Train Epoch: 8 [25600/71432 (36%)]\tLoss: 17.035320\n",
      "Train Epoch: 8 [32000/71432 (45%)]\tLoss: 19.898281\n",
      "Train Epoch: 8 [38400/71432 (54%)]\tLoss: 7.924694\n",
      "Train Epoch: 8 [44800/71432 (63%)]\tLoss: 10.455941\n",
      "Train Epoch: 8 [51200/71432 (72%)]\tLoss: 7.597138\n",
      "Train Epoch: 8 [57600/71432 (81%)]\tLoss: 6.941180\n",
      "Train Epoch: 8 [64000/71432 (90%)]\tLoss: 19.657032\n",
      "Train Epoch: 8 [70400/71432 (98%)]\tLoss: 16.437561\n",
      "\n",
      "Test set: Average loss: 0.1734, Accuracy: 7570/8141 (93%), Positive accuracy: 3699/4091 (90%), Negative accuracy: 3871/4050 (96%), train loss: 0.1751\n",
      "\n",
      "Train Epoch: 9 [0/71432 (0%)]\tLoss: 18.733356\n",
      "Train Epoch: 9 [6400/71432 (9%)]\tLoss: 17.814951\n",
      "Train Epoch: 9 [12800/71432 (18%)]\tLoss: 10.477026\n",
      "Train Epoch: 9 [19200/71432 (27%)]\tLoss: 14.028068\n",
      "Train Epoch: 9 [25600/71432 (36%)]\tLoss: 7.695834\n",
      "Train Epoch: 9 [32000/71432 (45%)]\tLoss: 9.000176\n",
      "Train Epoch: 9 [38400/71432 (54%)]\tLoss: 8.873530\n",
      "Train Epoch: 9 [44800/71432 (63%)]\tLoss: 12.880066\n",
      "Train Epoch: 9 [51200/71432 (72%)]\tLoss: 13.494879\n",
      "Train Epoch: 9 [57600/71432 (81%)]\tLoss: 13.134168\n",
      "Train Epoch: 9 [64000/71432 (90%)]\tLoss: 10.401440\n",
      "Train Epoch: 9 [70400/71432 (98%)]\tLoss: 15.607653\n",
      "\n",
      "Test set: Average loss: 0.2045, Accuracy: 7470/8141 (92%), Positive accuracy: 3698/4091 (90%), Negative accuracy: 3772/4050 (93%), train loss: 0.1750\n",
      "\n",
      "Train Epoch: 10 [0/71432 (0%)]\tLoss: 20.252439\n",
      "Train Epoch: 10 [6400/71432 (9%)]\tLoss: 9.382166\n",
      "Train Epoch: 10 [12800/71432 (18%)]\tLoss: 15.376451\n",
      "Train Epoch: 10 [19200/71432 (27%)]\tLoss: 13.227932\n",
      "Train Epoch: 10 [25600/71432 (36%)]\tLoss: 11.506997\n",
      "Train Epoch: 10 [32000/71432 (45%)]\tLoss: 14.068561\n",
      "Train Epoch: 10 [38400/71432 (54%)]\tLoss: 11.032742\n",
      "Train Epoch: 10 [44800/71432 (63%)]\tLoss: 12.207366\n",
      "Train Epoch: 10 [51200/71432 (72%)]\tLoss: 13.479105\n",
      "Train Epoch: 10 [57600/71432 (81%)]\tLoss: 7.141129\n",
      "Train Epoch: 10 [64000/71432 (90%)]\tLoss: 8.098031\n",
      "Train Epoch: 10 [70400/71432 (98%)]\tLoss: 8.891722\n",
      "\n",
      "Test set: Average loss: 0.1667, Accuracy: 7618/8141 (94%), Positive accuracy: 3735/4091 (91%), Negative accuracy: 3883/4050 (96%), train loss: 0.1719\n",
      "\n",
      "Train Epoch: 11 [0/71432 (0%)]\tLoss: 9.210414\n",
      "Train Epoch: 11 [6400/71432 (9%)]\tLoss: 9.353351\n",
      "Train Epoch: 11 [12800/71432 (18%)]\tLoss: 16.877169\n",
      "Train Epoch: 11 [19200/71432 (27%)]\tLoss: 8.854151\n",
      "Train Epoch: 11 [25600/71432 (36%)]\tLoss: 9.458814\n",
      "Train Epoch: 11 [32000/71432 (45%)]\tLoss: 7.894894\n",
      "Train Epoch: 11 [38400/71432 (54%)]\tLoss: 17.107529\n",
      "Train Epoch: 11 [44800/71432 (63%)]\tLoss: 9.189207\n",
      "Train Epoch: 11 [51200/71432 (72%)]\tLoss: 6.933424\n",
      "Train Epoch: 11 [57600/71432 (81%)]\tLoss: 11.677713\n",
      "Train Epoch: 11 [64000/71432 (90%)]\tLoss: 12.811179\n",
      "Train Epoch: 11 [70400/71432 (98%)]\tLoss: 3.962044\n",
      "\n",
      "Test set: Average loss: 0.1899, Accuracy: 7508/8141 (92%), Positive accuracy: 3604/4091 (88%), Negative accuracy: 3904/4050 (96%), train loss: 0.1698\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [0/71432 (0%)]\tLoss: 7.344242\n",
      "Train Epoch: 12 [6400/71432 (9%)]\tLoss: 15.782540\n",
      "Train Epoch: 12 [12800/71432 (18%)]\tLoss: 9.478770\n",
      "Train Epoch: 12 [19200/71432 (27%)]\tLoss: 15.699148\n",
      "Train Epoch: 12 [25600/71432 (36%)]\tLoss: 19.821827\n",
      "Train Epoch: 12 [32000/71432 (45%)]\tLoss: 9.793143\n",
      "Train Epoch: 12 [38400/71432 (54%)]\tLoss: 10.953063\n",
      "Train Epoch: 12 [44800/71432 (63%)]\tLoss: 7.606368\n",
      "Train Epoch: 12 [51200/71432 (72%)]\tLoss: 11.750005\n",
      "Train Epoch: 12 [57600/71432 (81%)]\tLoss: 13.582916\n",
      "Train Epoch: 12 [64000/71432 (90%)]\tLoss: 6.380159\n",
      "Train Epoch: 12 [70400/71432 (98%)]\tLoss: 15.152920\n",
      "\n",
      "Test set: Average loss: 0.2355, Accuracy: 7374/8141 (91%), Positive accuracy: 3474/4091 (85%), Negative accuracy: 3900/4050 (96%), train loss: 0.1692\n",
      "\n",
      "Train Epoch: 13 [0/71432 (0%)]\tLoss: 10.149737\n",
      "Train Epoch: 13 [6400/71432 (9%)]\tLoss: 10.663799\n",
      "Train Epoch: 13 [12800/71432 (18%)]\tLoss: 9.421299\n",
      "Train Epoch: 13 [19200/71432 (27%)]\tLoss: 16.689062\n",
      "Train Epoch: 13 [25600/71432 (36%)]\tLoss: 7.014350\n",
      "Train Epoch: 13 [32000/71432 (45%)]\tLoss: 7.631852\n",
      "Train Epoch: 13 [38400/71432 (54%)]\tLoss: 7.429485\n",
      "Train Epoch: 13 [44800/71432 (63%)]\tLoss: 12.930496\n",
      "Train Epoch: 13 [51200/71432 (72%)]\tLoss: 13.336860\n",
      "Train Epoch: 13 [57600/71432 (81%)]\tLoss: 5.598822\n",
      "Train Epoch: 13 [64000/71432 (90%)]\tLoss: 4.667440\n",
      "Train Epoch: 13 [70400/71432 (98%)]\tLoss: 13.857121\n",
      "\n",
      "Test set: Average loss: 0.1631, Accuracy: 7636/8141 (94%), Positive accuracy: 3786/4091 (93%), Negative accuracy: 3850/4050 (95%), train loss: 0.1683\n",
      "\n",
      "Train Epoch: 14 [0/71432 (0%)]\tLoss: 10.898941\n",
      "Train Epoch: 14 [6400/71432 (9%)]\tLoss: 2.728095\n",
      "Train Epoch: 14 [12800/71432 (18%)]\tLoss: 9.181990\n",
      "Train Epoch: 14 [19200/71432 (27%)]\tLoss: 20.171225\n",
      "Train Epoch: 14 [25600/71432 (36%)]\tLoss: 11.842731\n",
      "Train Epoch: 14 [32000/71432 (45%)]\tLoss: 11.497828\n",
      "Train Epoch: 14 [38400/71432 (54%)]\tLoss: 6.955735\n",
      "Train Epoch: 14 [44800/71432 (63%)]\tLoss: 19.061993\n",
      "Train Epoch: 14 [51200/71432 (72%)]\tLoss: 8.988092\n",
      "Train Epoch: 14 [57600/71432 (81%)]\tLoss: 11.167157\n",
      "Train Epoch: 14 [64000/71432 (90%)]\tLoss: 16.597351\n",
      "Train Epoch: 14 [70400/71432 (98%)]\tLoss: 14.609114\n",
      "\n",
      "Test set: Average loss: 0.2125, Accuracy: 7443/8141 (91%), Positive accuracy: 3524/4091 (86%), Negative accuracy: 3919/4050 (97%), train loss: 0.1669\n",
      "\n",
      "Train Epoch: 15 [0/71432 (0%)]\tLoss: 9.780563\n",
      "Train Epoch: 15 [6400/71432 (9%)]\tLoss: 22.115625\n",
      "Train Epoch: 15 [12800/71432 (18%)]\tLoss: 4.978778\n",
      "Train Epoch: 15 [19200/71432 (27%)]\tLoss: 12.791647\n",
      "Train Epoch: 15 [25600/71432 (36%)]\tLoss: 9.499985\n",
      "Train Epoch: 15 [32000/71432 (45%)]\tLoss: 10.858666\n",
      "Train Epoch: 15 [38400/71432 (54%)]\tLoss: 8.961700\n",
      "Train Epoch: 15 [44800/71432 (63%)]\tLoss: 10.336700\n",
      "Train Epoch: 15 [51200/71432 (72%)]\tLoss: 11.724525\n",
      "Train Epoch: 15 [57600/71432 (81%)]\tLoss: 6.971073\n",
      "Train Epoch: 15 [64000/71432 (90%)]\tLoss: 8.148758\n",
      "Train Epoch: 15 [70400/71432 (98%)]\tLoss: 9.422707\n",
      "\n",
      "Test set: Average loss: 0.1829, Accuracy: 7530/8141 (92%), Positive accuracy: 3635/4091 (89%), Negative accuracy: 3895/4050 (96%), train loss: 0.1651\n",
      "\n",
      "Train Epoch: 16 [0/71432 (0%)]\tLoss: 3.326846\n",
      "Train Epoch: 16 [6400/71432 (9%)]\tLoss: 11.020884\n",
      "Train Epoch: 16 [12800/71432 (18%)]\tLoss: 10.604300\n",
      "Train Epoch: 16 [19200/71432 (27%)]\tLoss: 10.053862\n",
      "Train Epoch: 16 [25600/71432 (36%)]\tLoss: 10.738915\n",
      "Train Epoch: 16 [32000/71432 (45%)]\tLoss: 9.486262\n",
      "Train Epoch: 16 [38400/71432 (54%)]\tLoss: 15.183279\n",
      "Train Epoch: 16 [44800/71432 (63%)]\tLoss: 10.149918\n",
      "Train Epoch: 16 [51200/71432 (72%)]\tLoss: 29.103237\n",
      "Train Epoch: 16 [57600/71432 (81%)]\tLoss: 12.129298\n",
      "Train Epoch: 16 [64000/71432 (90%)]\tLoss: 8.262820\n",
      "Train Epoch: 16 [70400/71432 (98%)]\tLoss: 6.039827\n",
      "\n",
      "Test set: Average loss: 0.1654, Accuracy: 7658/8141 (94%), Positive accuracy: 3885/4091 (95%), Negative accuracy: 3773/4050 (93%), train loss: 0.1647\n",
      "\n",
      "\n",
      "correct_0: mean: 0.8503, std: 0.0813\n",
      "\n",
      "correct_1: mean: 0.2824, std: 0.1429\n",
      "\n",
      "wrong_0: mean: 0.5124, std: 0.1004\n",
      "\n",
      "wrong_1: mean: 0.7096, std: 0.0642\n",
      "Train Epoch: 17 [0/71432 (0%)]\tLoss: 12.343002\n",
      "Train Epoch: 17 [6400/71432 (9%)]\tLoss: 6.728675\n",
      "Train Epoch: 17 [12800/71432 (18%)]\tLoss: 21.326153\n",
      "Train Epoch: 17 [19200/71432 (27%)]\tLoss: 10.129022\n",
      "Train Epoch: 17 [25600/71432 (36%)]\tLoss: 3.894568\n",
      "Train Epoch: 17 [32000/71432 (45%)]\tLoss: 6.813775\n",
      "Train Epoch: 17 [38400/71432 (54%)]\tLoss: 12.364668\n",
      "Train Epoch: 17 [44800/71432 (63%)]\tLoss: 9.326601\n",
      "Train Epoch: 17 [51200/71432 (72%)]\tLoss: 19.456722\n",
      "Train Epoch: 17 [57600/71432 (81%)]\tLoss: 12.551371\n",
      "Train Epoch: 17 [64000/71432 (90%)]\tLoss: 10.418339\n",
      "Train Epoch: 17 [70400/71432 (98%)]\tLoss: 10.542188\n",
      "\n",
      "Test set: Average loss: 0.1905, Accuracy: 7508/8141 (92%), Positive accuracy: 3599/4091 (88%), Negative accuracy: 3909/4050 (97%), train loss: 0.1637\n",
      "\n",
      "Train Epoch: 18 [0/71432 (0%)]\tLoss: 9.135499\n",
      "Train Epoch: 18 [6400/71432 (9%)]\tLoss: 9.275992\n",
      "Train Epoch: 18 [12800/71432 (18%)]\tLoss: 9.276555\n",
      "Train Epoch: 18 [19200/71432 (27%)]\tLoss: 7.561688\n",
      "Train Epoch: 18 [25600/71432 (36%)]\tLoss: 18.356808\n",
      "Train Epoch: 18 [32000/71432 (45%)]\tLoss: 8.763074\n",
      "Train Epoch: 18 [38400/71432 (54%)]\tLoss: 13.855391\n",
      "Train Epoch: 18 [44800/71432 (63%)]\tLoss: 8.927876\n",
      "Train Epoch: 18 [51200/71432 (72%)]\tLoss: 8.061585\n",
      "Train Epoch: 18 [57600/71432 (81%)]\tLoss: 15.106682\n",
      "Train Epoch: 18 [64000/71432 (90%)]\tLoss: 8.048877\n",
      "Train Epoch: 18 [70400/71432 (98%)]\tLoss: 7.480063\n",
      "\n",
      "Test set: Average loss: 0.1977, Accuracy: 7500/8141 (92%), Positive accuracy: 3567/4091 (87%), Negative accuracy: 3933/4050 (97%), train loss: 0.1642\n",
      "\n",
      "Train Epoch: 19 [0/71432 (0%)]\tLoss: 16.305235\n",
      "Train Epoch: 19 [6400/71432 (9%)]\tLoss: 7.011967\n",
      "Train Epoch: 19 [12800/71432 (18%)]\tLoss: 22.955692\n",
      "Train Epoch: 19 [19200/71432 (27%)]\tLoss: 9.344751\n",
      "Train Epoch: 19 [25600/71432 (36%)]\tLoss: 9.915250\n",
      "Train Epoch: 19 [32000/71432 (45%)]\tLoss: 8.811098\n",
      "Train Epoch: 19 [38400/71432 (54%)]\tLoss: 20.054977\n",
      "Train Epoch: 19 [44800/71432 (63%)]\tLoss: 4.999605\n",
      "Train Epoch: 19 [51200/71432 (72%)]\tLoss: 6.266148\n",
      "Train Epoch: 19 [57600/71432 (81%)]\tLoss: 6.759766\n",
      "Train Epoch: 19 [64000/71432 (90%)]\tLoss: 5.513367\n",
      "Train Epoch: 19 [70400/71432 (98%)]\tLoss: 9.196118\n",
      "\n",
      "Test set: Average loss: 0.1761, Accuracy: 7557/8141 (93%), Positive accuracy: 3684/4091 (90%), Negative accuracy: 3873/4050 (96%), train loss: 0.1625\n",
      "\n",
      "Train Epoch: 20 [0/71432 (0%)]\tLoss: 7.233092\n",
      "Train Epoch: 20 [6400/71432 (9%)]\tLoss: 12.382372\n",
      "Train Epoch: 20 [12800/71432 (18%)]\tLoss: 12.722471\n",
      "Train Epoch: 20 [19200/71432 (27%)]\tLoss: 18.176592\n",
      "Train Epoch: 20 [25600/71432 (36%)]\tLoss: 18.616720\n",
      "Train Epoch: 20 [32000/71432 (45%)]\tLoss: 7.771194\n",
      "Train Epoch: 20 [38400/71432 (54%)]\tLoss: 5.234972\n",
      "Train Epoch: 20 [44800/71432 (63%)]\tLoss: 10.715703\n",
      "Train Epoch: 20 [51200/71432 (72%)]\tLoss: 14.558449\n",
      "Train Epoch: 20 [57600/71432 (81%)]\tLoss: 20.263721\n",
      "Train Epoch: 20 [64000/71432 (90%)]\tLoss: 6.211475\n",
      "Train Epoch: 20 [70400/71432 (98%)]\tLoss: 6.730323\n",
      "\n",
      "Test set: Average loss: 0.1738, Accuracy: 7566/8141 (93%), Positive accuracy: 3684/4091 (90%), Negative accuracy: 3882/4050 (96%), train loss: 0.1613\n",
      "\n",
      "[7435, 7527, 7538, 7568, 7587, 7611, 7627, 7570, 7470, 7618, 7508, 7374, 7636, 7443, 7530, 7658, 7508, 7500, 7557, 7566]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def myLoss(output, target):\n",
    "#     print(\"size,size:\",output.size(), target.size())\n",
    "#     print(\"size,\", ((1-2*target) * torch.log(output)).size())\n",
    "#     return -torch.sum(target * torch.log(output) + (1-target) * torch.log(1-output)) / len(output)\n",
    "    return -torch.sum((36885/34546)*target * torch.log(output) + (1-target) * torch.log(1-output)) / len(output)\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)[0]\n",
    "#         loss = F.nll_loss(output, target)\n",
    "        loss = myLoss(output, target)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item()/64)\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    train_loss = sum(loss_list)/len(loss_list)\n",
    "    return train_loss\n",
    "\n",
    "        \n",
    "def test(args, model, device, test_loader,count,epoch,train_loss):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    result= [[0,0], [0,0]] \n",
    "    correct_pc_true0 = []\n",
    "    correct_pc_true1 = []\n",
    "    wrong_pc_true0 = []\n",
    "    wrong_pc_true1 = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx,(data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)[0]\n",
    "#             test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            test_loss += myLoss(output, target)\n",
    "#             pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            t = Variable(torch.Tensor([0.5]))\n",
    "            pred = (output > t) * 1\n",
    "            pred = torch.reshape(pred, (len(target), 1))\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            cmat = confusion_matrix(target.view_as(pred), pred, labels=[0, 1]) \n",
    "            result = [[result[i][j] + cmat[i][j]  for j in range(len(result[0]))] for i in range(len(result))] \n",
    "             # Store wrongly predicted images\n",
    "            if epoch == 16:\n",
    "                num = 0\n",
    "                wrong_idx = (pred != target.view_as(pred)).nonzero()[:, 0]\n",
    "                wrong_samples = data[wrong_idx]\n",
    "                wrong_preds = pred[wrong_idx]\n",
    "                actual_preds = target.view_as(pred)[wrong_idx]\n",
    "                true_idx = (pred == target.view_as(pred)).nonzero()[:, 0]\n",
    "                true_samples = data[true_idx]\n",
    "                true_preds = pred[true_idx]\n",
    "                for i in range(len(true_idx)):\n",
    "                    true_pred = true_preds[i]\n",
    "                    if true_pred.item() == 0:\n",
    "                        correct_pc_true0.append(model(data)[1][true_idx[i]].item())\n",
    "                    else:\n",
    "                        correct_pc_true1.append(model(data)[1][true_idx[i]].item())\n",
    "                        \n",
    "                for i in range(len(wrong_idx)):\n",
    "                    actual_pred = actual_preds[i]\n",
    "                    if actual_pred.item() == 0:\n",
    "                        wrong_pc_true0.append(model(data)[1][wrong_idx[i]].item())\n",
    "                    else:\n",
    "                        wrong_pc_true1.append(model(data)[1][wrong_idx[i]].item())\n",
    "                    sample = wrong_samples[i]\n",
    "                    wrong_pred = wrong_preds[i]\n",
    "                    actual_pred = actual_preds[i]\n",
    "                    # Undo normalization\n",
    "            #         sample = sample * 0.3081\n",
    "            #         sample = sample + 0.1307\n",
    "                    sample = sample * 255.\n",
    "                    sample = sample.byte()\n",
    "                    img = TF.to_pil_image(sample)\n",
    "                    num = num + 1\n",
    "                    img.save('./wrong-gabor/{}_true{}_pc{:.4f}.png'.format(\n",
    "                    batch_idx*64+num, actual_pred.item(),model(data)[1][wrong_idx[i]].item()))\n",
    "                    \n",
    "    #                 print(batch_idx,wrong_idx[i])\n",
    "                    img_ori = origin_dataset[num][0].numpy()\n",
    "                    plt.imsave('./wrong-gabor/{}_true{}__pc{:.4f}_ori.png'.format(\n",
    "                    batch_idx*64+num, actual_pred.item(), model(data)[1][wrong_idx[i]].item()),img_ori[0], cmap = 'gray')\n",
    "                    \n",
    "            \n",
    "                \n",
    "                    \n",
    "                \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Positive accuracy: {}/{} ({:.0f}%), Negative accuracy: {}/{} ({:.0f}%), train loss: {:.4f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset), \n",
    "        result[1][1],result[1][1]+result[1][0],100. * result[1][1]/(result[1][1]+result[1][0]),\n",
    "        result[0][0],result[0][0]+result[0][1],100. * result[0][0]/(result[0][0]+result[0][1]),train_loss))\n",
    "    if len(correct_pc_true0) != 0:\n",
    "        print('\\ncorrect_0: mean: {:.4f}, std: {:.4f}'.format(statistics.mean(correct_pc_true0),statistics.pstdev(correct_pc_true0)))\n",
    "        print('\\ncorrect_1: mean: {:.4f}, std: {:.4f}'.format(statistics.mean(correct_pc_true1),statistics.pstdev(correct_pc_true1)))\n",
    "        print('\\nwrong_0: mean: {:.4f}, std: {:.4f}'.format(statistics.mean(wrong_pc_true0), statistics.pstdev(wrong_pc_true0)))\n",
    "        print('\\nwrong_1: mean: {:.4f}, std: {:.4f}'.format(statistics.mean(wrong_pc_true1), statistics.pstdev(wrong_pc_true1)))\n",
    "    return test_loss, correct\n",
    "    \n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=True,\n",
    "                        help='For Saving the current Model')\n",
    "    parser.add_argument('--std', type=float, default=0, metavar='STD',\n",
    "                        help='noise-std (default: 0)')\n",
    "    parser.add_argument('--mean', type=float, default=0, metavar='MEAN',\n",
    "                        help='noise-std (default: 0)')\n",
    "#     args = parser.parse_args()\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "#     transform=transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,))\n",
    "#         ])\n",
    "    model = GaborConvPC(19, 1, 8, 6).to(device)\n",
    "    # if torch.cuda.is_available():\n",
    "    #     torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    count = 0\n",
    "    for param in model.parameters():\n",
    "        print(type(param.data), param.size())\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_accuracy_list = []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_result = test(args, model, device, test_loader,count,epoch,train_loss)\n",
    "        test_loss_list.append(test_result[0])\n",
    "        test_accuracy_list.append(test_result[1])\n",
    "        # for param in model.parameters():\n",
    "        #     print(param.size(), param.data)\n",
    "        # print(model.state_dict())\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    if (args.save_model):\n",
    "        torch.save(model.state_dict(),\"pretrain_gabor.pt\")\n",
    "    plt.plot(train_loss_list, color='blue',label='train loss')  \n",
    "    plt.plot(test_loss_list, color='red',label='test loss')  \n",
    "    plt.legend()\n",
    "    print(test_accuracy_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/71432 (0%)]\tLoss: 0.692208\n",
      "Train Epoch: 1 [6400/71432 (9%)]\tLoss: 0.681850\n",
      "Train Epoch: 1 [12800/71432 (18%)]\tLoss: 0.620283\n",
      "Train Epoch: 1 [19200/71432 (27%)]\tLoss: 0.434132\n",
      "Train Epoch: 1 [25600/71432 (36%)]\tLoss: 0.380064\n",
      "Train Epoch: 1 [32000/71432 (45%)]\tLoss: 0.387745\n",
      "Train Epoch: 1 [38400/71432 (54%)]\tLoss: 0.371992\n",
      "Train Epoch: 1 [44800/71432 (63%)]\tLoss: 0.411371\n",
      "Train Epoch: 1 [51200/71432 (72%)]\tLoss: 0.384105\n",
      "Train Epoch: 1 [57600/71432 (81%)]\tLoss: 0.270502\n",
      "Train Epoch: 1 [64000/71432 (90%)]\tLoss: 0.394748\n",
      "Train Epoch: 1 [70400/71432 (98%)]\tLoss: 0.372798\n",
      "\n",
      "Test set: Average loss: 0.2699, Accuracy: 7186/8141 (88%), Positive accuracy: 3743/4091 (91%), Negative accuracy: 3443/4050 (85%), f1 score: 0.8837, Train loss: 0.4389\n",
      "\n",
      "Train Epoch: 2 [0/71432 (0%)]\tLoss: 0.243357\n",
      "Train Epoch: 2 [6400/71432 (9%)]\tLoss: 0.303426\n",
      "Train Epoch: 2 [12800/71432 (18%)]\tLoss: 0.293365\n",
      "Train Epoch: 2 [19200/71432 (27%)]\tLoss: 0.222958\n",
      "Train Epoch: 2 [25600/71432 (36%)]\tLoss: 0.250056\n",
      "Train Epoch: 2 [32000/71432 (45%)]\tLoss: 0.212768\n",
      "Train Epoch: 2 [38400/71432 (54%)]\tLoss: 0.237044\n",
      "Train Epoch: 2 [44800/71432 (63%)]\tLoss: 0.241775\n",
      "Train Epoch: 2 [51200/71432 (72%)]\tLoss: 0.142465\n",
      "Train Epoch: 2 [57600/71432 (81%)]\tLoss: 0.157836\n",
      "Train Epoch: 2 [64000/71432 (90%)]\tLoss: 0.192026\n",
      "Train Epoch: 2 [70400/71432 (98%)]\tLoss: 0.137392\n",
      "\n",
      "Test set: Average loss: 0.1470, Accuracy: 7662/8141 (94%), Positive accuracy: 3926/4091 (96%), Negative accuracy: 3736/4050 (92%), f1 score: 0.9416, Train loss: 0.2447\n",
      "\n",
      "Train Epoch: 3 [0/71432 (0%)]\tLoss: 0.115863\n",
      "Train Epoch: 3 [6400/71432 (9%)]\tLoss: 0.151987\n",
      "Train Epoch: 3 [12800/71432 (18%)]\tLoss: 0.124285\n",
      "Train Epoch: 3 [19200/71432 (27%)]\tLoss: 0.147770\n",
      "Train Epoch: 3 [25600/71432 (36%)]\tLoss: 0.150912\n",
      "Train Epoch: 3 [32000/71432 (45%)]\tLoss: 0.181994\n",
      "Train Epoch: 3 [38400/71432 (54%)]\tLoss: 0.071258\n",
      "Train Epoch: 3 [44800/71432 (63%)]\tLoss: 0.159391\n",
      "Train Epoch: 3 [51200/71432 (72%)]\tLoss: 0.183982\n",
      "Train Epoch: 3 [57600/71432 (81%)]\tLoss: 0.089612\n",
      "Train Epoch: 3 [64000/71432 (90%)]\tLoss: 0.112651\n",
      "Train Epoch: 3 [70400/71432 (98%)]\tLoss: 0.177634\n",
      "\n",
      "Test set: Average loss: 0.1286, Accuracy: 7716/8141 (95%), Positive accuracy: 3872/4091 (95%), Negative accuracy: 3844/4050 (95%), f1 score: 0.9478, Train loss: 0.1606\n",
      "\n",
      "Train Epoch: 4 [0/71432 (0%)]\tLoss: 0.104506\n",
      "Train Epoch: 4 [6400/71432 (9%)]\tLoss: 0.111383\n",
      "Train Epoch: 4 [12800/71432 (18%)]\tLoss: 0.189019\n",
      "Train Epoch: 4 [19200/71432 (27%)]\tLoss: 0.209996\n",
      "Train Epoch: 4 [25600/71432 (36%)]\tLoss: 0.276096\n",
      "Train Epoch: 4 [32000/71432 (45%)]\tLoss: 0.110612\n",
      "Train Epoch: 4 [38400/71432 (54%)]\tLoss: 0.118561\n",
      "Train Epoch: 4 [44800/71432 (63%)]\tLoss: 0.100010\n",
      "Train Epoch: 4 [51200/71432 (72%)]\tLoss: 0.104701\n",
      "Train Epoch: 4 [57600/71432 (81%)]\tLoss: 0.127103\n",
      "Train Epoch: 4 [64000/71432 (90%)]\tLoss: 0.166843\n",
      "Train Epoch: 4 [70400/71432 (98%)]\tLoss: 0.166044\n",
      "\n",
      "Test set: Average loss: 0.1277, Accuracy: 7727/8141 (95%), Positive accuracy: 3811/4091 (93%), Negative accuracy: 3916/4050 (97%), f1 score: 0.9494, Train loss: 0.1484\n",
      "\n",
      "Train Epoch: 5 [0/71432 (0%)]\tLoss: 0.108233\n",
      "Train Epoch: 5 [6400/71432 (9%)]\tLoss: 0.154660\n",
      "Train Epoch: 5 [12800/71432 (18%)]\tLoss: 0.115341\n",
      "Train Epoch: 5 [19200/71432 (27%)]\tLoss: 0.076972\n",
      "Train Epoch: 5 [25600/71432 (36%)]\tLoss: 0.118043\n",
      "Train Epoch: 5 [32000/71432 (45%)]\tLoss: 0.106744\n",
      "Train Epoch: 5 [38400/71432 (54%)]\tLoss: 0.174676\n",
      "Train Epoch: 5 [44800/71432 (63%)]\tLoss: 0.050830\n",
      "Train Epoch: 5 [51200/71432 (72%)]\tLoss: 0.155986\n",
      "Train Epoch: 5 [57600/71432 (81%)]\tLoss: 0.163319\n",
      "Train Epoch: 5 [64000/71432 (90%)]\tLoss: 0.141706\n",
      "Train Epoch: 5 [70400/71432 (98%)]\tLoss: 0.146558\n",
      "\n",
      "Test set: Average loss: 0.1201, Accuracy: 7748/8141 (95%), Positive accuracy: 3903/4091 (95%), Negative accuracy: 3845/4050 (95%), f1 score: 0.9517, Train loss: 0.1417\n",
      "\n",
      "Train Epoch: 6 [0/71432 (0%)]\tLoss: 0.242830\n",
      "Train Epoch: 6 [6400/71432 (9%)]\tLoss: 0.126483\n",
      "Train Epoch: 6 [12800/71432 (18%)]\tLoss: 0.081789\n",
      "Train Epoch: 6 [19200/71432 (27%)]\tLoss: 0.178466\n",
      "Train Epoch: 6 [25600/71432 (36%)]\tLoss: 0.112615\n",
      "Train Epoch: 6 [32000/71432 (45%)]\tLoss: 0.190780\n",
      "Train Epoch: 6 [38400/71432 (54%)]\tLoss: 0.160519\n",
      "Train Epoch: 6 [44800/71432 (63%)]\tLoss: 0.121867\n",
      "Train Epoch: 6 [51200/71432 (72%)]\tLoss: 0.147107\n",
      "Train Epoch: 6 [57600/71432 (81%)]\tLoss: 0.202652\n",
      "Train Epoch: 6 [64000/71432 (90%)]\tLoss: 0.122766\n",
      "Train Epoch: 6 [70400/71432 (98%)]\tLoss: 0.121599\n",
      "\n",
      "Test set: Average loss: 0.1186, Accuracy: 7751/8141 (95%), Positive accuracy: 3883/4091 (95%), Negative accuracy: 3868/4050 (96%), f1 score: 0.9521, Train loss: 0.1410\n",
      "\n",
      "Train Epoch: 7 [0/71432 (0%)]\tLoss: 0.175651\n",
      "Train Epoch: 7 [6400/71432 (9%)]\tLoss: 0.128935\n",
      "Train Epoch: 7 [12800/71432 (18%)]\tLoss: 0.069513\n",
      "Train Epoch: 7 [19200/71432 (27%)]\tLoss: 0.082556\n",
      "Train Epoch: 7 [25600/71432 (36%)]\tLoss: 0.195547\n",
      "Train Epoch: 7 [32000/71432 (45%)]\tLoss: 0.148574\n",
      "Train Epoch: 7 [38400/71432 (54%)]\tLoss: 0.121438\n",
      "Train Epoch: 7 [44800/71432 (63%)]\tLoss: 0.129285\n",
      "Train Epoch: 7 [51200/71432 (72%)]\tLoss: 0.154456\n",
      "Train Epoch: 7 [57600/71432 (81%)]\tLoss: 0.227768\n",
      "Train Epoch: 7 [64000/71432 (90%)]\tLoss: 0.153303\n",
      "Train Epoch: 7 [70400/71432 (98%)]\tLoss: 0.137018\n",
      "\n",
      "Test set: Average loss: 0.1179, Accuracy: 7758/8141 (95%), Positive accuracy: 3890/4091 (95%), Negative accuracy: 3868/4050 (96%), f1 score: 0.9529, Train loss: 0.1386\n",
      "\n",
      "Train Epoch: 8 [0/71432 (0%)]\tLoss: 0.148044\n",
      "Train Epoch: 8 [6400/71432 (9%)]\tLoss: 0.102044\n",
      "Train Epoch: 8 [12800/71432 (18%)]\tLoss: 0.078511\n",
      "Train Epoch: 8 [19200/71432 (27%)]\tLoss: 0.126448\n",
      "Train Epoch: 8 [25600/71432 (36%)]\tLoss: 0.078731\n",
      "Train Epoch: 8 [32000/71432 (45%)]\tLoss: 0.140754\n",
      "Train Epoch: 8 [38400/71432 (54%)]\tLoss: 0.173438\n",
      "Train Epoch: 8 [44800/71432 (63%)]\tLoss: 0.094143\n",
      "Train Epoch: 8 [51200/71432 (72%)]\tLoss: 0.088574\n",
      "Train Epoch: 8 [57600/71432 (81%)]\tLoss: 0.138766\n",
      "Train Epoch: 8 [64000/71432 (90%)]\tLoss: 0.203075\n",
      "Train Epoch: 8 [70400/71432 (98%)]\tLoss: 0.205882\n",
      "\n",
      "Test set: Average loss: 0.1196, Accuracy: 7748/8141 (95%), Positive accuracy: 3864/4091 (94%), Negative accuracy: 3884/4050 (96%), f1 score: 0.9517, Train loss: 0.1369\n",
      "\n",
      "Train Epoch: 9 [0/71432 (0%)]\tLoss: 0.181974\n",
      "Train Epoch: 9 [6400/71432 (9%)]\tLoss: 0.395076\n",
      "Train Epoch: 9 [12800/71432 (18%)]\tLoss: 0.176565\n",
      "Train Epoch: 9 [19200/71432 (27%)]\tLoss: 0.102381\n",
      "Train Epoch: 9 [25600/71432 (36%)]\tLoss: 0.208083\n",
      "Train Epoch: 9 [32000/71432 (45%)]\tLoss: 0.107434\n",
      "Train Epoch: 9 [38400/71432 (54%)]\tLoss: 0.148754\n",
      "Train Epoch: 9 [44800/71432 (63%)]\tLoss: 0.152815\n",
      "Train Epoch: 9 [51200/71432 (72%)]\tLoss: 0.148400\n",
      "Train Epoch: 9 [57600/71432 (81%)]\tLoss: 0.151803\n",
      "Train Epoch: 9 [64000/71432 (90%)]\tLoss: 0.065334\n",
      "Train Epoch: 9 [70400/71432 (98%)]\tLoss: 0.104367\n",
      "\n",
      "Test set: Average loss: 0.1180, Accuracy: 7759/8141 (95%), Positive accuracy: 3892/4091 (95%), Negative accuracy: 3867/4050 (95%), f1 score: 0.9531, Train loss: 0.1381\n",
      "\n",
      "Train Epoch: 10 [0/71432 (0%)]\tLoss: 0.091237\n",
      "Train Epoch: 10 [6400/71432 (9%)]\tLoss: 0.154971\n",
      "Train Epoch: 10 [12800/71432 (18%)]\tLoss: 0.151702\n",
      "Train Epoch: 10 [19200/71432 (27%)]\tLoss: 0.095162\n",
      "Train Epoch: 10 [25600/71432 (36%)]\tLoss: 0.065879\n",
      "Train Epoch: 10 [32000/71432 (45%)]\tLoss: 0.194076\n",
      "Train Epoch: 10 [38400/71432 (54%)]\tLoss: 0.248538\n",
      "Train Epoch: 10 [44800/71432 (63%)]\tLoss: 0.128466\n",
      "Train Epoch: 10 [51200/71432 (72%)]\tLoss: 0.146634\n",
      "Train Epoch: 10 [57600/71432 (81%)]\tLoss: 0.118423\n",
      "Train Epoch: 10 [64000/71432 (90%)]\tLoss: 0.098908\n",
      "Train Epoch: 10 [70400/71432 (98%)]\tLoss: 0.091219\n",
      "\n",
      "Test set: Average loss: 0.1175, Accuracy: 7759/8141 (95%), Positive accuracy: 3892/4091 (95%), Negative accuracy: 3867/4050 (95%), f1 score: 0.9531, Train loss: 0.1356\n",
      "\n",
      "Train Epoch: 11 [0/71432 (0%)]\tLoss: 0.129147\n",
      "Train Epoch: 11 [6400/71432 (9%)]\tLoss: 0.115831\n",
      "Train Epoch: 11 [12800/71432 (18%)]\tLoss: 0.048753\n",
      "Train Epoch: 11 [19200/71432 (27%)]\tLoss: 0.270948\n",
      "Train Epoch: 11 [25600/71432 (36%)]\tLoss: 0.134912\n",
      "Train Epoch: 11 [32000/71432 (45%)]\tLoss: 0.174958\n",
      "Train Epoch: 11 [38400/71432 (54%)]\tLoss: 0.160644\n",
      "Train Epoch: 11 [44800/71432 (63%)]\tLoss: 0.133561\n",
      "Train Epoch: 11 [51200/71432 (72%)]\tLoss: 0.122680\n",
      "Train Epoch: 11 [57600/71432 (81%)]\tLoss: 0.080557\n",
      "Train Epoch: 11 [64000/71432 (90%)]\tLoss: 0.271946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [70400/71432 (98%)]\tLoss: 0.150002\n",
      "\n",
      "Test set: Average loss: 0.1182, Accuracy: 7753/8141 (95%), Positive accuracy: 3876/4091 (95%), Negative accuracy: 3877/4050 (96%), f1 score: 0.9523, Train loss: 0.1366\n",
      "\n",
      "Train Epoch: 12 [0/71432 (0%)]\tLoss: 0.095757\n",
      "Train Epoch: 12 [6400/71432 (9%)]\tLoss: 0.087062\n",
      "Train Epoch: 12 [12800/71432 (18%)]\tLoss: 0.054357\n",
      "Train Epoch: 12 [19200/71432 (27%)]\tLoss: 0.178713\n",
      "Train Epoch: 12 [25600/71432 (36%)]\tLoss: 0.216402\n",
      "Train Epoch: 12 [32000/71432 (45%)]\tLoss: 0.073902\n",
      "Train Epoch: 12 [38400/71432 (54%)]\tLoss: 0.141404\n",
      "Train Epoch: 12 [44800/71432 (63%)]\tLoss: 0.185083\n",
      "Train Epoch: 12 [51200/71432 (72%)]\tLoss: 0.179244\n",
      "Train Epoch: 12 [57600/71432 (81%)]\tLoss: 0.098940\n",
      "Train Epoch: 12 [64000/71432 (90%)]\tLoss: 0.151811\n",
      "Train Epoch: 12 [70400/71432 (98%)]\tLoss: 0.073463\n",
      "\n",
      "Test set: Average loss: 0.1192, Accuracy: 7746/8141 (95%), Positive accuracy: 3861/4091 (94%), Negative accuracy: 3885/4050 (96%), f1 score: 0.9515, Train loss: 0.1364\n",
      "\n",
      "Train Epoch: 13 [0/71432 (0%)]\tLoss: 0.136108\n",
      "Train Epoch: 13 [6400/71432 (9%)]\tLoss: 0.180498\n",
      "Train Epoch: 13 [12800/71432 (18%)]\tLoss: 0.113093\n",
      "Train Epoch: 13 [19200/71432 (27%)]\tLoss: 0.158720\n",
      "Train Epoch: 13 [25600/71432 (36%)]\tLoss: 0.056105\n",
      "Train Epoch: 13 [32000/71432 (45%)]\tLoss: 0.140744\n",
      "Train Epoch: 13 [38400/71432 (54%)]\tLoss: 0.103274\n",
      "Train Epoch: 13 [44800/71432 (63%)]\tLoss: 0.226260\n",
      "Train Epoch: 13 [51200/71432 (72%)]\tLoss: 0.073560\n",
      "Train Epoch: 13 [57600/71432 (81%)]\tLoss: 0.091134\n",
      "Train Epoch: 13 [64000/71432 (90%)]\tLoss: 0.127875\n",
      "Train Epoch: 13 [70400/71432 (98%)]\tLoss: 0.177573\n",
      "\n",
      "Test set: Average loss: 0.1179, Accuracy: 7761/8141 (95%), Positive accuracy: 3888/4091 (95%), Negative accuracy: 3873/4050 (96%), f1 score: 0.9533, Train loss: 0.1359\n",
      "\n",
      "Train Epoch: 14 [0/71432 (0%)]\tLoss: 0.129438\n",
      "Train Epoch: 14 [6400/71432 (9%)]\tLoss: 0.130128\n",
      "Train Epoch: 14 [12800/71432 (18%)]\tLoss: 0.200594\n",
      "Train Epoch: 14 [19200/71432 (27%)]\tLoss: 0.192493\n",
      "Train Epoch: 14 [25600/71432 (36%)]\tLoss: 0.164397\n",
      "Train Epoch: 14 [32000/71432 (45%)]\tLoss: 0.062227\n",
      "Train Epoch: 14 [38400/71432 (54%)]\tLoss: 0.157084\n",
      "Train Epoch: 14 [44800/71432 (63%)]\tLoss: 0.152892\n",
      "Train Epoch: 14 [51200/71432 (72%)]\tLoss: 0.137447\n",
      "Train Epoch: 14 [57600/71432 (81%)]\tLoss: 0.200480\n",
      "Train Epoch: 14 [64000/71432 (90%)]\tLoss: 0.151265\n",
      "Train Epoch: 14 [70400/71432 (98%)]\tLoss: 0.157580\n",
      "\n",
      "Test set: Average loss: 0.1181, Accuracy: 7753/8141 (95%), Positive accuracy: 3876/4091 (95%), Negative accuracy: 3877/4050 (96%), f1 score: 0.9523, Train loss: 0.1356\n",
      "\n",
      "Train Epoch: 15 [0/71432 (0%)]\tLoss: 0.159277\n",
      "Train Epoch: 15 [6400/71432 (9%)]\tLoss: 0.051979\n",
      "Train Epoch: 15 [12800/71432 (18%)]\tLoss: 0.087838\n",
      "Train Epoch: 15 [19200/71432 (27%)]\tLoss: 0.156636\n",
      "Train Epoch: 15 [25600/71432 (36%)]\tLoss: 0.098722\n",
      "Train Epoch: 15 [32000/71432 (45%)]\tLoss: 0.117296\n",
      "Train Epoch: 15 [38400/71432 (54%)]\tLoss: 0.054884\n",
      "Train Epoch: 15 [44800/71432 (63%)]\tLoss: 0.179438\n",
      "Train Epoch: 15 [51200/71432 (72%)]\tLoss: 0.083850\n",
      "Train Epoch: 15 [57600/71432 (81%)]\tLoss: 0.119676\n",
      "Train Epoch: 15 [64000/71432 (90%)]\tLoss: 0.180205\n",
      "Train Epoch: 15 [70400/71432 (98%)]\tLoss: 0.136525\n",
      "\n",
      "Test set: Average loss: 0.1175, Accuracy: 7760/8141 (95%), Positive accuracy: 3890/4091 (95%), Negative accuracy: 3870/4050 (96%), f1 score: 0.9532, Train loss: 0.1368\n",
      "\n",
      "Train Epoch: 16 [0/71432 (0%)]\tLoss: 0.218062\n",
      "Train Epoch: 16 [6400/71432 (9%)]\tLoss: 0.161739\n",
      "Train Epoch: 16 [12800/71432 (18%)]\tLoss: 0.108264\n",
      "Train Epoch: 16 [19200/71432 (27%)]\tLoss: 0.027026\n",
      "Train Epoch: 16 [25600/71432 (36%)]\tLoss: 0.207318\n",
      "Train Epoch: 16 [32000/71432 (45%)]\tLoss: 0.154826\n",
      "Train Epoch: 16 [38400/71432 (54%)]\tLoss: 0.163696\n",
      "Train Epoch: 16 [44800/71432 (63%)]\tLoss: 0.043278\n",
      "Train Epoch: 16 [51200/71432 (72%)]\tLoss: 0.066706\n",
      "Train Epoch: 16 [57600/71432 (81%)]\tLoss: 0.106392\n",
      "Train Epoch: 16 [64000/71432 (90%)]\tLoss: 0.139189\n",
      "Train Epoch: 16 [70400/71432 (98%)]\tLoss: 0.112713\n",
      "\n",
      "Test set: Average loss: 0.1178, Accuracy: 7758/8141 (95%), Positive accuracy: 3884/4091 (95%), Negative accuracy: 3874/4050 (96%), f1 score: 0.9529, Train loss: 0.1356\n",
      "\n",
      "Train Epoch: 17 [0/71432 (0%)]\tLoss: 0.113888\n",
      "Train Epoch: 17 [6400/71432 (9%)]\tLoss: 0.136599\n",
      "Train Epoch: 17 [12800/71432 (18%)]\tLoss: 0.171526\n",
      "Train Epoch: 17 [19200/71432 (27%)]\tLoss: 0.081305\n",
      "Train Epoch: 17 [25600/71432 (36%)]\tLoss: 0.222466\n",
      "Train Epoch: 17 [32000/71432 (45%)]\tLoss: 0.163328\n",
      "Train Epoch: 17 [38400/71432 (54%)]\tLoss: 0.136237\n",
      "Train Epoch: 17 [44800/71432 (63%)]\tLoss: 0.170806\n",
      "Train Epoch: 17 [51200/71432 (72%)]\tLoss: 0.093356\n",
      "Train Epoch: 17 [57600/71432 (81%)]\tLoss: 0.104392\n",
      "Train Epoch: 17 [64000/71432 (90%)]\tLoss: 0.115690\n",
      "Train Epoch: 17 [70400/71432 (98%)]\tLoss: 0.143813\n",
      "\n",
      "Test set: Average loss: 0.1184, Accuracy: 7748/8141 (95%), Positive accuracy: 3868/4091 (95%), Negative accuracy: 3880/4050 (96%), f1 score: 0.9517, Train loss: 0.1359\n",
      "\n",
      "Train Epoch: 18 [0/71432 (0%)]\tLoss: 0.164745\n",
      "Train Epoch: 18 [6400/71432 (9%)]\tLoss: 0.201513\n",
      "Train Epoch: 18 [12800/71432 (18%)]\tLoss: 0.260859\n",
      "Train Epoch: 18 [19200/71432 (27%)]\tLoss: 0.137848\n",
      "Train Epoch: 18 [25600/71432 (36%)]\tLoss: 0.169054\n",
      "Train Epoch: 18 [32000/71432 (45%)]\tLoss: 0.176174\n",
      "Train Epoch: 18 [38400/71432 (54%)]\tLoss: 0.123624\n",
      "Train Epoch: 18 [44800/71432 (63%)]\tLoss: 0.148382\n",
      "Train Epoch: 18 [51200/71432 (72%)]\tLoss: 0.069301\n",
      "Train Epoch: 18 [57600/71432 (81%)]\tLoss: 0.095589\n",
      "Train Epoch: 18 [64000/71432 (90%)]\tLoss: 0.248083\n",
      "Train Epoch: 18 [70400/71432 (98%)]\tLoss: 0.102457\n",
      "\n",
      "Test set: Average loss: 0.1181, Accuracy: 7755/8141 (95%), Positive accuracy: 3876/4091 (95%), Negative accuracy: 3879/4050 (96%), f1 score: 0.9526, Train loss: 0.1347\n",
      "\n",
      "Train Epoch: 19 [0/71432 (0%)]\tLoss: 0.090915\n",
      "Train Epoch: 19 [6400/71432 (9%)]\tLoss: 0.152030\n",
      "Train Epoch: 19 [12800/71432 (18%)]\tLoss: 0.073032\n",
      "Train Epoch: 19 [19200/71432 (27%)]\tLoss: 0.112364\n",
      "Train Epoch: 19 [25600/71432 (36%)]\tLoss: 0.119500\n",
      "Train Epoch: 19 [32000/71432 (45%)]\tLoss: 0.050380\n",
      "Train Epoch: 19 [38400/71432 (54%)]\tLoss: 0.173245\n",
      "Train Epoch: 19 [44800/71432 (63%)]\tLoss: 0.160533\n",
      "Train Epoch: 19 [51200/71432 (72%)]\tLoss: 0.209896\n",
      "Train Epoch: 19 [57600/71432 (81%)]\tLoss: 0.146953\n",
      "Train Epoch: 19 [64000/71432 (90%)]\tLoss: 0.051926\n",
      "Train Epoch: 19 [70400/71432 (98%)]\tLoss: 0.145201\n",
      "\n",
      "Test set: Average loss: 0.1180, Accuracy: 7754/8141 (95%), Positive accuracy: 3876/4091 (95%), Negative accuracy: 3878/4050 (96%), f1 score: 0.9525, Train loss: 0.1357\n",
      "\n",
      "Train Epoch: 20 [0/71432 (0%)]\tLoss: 0.096634\n",
      "Train Epoch: 20 [6400/71432 (9%)]\tLoss: 0.115606\n",
      "Train Epoch: 20 [12800/71432 (18%)]\tLoss: 0.089061\n",
      "Train Epoch: 20 [19200/71432 (27%)]\tLoss: 0.133923\n",
      "Train Epoch: 20 [25600/71432 (36%)]\tLoss: 0.175875\n",
      "Train Epoch: 20 [32000/71432 (45%)]\tLoss: 0.182372\n",
      "Train Epoch: 20 [38400/71432 (54%)]\tLoss: 0.085683\n",
      "Train Epoch: 20 [44800/71432 (63%)]\tLoss: 0.089595\n",
      "Train Epoch: 20 [51200/71432 (72%)]\tLoss: 0.188211\n",
      "Train Epoch: 20 [57600/71432 (81%)]\tLoss: 0.140887\n",
      "Train Epoch: 20 [64000/71432 (90%)]\tLoss: 0.165253\n",
      "Train Epoch: 20 [70400/71432 (98%)]\tLoss: 0.186826\n",
      "\n",
      "Test set: Average loss: 0.1179, Accuracy: 7756/8141 (95%), Positive accuracy: 3879/4091 (95%), Negative accuracy: 3877/4050 (96%), f1 score: 0.9527, Train loss: 0.1351\n",
      "\n",
      "[7186, 7662, 7716, 7727, 7748, 7751, 7758, 7748, 7759, 7759, 7753, 7746, 7761, 7753, 7760, 7758, 7748, 7755, 7754, 7756]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, 1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(6400, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2)\n",
    "#         self.fc3 = nn.Linear(128, 2)\n",
    "#         self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 3, 1)\n",
    "# #         self.conv4 = nn.Conv2d(64, 128, 3, 1)\n",
    "# #         self.conv5 = nn.Conv2d(128, 256, 3, 1)\n",
    "#         # self.fc1 = nn.Linear(9216, 128)\n",
    "# #         self.fc2 = nn.Linear(128, 2)\n",
    "#         self.fc1 = nn.Linear(2304, 1024)\n",
    "#         self.fc2 = nn.Linear(1024, 128)\n",
    "#         self.fc3 = nn.Linear(128, 2)\n",
    "# #         self.fc4 = nn.Linear(256, 64)\n",
    "# # #         self.fc5 = nn.Linear(64, 16)\n",
    "# # #         self.fc6 = nn.Linear(16, 6)\n",
    "# #         self.fc5 = nn.Linear(64, 2)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x_ori):\n",
    "        x = self.conv1(x_ori)\n",
    "        conv1 = F.relu(x)  #store conv1\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv2 = F.relu(conv2) #store conv2\n",
    "#         x = F.max_pool2d(conv2, 2)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv3 = F.relu(conv3) #store conv2\n",
    "        conv4 = self.conv4(conv3)\n",
    "        conv4 = F.relu(conv4) #store conv2\n",
    "        x = F.max_pool2d(conv4, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dropout3(x)\n",
    "#         x = self.fc3(x)\n",
    "#         x = self.conv1(x_ori)\n",
    "#         conv1 = F.relu(x)  #store conv1\n",
    "#         conv2 = self.conv2(conv1)\n",
    "#         conv2 = F.relu(conv2) #store conv2\n",
    "#         conv3 = self.conv3(conv2)\n",
    "#         conv3 = F.relu(conv3) #store conv2\n",
    "# #         conv4 = self.conv4(conv3)\n",
    "# #         conv4 = F.relu(conv4) #store conv2\n",
    "# #         conv5 = self.conv5(conv4)\n",
    "# #         conv5 = F.relu(conv5) #store conv2\n",
    "#         x = F.max_pool2d(conv3, 2)\n",
    "# #         x = self.dropout1(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "# #         x = self.dropout2(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.fc3(x)\n",
    "# #         x = F.relu(x)\n",
    "# #         x = self.fc4(x)\n",
    "# #         x = F.relu(x)\n",
    "# # #         x = self.fc5(x)\n",
    "# # #         x = F.relu(x)\n",
    "# # #         x = self.fc6(x)\n",
    "# # #         x = F.relu(x)\n",
    "# #         x = self.fc5(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    num = 0\n",
    "    loss_list = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        weight = torch.tensor([34546/(36885+34546),36885/(36885+34546)])\n",
    "        loss = F.nll_loss(output, target, weight = weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "#         if epoch == 3 and batch_idx == 10:\n",
    "#             for i in range(64):\n",
    "#                 img_ori = output[1][i].numpy()\n",
    "#                 real = target[i].detach().numpy()\n",
    "#                 predict = output[0][i].detach().numpy()\n",
    "#                 plt.imsave('./CNN/real{}_{}_a.png'.format(real,num),img_ori[0],cmap = 'gray')\n",
    "#                 img_conv1 = output[2][i].detach().numpy()\n",
    "#                 for j in range(len(img_conv1)):\n",
    "#                     plt.imsave('./CNN/real{}_{}_conv1_{}.png'.format(real, num, j),img_conv1[0],cmap = 'gray')\n",
    "#                 img_conv2 = output[3][i].detach().numpy()\n",
    "#                 for k in range(len(img_conv2)):\n",
    "#                     plt.imsave('./CNN/real{}_{}_conv2_{}.png'.format(real,num,k),img_conv2[0],cmap = 'gray')\n",
    "#                 num = num+1\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "        \n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} '.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "            if args.dry_run:\n",
    "                break\n",
    "    train_loss = sum(loss_list)/len(loss_list)\n",
    "    return train_loss\n",
    "                \n",
    "\n",
    "\n",
    "def test(model, device, test_loader,count,epoch,train_loss):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    result= [[0,0], [0,0]] \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            cmat = confusion_matrix(target.view_as(pred), pred, labels=[0, 1]) \n",
    "            result = [[result[i][j] + cmat[i][j]  for j in range(len(result[0]))] for i in range(len(result))] \n",
    "            \n",
    "            if epoch == 13:\n",
    "                wrong_idx = (pred != target.view_as(pred)).nonzero()[:, 0]\n",
    "                wrong_samples = data[wrong_idx]\n",
    "                wrong_preds = pred[wrong_idx]\n",
    "                actual_preds = target.view_as(pred)[wrong_idx]\n",
    "                for i in range(len(wrong_idx)):\n",
    "                    sample = wrong_samples[i]\n",
    "                    wrong_pred = wrong_preds[i]\n",
    "                    actual_pred = actual_preds[i]\n",
    "                    # Undo normalization\n",
    "            #         sample = sample * 0.3081\n",
    "            #         sample = sample + 0.1307\n",
    "                    sample = sample * 255.\n",
    "                    sample = sample.byte()\n",
    "                    img = TF.to_pil_image(sample)\n",
    "                    count = count+1\n",
    "                    img.save('./wrong-cnn/batch{}_i{}_actual{}.png'.format(\n",
    "                    batch_idx,wrong_idx[i], actual_pred.item()))\n",
    "                    num = batch_idx * 64 + wrong_idx[i]\n",
    "    #                 print(batch_idx,wrong_idx[i])\n",
    "                    img_ori = origin_dataset[num][0].numpy()\n",
    "                    plt.imsave('./wrong-cnn/batch{}_i{}_actual{}_ori.png'.format(\n",
    "                    batch_idx,wrong_idx[i], actual_pred.item()), img_ori[0], cmap = 'gray')\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    precision = result[1][1]/(result[1][1]+result[0][1])\n",
    "    recall = result[0][0]/(result[0][0]+result[1][0])\n",
    "    f1score = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Positive accuracy: {}/{} ({:.0f}%), Negative accuracy: {}/{} ({:.0f}%), f1 score: {:.4f}, Train loss: {:.4f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset), \n",
    "        result[1][1],result[1][1]+result[1][0],100. * result[1][1]/(result[1][1]+result[1][0]),\n",
    "        result[0][0],result[0][0]+result[0][1],100. * result[0][0]/(result[0][0]+result[0][1]),f1score,train_loss))\n",
    "    return test_loss, correct\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'batch_size': args.batch_size}\n",
    "    if use_cuda:\n",
    "        kwargs.update({'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True},\n",
    "                     )\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "    count = 0\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_accuracy_list = []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_result = test(model, device, test_loader, count, epoch,train_loss)\n",
    "        test_loss_list.append(test_result[0])\n",
    "        test_accuracy_list.append(test_result[1])\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "    plt.plot(train_loss_list, color='blue',label='train loss')  \n",
    "    plt.plot(test_loss_list, color='red',label='test loss')  \n",
    "    plt.legend()\n",
    "    print(test_accuracy_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-14-d90b47fe309a>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-d90b47fe309a>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    scale = 0\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "                figure, b = plt.subplots()\n",
    "                figure.set_size_inches(19, 19)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(model.filter_cos[i].detach().numpy()[0], cmap='gray')\n",
    "#                 np.savetxt('train-coeff.txt', model.filter_cos[i].detach().numpy()[0], delimiter='    ',fmt='%1.2f')\n",
    "#                 img.save('./filter/{}_ori_{}scale_{}.png'.format('cos',ori,scale))\n",
    "                si = model.sigma1.detach().numpy()*(2.1**scale)\n",
    "                de = model.theta1.detach().numpy()+ori*np.pi/8\n",
    "                plt.savefig(\"./filter-11.3_9/%s_scale_%.2fdeg_%.2f.png\" % ('cos',si,de), dpi=1,pad_inches=0.0,bbox_inches='tight')\n",
    "                if scale == 5:\n",
    "                    scale = 0\n",
    "                    ori = ori+1\n",
    "                else:\n",
    "                    scale = scale + 1\n",
    "\n",
    "            scale = 0\n",
    "            ori = 0\n",
    "            for i in range(len(model.filter_sin)):\n",
    "#                 sample = model.filter_sin[i]\n",
    "#                 sample = sample * 255.\n",
    "#                 sample = sample.byte()\n",
    "#                 img = TF.to_pil_image(sample)\n",
    "                figure, b = plt.subplots()\n",
    "                figure.set_size_inches(0.19, 0.19)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(model.filter_sin[i].detach().numpy()[0], cmap='gray')\n",
    "#                 np.savetxt('train-coeff.txt', model.filter_sin[i].detach().numpy()[0], delimiter='    ',fmt='%1.2f')\n",
    "                si = model.sigma1.detach().numpy()*(2.1**scale)\n",
    "                de = model.theta1.detach().numpy()+ori*np.pi/8\n",
    "#                 img.save('./filter/{}_ori_{}scale_{}.png'.ormat('sin',ori,scale))\n",
    "                plt.savefig(\"./filter-11.3_9/%s_scale_%.2fdeg_%.2f.png\" % ('sin',si,de), dpi=100,pad_inches=0.0,bbox_inches='tight')\n",
    "                if scale == 5:\n",
    "                    scale = 0\n",
    "                    ori = ori + 1\n",
    "                else:\n",
    "                    scale = scale + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-15.2498]])\n",
      "tensor([[[ 1.7825e+00],\n",
      "         [ 1.0330e+00],\n",
      "         [ 9.2251e-01],\n",
      "         [ 1.0068e+00],\n",
      "         [ 1.2314e-01],\n",
      "         [-1.1066e-02],\n",
      "         [ 1.6262e+00],\n",
      "         [ 1.3829e+00],\n",
      "         [-1.8511e+00],\n",
      "         [-1.3031e+00],\n",
      "         [ 1.6012e+00],\n",
      "         [ 2.4274e+00],\n",
      "         [ 2.9228e+00],\n",
      "         [ 1.7475e-03],\n",
      "         [-2.2883e+00],\n",
      "         [ 1.7760e+00],\n",
      "         [ 3.7275e-01],\n",
      "         [ 9.1276e-03],\n",
      "         [ 1.8626e-01],\n",
      "         [ 1.3131e+00],\n",
      "         [ 2.4674e+00],\n",
      "         [-1.5379e+00],\n",
      "         [ 8.9394e-03],\n",
      "         [ 3.4271e-01],\n",
      "         [-1.9210e+00],\n",
      "         [-1.0494e+00],\n",
      "         [-4.0725e-03],\n",
      "         [-2.0297e+00],\n",
      "         [-4.9918e-02],\n",
      "         [-2.9350e-01],\n",
      "         [ 9.6965e-01],\n",
      "         [ 1.4379e+00],\n",
      "         [ 9.0205e-01],\n",
      "         [ 5.8989e-01],\n",
      "         [ 1.9992e-05],\n",
      "         [ 1.1629e+00],\n",
      "         [-1.9780e+00],\n",
      "         [-8.0132e-01],\n",
      "         [ 2.1986e+00],\n",
      "         [ 5.4490e-01],\n",
      "         [-2.4459e-02],\n",
      "         [-1.0857e+00],\n",
      "         [ 8.0864e-01],\n",
      "         [ 4.4544e-01],\n",
      "         [ 1.3919e+00],\n",
      "         [-3.8577e-01],\n",
      "         [-3.1911e-02],\n",
      "         [ 1.7979e+00]]])\n"
     ]
    }
   ],
   "source": [
    "pretrain_gabor_model = torch.load('pretrain_gabor.pt')\n",
    "filter_cos = pretrain_gabor_model['filter_cos']\n",
    "filter_sin = pretrain_gabor_model['filter_sin']\n",
    "bias1 = pretrain_gabor_model['bias1']\n",
    "bias2 = pretrain_gabor_model['bias2']\n",
    "weights = pretrain_gabor_model['weights']\n",
    "w = pretrain_gabor_model['w']\n",
    "b = pretrain_gabor_model['b']\n",
    "print(w)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc(x): #x as a picture\n",
    "    x_cos = F.conv2d(x, filter_cos, bias=bias1)\n",
    "    x_sin = F.conv2d(x, filter_sin, bias=bias2)\n",
    "    x_comb = torch.cat((x_cos, x_sin), 2)\n",
    "\n",
    "    x_cos = x_cos.view(len(x), 1, 1, 24)\n",
    "    x_sin = x_sin.view(len(x), 1, 1, 24)\n",
    "    weighted_cos = (torch.matmul(x_cos, weights)).view(len(x), 1)\n",
    "    weighted_sin = (torch.matmul(x_sin, weights)).view(len(x), 1)\n",
    "\n",
    "    numerator = torch.norm(torch.cat([weighted_cos, weighted_sin], 1), dim=1)\n",
    "#         print(\"numerator\", numerator.size())\n",
    "    x_comb_norm = torch.norm(x_comb, dim=2)\n",
    "    x_comb_norm = x_comb_norm.view(len(x), 1, 24)\n",
    "#         print(\"x_comb_norm\", x_comb_norm.size())\n",
    "    denominator = torch.matmul(x_comb_norm, torch.abs(weights))\n",
    "    denominator = denominator.view(len(x))\n",
    "#         print(\"size:\", numerator.size(), denominator.size())\n",
    "    pc = numerator / denominator                \n",
    "#     return torch.sigmoid(w * pc + b)\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/71432 (0%)]\tLoss: 0.628911\n",
      "Train Epoch: 1 [6400/71432 (9%)]\tLoss: 0.295053\n",
      "Train Epoch: 1 [12800/71432 (18%)]\tLoss: 0.262737\n",
      "Train Epoch: 1 [19200/71432 (27%)]\tLoss: 0.184690\n",
      "Train Epoch: 1 [25600/71432 (36%)]\tLoss: 0.136053\n",
      "Train Epoch: 1 [32000/71432 (45%)]\tLoss: 0.114957\n",
      "Train Epoch: 1 [38400/71432 (54%)]\tLoss: 0.085034\n",
      "Train Epoch: 1 [44800/71432 (63%)]\tLoss: 0.084477\n",
      "Train Epoch: 1 [51200/71432 (72%)]\tLoss: 0.152466\n",
      "Train Epoch: 1 [57600/71432 (81%)]\tLoss: 0.074285\n",
      "Train Epoch: 1 [64000/71432 (90%)]\tLoss: 0.152649\n",
      "Train Epoch: 1 [70400/71432 (98%)]\tLoss: 0.175278\n",
      "\n",
      "Test set: Average loss: 0.1372, Accuracy: 7698/8141 (95%), Positive accuracy: 3737/4091 (91%), Negative accuracy: 3961/4050 (98%), f1 score: 0.9464\n",
      "\n",
      "Train Epoch: 2 [0/71432 (0%)]\tLoss: 0.093958\n",
      "Train Epoch: 2 [6400/71432 (9%)]\tLoss: 0.198442\n",
      "Train Epoch: 2 [12800/71432 (18%)]\tLoss: 0.119478\n",
      "Train Epoch: 2 [19200/71432 (27%)]\tLoss: 0.054716\n",
      "Train Epoch: 2 [25600/71432 (36%)]\tLoss: 0.035890\n",
      "Train Epoch: 2 [32000/71432 (45%)]\tLoss: 0.093249\n",
      "Train Epoch: 2 [38400/71432 (54%)]\tLoss: 0.084860\n",
      "Train Epoch: 2 [44800/71432 (63%)]\tLoss: 0.115456\n",
      "Train Epoch: 2 [51200/71432 (72%)]\tLoss: 0.027868\n",
      "Train Epoch: 2 [57600/71432 (81%)]\tLoss: 0.156310\n",
      "Train Epoch: 2 [64000/71432 (90%)]\tLoss: 0.171898\n",
      "Train Epoch: 2 [70400/71432 (98%)]\tLoss: 0.129014\n",
      "\n",
      "Test set: Average loss: 0.1187, Accuracy: 7776/8141 (96%), Positive accuracy: 3939/4091 (96%), Negative accuracy: 3837/4050 (95%), f1 score: 0.9553\n",
      "\n",
      "Train Epoch: 3 [0/71432 (0%)]\tLoss: 0.123147\n",
      "Train Epoch: 3 [6400/71432 (9%)]\tLoss: 0.064148\n",
      "Train Epoch: 3 [12800/71432 (18%)]\tLoss: 0.057013\n",
      "Train Epoch: 3 [19200/71432 (27%)]\tLoss: 0.051640\n",
      "Train Epoch: 3 [25600/71432 (36%)]\tLoss: 0.142964\n",
      "Train Epoch: 3 [32000/71432 (45%)]\tLoss: 0.052249\n",
      "Train Epoch: 3 [38400/71432 (54%)]\tLoss: 0.142406\n",
      "Train Epoch: 3 [44800/71432 (63%)]\tLoss: 0.224097\n",
      "Train Epoch: 3 [51200/71432 (72%)]\tLoss: 0.108885\n",
      "Train Epoch: 3 [57600/71432 (81%)]\tLoss: 0.118777\n",
      "Train Epoch: 3 [64000/71432 (90%)]\tLoss: 0.142764\n",
      "Train Epoch: 3 [70400/71432 (98%)]\tLoss: 0.078584\n",
      "\n",
      "Test set: Average loss: 0.1099, Accuracy: 7779/8141 (96%), Positive accuracy: 3859/4091 (94%), Negative accuracy: 3920/4050 (97%), f1 score: 0.9556\n",
      "\n",
      "Train Epoch: 4 [0/71432 (0%)]\tLoss: 0.072330\n",
      "Train Epoch: 4 [6400/71432 (9%)]\tLoss: 0.105764\n",
      "Train Epoch: 4 [12800/71432 (18%)]\tLoss: 0.096189\n",
      "Train Epoch: 4 [19200/71432 (27%)]\tLoss: 0.050050\n",
      "Train Epoch: 4 [25600/71432 (36%)]\tLoss: 0.083667\n",
      "Train Epoch: 4 [32000/71432 (45%)]\tLoss: 0.070155\n",
      "Train Epoch: 4 [38400/71432 (54%)]\tLoss: 0.088110\n",
      "Train Epoch: 4 [44800/71432 (63%)]\tLoss: 0.072408\n",
      "Train Epoch: 4 [51200/71432 (72%)]\tLoss: 0.095049\n",
      "Train Epoch: 4 [57600/71432 (81%)]\tLoss: 0.139827\n",
      "Train Epoch: 4 [64000/71432 (90%)]\tLoss: 0.208677\n",
      "Train Epoch: 4 [70400/71432 (98%)]\tLoss: 0.063020\n",
      "\n",
      "Test set: Average loss: 0.1061, Accuracy: 7800/8141 (96%), Positive accuracy: 3949/4091 (97%), Negative accuracy: 3851/4050 (95%), f1 score: 0.9582\n",
      "\n",
      "Train Epoch: 5 [0/71432 (0%)]\tLoss: 0.171333\n",
      "Train Epoch: 5 [6400/71432 (9%)]\tLoss: 0.141677\n",
      "Train Epoch: 5 [12800/71432 (18%)]\tLoss: 0.096760\n",
      "Train Epoch: 5 [19200/71432 (27%)]\tLoss: 0.040107\n",
      "Train Epoch: 5 [25600/71432 (36%)]\tLoss: 0.160389\n",
      "Train Epoch: 5 [32000/71432 (45%)]\tLoss: 0.081809\n",
      "Train Epoch: 5 [38400/71432 (54%)]\tLoss: 0.217380\n",
      "Train Epoch: 5 [44800/71432 (63%)]\tLoss: 0.066390\n",
      "Train Epoch: 5 [51200/71432 (72%)]\tLoss: 0.257002\n",
      "Train Epoch: 5 [57600/71432 (81%)]\tLoss: 0.068168\n",
      "Train Epoch: 5 [64000/71432 (90%)]\tLoss: 0.178072\n",
      "Train Epoch: 5 [70400/71432 (98%)]\tLoss: 0.091437\n",
      "\n",
      "Test set: Average loss: 0.1243, Accuracy: 7736/8141 (95%), Positive accuracy: 3764/4091 (92%), Negative accuracy: 3972/4050 (98%), f1 score: 0.9510\n",
      "\n",
      "Train Epoch: 6 [0/71432 (0%)]\tLoss: 0.093696\n",
      "Train Epoch: 6 [6400/71432 (9%)]\tLoss: 0.089412\n",
      "Train Epoch: 6 [12800/71432 (18%)]\tLoss: 0.053104\n",
      "Train Epoch: 6 [19200/71432 (27%)]\tLoss: 0.128564\n",
      "Train Epoch: 6 [25600/71432 (36%)]\tLoss: 0.061691\n",
      "Train Epoch: 6 [32000/71432 (45%)]\tLoss: 0.049206\n",
      "Train Epoch: 6 [38400/71432 (54%)]\tLoss: 0.093124\n",
      "Train Epoch: 6 [44800/71432 (63%)]\tLoss: 0.045356\n",
      "Train Epoch: 6 [51200/71432 (72%)]\tLoss: 0.117486\n",
      "Train Epoch: 6 [57600/71432 (81%)]\tLoss: 0.031344\n",
      "Train Epoch: 6 [64000/71432 (90%)]\tLoss: 0.096738\n",
      "Train Epoch: 6 [70400/71432 (98%)]\tLoss: 0.054412\n",
      "\n",
      "Test set: Average loss: 0.1038, Accuracy: 7798/8141 (96%), Positive accuracy: 3859/4091 (94%), Negative accuracy: 3939/4050 (97%), f1 score: 0.9580\n",
      "\n",
      "Train Epoch: 7 [0/71432 (0%)]\tLoss: 0.050202\n",
      "Train Epoch: 7 [6400/71432 (9%)]\tLoss: 0.164157\n",
      "Train Epoch: 7 [12800/71432 (18%)]\tLoss: 0.064871\n",
      "Train Epoch: 7 [19200/71432 (27%)]\tLoss: 0.126301\n",
      "Train Epoch: 7 [25600/71432 (36%)]\tLoss: 0.122827\n",
      "Train Epoch: 7 [32000/71432 (45%)]\tLoss: 0.087962\n",
      "Train Epoch: 7 [38400/71432 (54%)]\tLoss: 0.092184\n",
      "Train Epoch: 7 [44800/71432 (63%)]\tLoss: 0.067493\n",
      "Train Epoch: 7 [51200/71432 (72%)]\tLoss: 0.045258\n",
      "Train Epoch: 7 [57600/71432 (81%)]\tLoss: 0.087943\n",
      "Train Epoch: 7 [64000/71432 (90%)]\tLoss: 0.140034\n",
      "Train Epoch: 7 [70400/71432 (98%)]\tLoss: 0.050738\n",
      "\n",
      "Test set: Average loss: 0.1047, Accuracy: 7799/8141 (96%), Positive accuracy: 3864/4091 (94%), Negative accuracy: 3935/4050 (97%), f1 score: 0.9581\n",
      "\n",
      "Train Epoch: 8 [0/71432 (0%)]\tLoss: 0.183605\n",
      "Train Epoch: 8 [6400/71432 (9%)]\tLoss: 0.054177\n",
      "Train Epoch: 8 [12800/71432 (18%)]\tLoss: 0.045105\n",
      "Train Epoch: 8 [19200/71432 (27%)]\tLoss: 0.042597\n",
      "Train Epoch: 8 [25600/71432 (36%)]\tLoss: 0.053316\n",
      "Train Epoch: 8 [32000/71432 (45%)]\tLoss: 0.167242\n",
      "Train Epoch: 8 [38400/71432 (54%)]\tLoss: 0.154776\n",
      "Train Epoch: 8 [44800/71432 (63%)]\tLoss: 0.103956\n",
      "Train Epoch: 8 [51200/71432 (72%)]\tLoss: 0.119752\n",
      "Train Epoch: 8 [57600/71432 (81%)]\tLoss: 0.041732\n",
      "Train Epoch: 8 [64000/71432 (90%)]\tLoss: 0.074255\n",
      "Train Epoch: 8 [70400/71432 (98%)]\tLoss: 0.052287\n",
      "\n",
      "Test set: Average loss: 0.1018, Accuracy: 7808/8141 (96%), Positive accuracy: 3873/4091 (95%), Negative accuracy: 3935/4050 (97%), f1 score: 0.9592\n",
      "\n",
      "Train Epoch: 9 [0/71432 (0%)]\tLoss: 0.080405\n",
      "Train Epoch: 9 [6400/71432 (9%)]\tLoss: 0.222797\n",
      "Train Epoch: 9 [12800/71432 (18%)]\tLoss: 0.141201\n",
      "Train Epoch: 9 [19200/71432 (27%)]\tLoss: 0.082919\n",
      "Train Epoch: 9 [25600/71432 (36%)]\tLoss: 0.081865\n",
      "Train Epoch: 9 [32000/71432 (45%)]\tLoss: 0.049721\n",
      "Train Epoch: 9 [38400/71432 (54%)]\tLoss: 0.061186\n",
      "Train Epoch: 9 [44800/71432 (63%)]\tLoss: 0.032972\n",
      "Train Epoch: 9 [51200/71432 (72%)]\tLoss: 0.188676\n",
      "Train Epoch: 9 [57600/71432 (81%)]\tLoss: 0.127291\n",
      "Train Epoch: 9 [64000/71432 (90%)]\tLoss: 0.099966\n",
      "Train Epoch: 9 [70400/71432 (98%)]\tLoss: 0.034310\n",
      "\n",
      "Test set: Average loss: 0.1023, Accuracy: 7811/8141 (96%), Positive accuracy: 3874/4091 (95%), Negative accuracy: 3937/4050 (97%), f1 score: 0.9596\n",
      "\n",
      "Train Epoch: 10 [0/71432 (0%)]\tLoss: 0.072068\n",
      "Train Epoch: 10 [6400/71432 (9%)]\tLoss: 0.096265\n",
      "Train Epoch: 10 [12800/71432 (18%)]\tLoss: 0.096871\n",
      "Train Epoch: 10 [19200/71432 (27%)]\tLoss: 0.074134\n",
      "Train Epoch: 10 [25600/71432 (36%)]\tLoss: 0.187330\n",
      "Train Epoch: 10 [32000/71432 (45%)]\tLoss: 0.177263\n",
      "Train Epoch: 10 [38400/71432 (54%)]\tLoss: 0.079541\n",
      "Train Epoch: 10 [44800/71432 (63%)]\tLoss: 0.098711\n",
      "Train Epoch: 10 [51200/71432 (72%)]\tLoss: 0.055075\n",
      "Train Epoch: 10 [57600/71432 (81%)]\tLoss: 0.120037\n",
      "Train Epoch: 10 [64000/71432 (90%)]\tLoss: 0.077059\n",
      "Train Epoch: 10 [70400/71432 (98%)]\tLoss: 0.095046\n",
      "\n",
      "Test set: Average loss: 0.1002, Accuracy: 7813/8141 (96%), Positive accuracy: 3877/4091 (95%), Negative accuracy: 3936/4050 (97%), f1 score: 0.9598\n",
      "\n",
      "Train Epoch: 11 [0/71432 (0%)]\tLoss: 0.080987\n",
      "Train Epoch: 11 [6400/71432 (9%)]\tLoss: 0.082783\n",
      "Train Epoch: 11 [12800/71432 (18%)]\tLoss: 0.093649\n",
      "Train Epoch: 11 [19200/71432 (27%)]\tLoss: 0.080262\n",
      "Train Epoch: 11 [25600/71432 (36%)]\tLoss: 0.023133\n",
      "Train Epoch: 11 [32000/71432 (45%)]\tLoss: 0.088302\n",
      "Train Epoch: 11 [38400/71432 (54%)]\tLoss: 0.074070\n",
      "Train Epoch: 11 [44800/71432 (63%)]\tLoss: 0.163287\n",
      "Train Epoch: 11 [51200/71432 (72%)]\tLoss: 0.120242\n",
      "Train Epoch: 11 [57600/71432 (81%)]\tLoss: 0.184463\n",
      "Train Epoch: 11 [64000/71432 (90%)]\tLoss: 0.147104\n",
      "Train Epoch: 11 [70400/71432 (98%)]\tLoss: 0.116054\n",
      "\n",
      "Test set: Average loss: 0.0991, Accuracy: 7815/8141 (96%), Positive accuracy: 3883/4091 (95%), Negative accuracy: 3932/4050 (97%), f1 score: 0.9600\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [0/71432 (0%)]\tLoss: 0.083781\n",
      "Train Epoch: 12 [6400/71432 (9%)]\tLoss: 0.094759\n",
      "Train Epoch: 12 [12800/71432 (18%)]\tLoss: 0.058378\n",
      "Train Epoch: 12 [19200/71432 (27%)]\tLoss: 0.080779\n",
      "Train Epoch: 12 [25600/71432 (36%)]\tLoss: 0.067572\n",
      "Train Epoch: 12 [32000/71432 (45%)]\tLoss: 0.132605\n",
      "Train Epoch: 12 [38400/71432 (54%)]\tLoss: 0.112927\n",
      "Train Epoch: 12 [44800/71432 (63%)]\tLoss: 0.069503\n",
      "Train Epoch: 12 [51200/71432 (72%)]\tLoss: 0.078576\n",
      "Train Epoch: 12 [57600/71432 (81%)]\tLoss: 0.150780\n",
      "Train Epoch: 12 [64000/71432 (90%)]\tLoss: 0.130739\n",
      "Train Epoch: 12 [70400/71432 (98%)]\tLoss: 0.043161\n",
      "\n",
      "Test set: Average loss: 0.0995, Accuracy: 7814/8141 (96%), Positive accuracy: 3884/4091 (95%), Negative accuracy: 3930/4050 (97%), f1 score: 0.9599\n",
      "\n",
      "Train Epoch: 13 [0/71432 (0%)]\tLoss: 0.122115\n",
      "Train Epoch: 13 [6400/71432 (9%)]\tLoss: 0.129866\n",
      "Train Epoch: 13 [12800/71432 (18%)]\tLoss: 0.059112\n",
      "Train Epoch: 13 [19200/71432 (27%)]\tLoss: 0.114963\n",
      "Train Epoch: 13 [25600/71432 (36%)]\tLoss: 0.106861\n",
      "Train Epoch: 13 [32000/71432 (45%)]\tLoss: 0.131254\n",
      "Train Epoch: 13 [38400/71432 (54%)]\tLoss: 0.069102\n",
      "Train Epoch: 13 [44800/71432 (63%)]\tLoss: 0.086838\n",
      "Train Epoch: 13 [51200/71432 (72%)]\tLoss: 0.166093\n",
      "Train Epoch: 13 [57600/71432 (81%)]\tLoss: 0.053013\n",
      "Train Epoch: 13 [64000/71432 (90%)]\tLoss: 0.056699\n",
      "Train Epoch: 13 [70400/71432 (98%)]\tLoss: 0.119297\n",
      "\n",
      "Test set: Average loss: 0.0987, Accuracy: 7815/8141 (96%), Positive accuracy: 3884/4091 (95%), Negative accuracy: 3931/4050 (97%), f1 score: 0.9600\n",
      "\n",
      "Train Epoch: 14 [0/71432 (0%)]\tLoss: 0.114278\n",
      "Train Epoch: 14 [6400/71432 (9%)]\tLoss: 0.107919\n",
      "Train Epoch: 14 [12800/71432 (18%)]\tLoss: 0.129818\n",
      "Train Epoch: 14 [19200/71432 (27%)]\tLoss: 0.039510\n",
      "Train Epoch: 14 [25600/71432 (36%)]\tLoss: 0.042833\n",
      "Train Epoch: 14 [32000/71432 (45%)]\tLoss: 0.193245\n",
      "Train Epoch: 14 [38400/71432 (54%)]\tLoss: 0.106232\n",
      "Train Epoch: 14 [44800/71432 (63%)]\tLoss: 0.115025\n",
      "Train Epoch: 14 [51200/71432 (72%)]\tLoss: 0.165810\n",
      "Train Epoch: 14 [57600/71432 (81%)]\tLoss: 0.107673\n",
      "Train Epoch: 14 [64000/71432 (90%)]\tLoss: 0.101006\n",
      "Train Epoch: 14 [70400/71432 (98%)]\tLoss: 0.035910\n",
      "\n",
      "Test set: Average loss: 0.1013, Accuracy: 7809/8141 (96%), Positive accuracy: 3872/4091 (95%), Negative accuracy: 3937/4050 (97%), f1 score: 0.9593\n",
      "\n",
      "Train Epoch: 15 [0/71432 (0%)]\tLoss: 0.065083\n",
      "Train Epoch: 15 [6400/71432 (9%)]\tLoss: 0.093368\n",
      "Train Epoch: 15 [12800/71432 (18%)]\tLoss: 0.083543\n",
      "Train Epoch: 15 [19200/71432 (27%)]\tLoss: 0.068609\n",
      "Train Epoch: 15 [25600/71432 (36%)]\tLoss: 0.100358\n",
      "Train Epoch: 15 [32000/71432 (45%)]\tLoss: 0.042973\n",
      "Train Epoch: 15 [38400/71432 (54%)]\tLoss: 0.046921\n",
      "Train Epoch: 15 [44800/71432 (63%)]\tLoss: 0.115877\n",
      "Train Epoch: 15 [51200/71432 (72%)]\tLoss: 0.143330\n",
      "Train Epoch: 15 [57600/71432 (81%)]\tLoss: 0.102971\n",
      "Train Epoch: 15 [64000/71432 (90%)]\tLoss: 0.125153\n",
      "Train Epoch: 15 [70400/71432 (98%)]\tLoss: 0.069280\n",
      "\n",
      "Test set: Average loss: 0.0983, Accuracy: 7815/8141 (96%), Positive accuracy: 3889/4091 (95%), Negative accuracy: 3926/4050 (97%), f1 score: 0.9600\n",
      "\n",
      "Train Epoch: 16 [0/71432 (0%)]\tLoss: 0.047638\n",
      "Train Epoch: 16 [6400/71432 (9%)]\tLoss: 0.061732\n",
      "Train Epoch: 16 [12800/71432 (18%)]\tLoss: 0.047918\n",
      "Train Epoch: 16 [19200/71432 (27%)]\tLoss: 0.181946\n",
      "Train Epoch: 16 [25600/71432 (36%)]\tLoss: 0.058396\n",
      "Train Epoch: 16 [32000/71432 (45%)]\tLoss: 0.086347\n",
      "Train Epoch: 16 [38400/71432 (54%)]\tLoss: 0.104609\n",
      "Train Epoch: 16 [44800/71432 (63%)]\tLoss: 0.114340\n",
      "Train Epoch: 16 [51200/71432 (72%)]\tLoss: 0.083960\n",
      "Train Epoch: 16 [57600/71432 (81%)]\tLoss: 0.147891\n",
      "Train Epoch: 16 [64000/71432 (90%)]\tLoss: 0.144431\n",
      "Train Epoch: 16 [70400/71432 (98%)]\tLoss: 0.091007\n",
      "\n",
      "Test set: Average loss: 0.0997, Accuracy: 7817/8141 (96%), Positive accuracy: 3883/4091 (95%), Negative accuracy: 3934/4050 (97%), f1 score: 0.9603\n",
      "\n",
      "Train Epoch: 17 [0/71432 (0%)]\tLoss: 0.048393\n",
      "Train Epoch: 17 [6400/71432 (9%)]\tLoss: 0.147594\n",
      "Train Epoch: 17 [12800/71432 (18%)]\tLoss: 0.050461\n",
      "Train Epoch: 17 [19200/71432 (27%)]\tLoss: 0.093160\n",
      "Train Epoch: 17 [25600/71432 (36%)]\tLoss: 0.074909\n",
      "Train Epoch: 17 [32000/71432 (45%)]\tLoss: 0.102596\n",
      "Train Epoch: 17 [38400/71432 (54%)]\tLoss: 0.038498\n",
      "Train Epoch: 17 [44800/71432 (63%)]\tLoss: 0.105612\n",
      "Train Epoch: 17 [51200/71432 (72%)]\tLoss: 0.224443\n",
      "Train Epoch: 17 [57600/71432 (81%)]\tLoss: 0.341571\n",
      "Train Epoch: 17 [64000/71432 (90%)]\tLoss: 0.148425\n",
      "Train Epoch: 17 [70400/71432 (98%)]\tLoss: 0.069144\n",
      "\n",
      "Test set: Average loss: 0.0997, Accuracy: 7816/8141 (96%), Positive accuracy: 3883/4091 (95%), Negative accuracy: 3933/4050 (97%), f1 score: 0.9601\n",
      "\n",
      "Train Epoch: 18 [0/71432 (0%)]\tLoss: 0.305515\n",
      "Train Epoch: 18 [6400/71432 (9%)]\tLoss: 0.033998\n",
      "Train Epoch: 18 [12800/71432 (18%)]\tLoss: 0.101539\n",
      "Train Epoch: 18 [19200/71432 (27%)]\tLoss: 0.059131\n",
      "Train Epoch: 18 [25600/71432 (36%)]\tLoss: 0.060571\n",
      "Train Epoch: 18 [32000/71432 (45%)]\tLoss: 0.136280\n",
      "Train Epoch: 18 [38400/71432 (54%)]\tLoss: 0.018740\n",
      "Train Epoch: 18 [44800/71432 (63%)]\tLoss: 0.091572\n",
      "Train Epoch: 18 [51200/71432 (72%)]\tLoss: 0.180748\n",
      "Train Epoch: 18 [57600/71432 (81%)]\tLoss: 0.141591\n",
      "Train Epoch: 18 [64000/71432 (90%)]\tLoss: 0.080921\n",
      "Train Epoch: 18 [70400/71432 (98%)]\tLoss: 0.101809\n",
      "\n",
      "Test set: Average loss: 0.0995, Accuracy: 7816/8141 (96%), Positive accuracy: 3883/4091 (95%), Negative accuracy: 3933/4050 (97%), f1 score: 0.9601\n",
      "\n",
      "Train Epoch: 19 [0/71432 (0%)]\tLoss: 0.130183\n",
      "Train Epoch: 19 [6400/71432 (9%)]\tLoss: 0.097520\n",
      "Train Epoch: 19 [12800/71432 (18%)]\tLoss: 0.036642\n",
      "Train Epoch: 19 [19200/71432 (27%)]\tLoss: 0.296278\n",
      "Train Epoch: 19 [25600/71432 (36%)]\tLoss: 0.103613\n",
      "Train Epoch: 19 [32000/71432 (45%)]\tLoss: 0.123244\n",
      "Train Epoch: 19 [38400/71432 (54%)]\tLoss: 0.048501\n",
      "Train Epoch: 19 [44800/71432 (63%)]\tLoss: 0.162154\n",
      "Train Epoch: 19 [51200/71432 (72%)]\tLoss: 0.117224\n",
      "Train Epoch: 19 [57600/71432 (81%)]\tLoss: 0.064882\n",
      "Train Epoch: 19 [64000/71432 (90%)]\tLoss: 0.055350\n",
      "Train Epoch: 19 [70400/71432 (98%)]\tLoss: 0.062591\n",
      "\n",
      "Test set: Average loss: 0.0991, Accuracy: 7816/8141 (96%), Positive accuracy: 3885/4091 (95%), Negative accuracy: 3931/4050 (97%), f1 score: 0.9601\n",
      "\n",
      "Train Epoch: 20 [0/71432 (0%)]\tLoss: 0.135870\n",
      "Train Epoch: 20 [6400/71432 (9%)]\tLoss: 0.080118\n",
      "Train Epoch: 20 [12800/71432 (18%)]\tLoss: 0.196047\n",
      "Train Epoch: 20 [19200/71432 (27%)]\tLoss: 0.054095\n",
      "Train Epoch: 20 [25600/71432 (36%)]\tLoss: 0.171291\n",
      "Train Epoch: 20 [32000/71432 (45%)]\tLoss: 0.126950\n",
      "Train Epoch: 20 [38400/71432 (54%)]\tLoss: 0.061922\n",
      "Train Epoch: 20 [44800/71432 (63%)]\tLoss: 0.043741\n",
      "Train Epoch: 20 [51200/71432 (72%)]\tLoss: 0.117166\n",
      "Train Epoch: 20 [57600/71432 (81%)]\tLoss: 0.059137\n",
      "Train Epoch: 20 [64000/71432 (90%)]\tLoss: 0.066605\n",
      "Train Epoch: 20 [70400/71432 (98%)]\tLoss: 0.093014\n",
      "\n",
      "Test set: Average loss: 0.0997, Accuracy: 7815/8141 (96%), Positive accuracy: 3882/4091 (95%), Negative accuracy: 3933/4050 (97%), f1 score: 0.9600\n",
      "\n",
      "[7698, 7776, 7779, 7800, 7736, 7798, 7799, 7808, 7811, 7813, 7815, 7814, 7815, 7809, 7815, 7817, 7816, 7816, 7816, 7815]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class CNNTrainedGabor(nn.Module):\n",
    "    def __init__(self, pretrain_gabor_model):\n",
    "        super(CNNTrainedGabor, self).__init__()\n",
    "        self.pretrain_gabor_model = pretrain_gabor_model\n",
    "        self.conv1 = nn.Conv2d(1, 4, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 3, 1)\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "#         self.dropout2 = nn.Dropout2d(0.5)\n",
    "        # self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc1 = nn.Linear(72,15)\n",
    "#         self.fc2 = nn.Linear(128, 2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(16, 2)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        pc = self.pc(x).view(x.shape[0],1)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "#         x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "#         x = self.dropout2(x)\n",
    "#         print(pc.shape, x.shape)\n",
    "        x = torch.cat((pc, x), 1)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    def pc(self, x):\n",
    "        filter_cos = self.pretrain_gabor_model['filter_cos']\n",
    "        filter_sin = self.pretrain_gabor_model['filter_sin']\n",
    "        bias1 = self.pretrain_gabor_model['bias1']\n",
    "        bias2 = self.pretrain_gabor_model['bias2']\n",
    "        weights = self.pretrain_gabor_model['weights']\n",
    "        w = self.pretrain_gabor_model['w']\n",
    "        b = self.pretrain_gabor_model['b']\n",
    "        \n",
    "        x_cos = F.conv2d(x, filter_cos, bias=bias1)\n",
    "        x_sin = F.conv2d(x, filter_sin, bias=bias2)\n",
    "        x_comb = torch.cat((x_cos, x_sin), 2)\n",
    "\n",
    "        x_cos = x_cos.view(len(x), 1, 1, 48)\n",
    "        x_sin = x_sin.view(len(x), 1, 1, 48)\n",
    "        weighted_cos = (torch.matmul(x_cos, weights)).view(len(x), 1)\n",
    "        weighted_sin = (torch.matmul(x_sin, weights)).view(len(x), 1)\n",
    "\n",
    "        numerator = torch.norm(torch.cat([weighted_cos, weighted_sin], 1), dim=1)\n",
    "    #         print(\"numerator\", numerator.size())\n",
    "        x_comb_norm = torch.norm(x_comb, dim=2)\n",
    "        x_comb_norm = x_comb_norm.view(len(x), 1, 48)\n",
    "    #         print(\"x_comb_norm\", x_comb_norm.size())\n",
    "        denominator = torch.matmul(x_comb_norm, torch.abs(weights))\n",
    "        denominator = denominator.view(len(x))\n",
    "    #         print(\"size:\", numerator.size(), denominator.size())\n",
    "        pc = numerator / denominator                \n",
    "        return torch.sigmoid(w * pc + b)\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "    train_loss = sum(loss_list)/len(loss_list)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "# def test(model, device, test_loader):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             output = model(data)\n",
    "#             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "#             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "#             correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "\n",
    "#     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         test_loss, correct, len(test_loader.dataset),\n",
    "#         100. * correct / len(test_loader.dataset)))\n",
    "def test(model, device, test_loader,epoch,train_loss):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    result= [[0,0], [0,0]] \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            cmat = confusion_matrix(target.view_as(pred), pred, labels=[0, 1]) \n",
    "            result = [[result[i][j] + cmat[i][j]  for j in range(len(result[0]))] for i in range(len(result))] \n",
    "            \n",
    "            # Store wrongly predicted images\n",
    "            if epoch == 7:\n",
    "                wrong_idx = (pred != target.view_as(pred)).nonzero()[:, 0]\n",
    "                wrong_samples = data[wrong_idx]\n",
    "                wrong_preds = pred[wrong_idx]\n",
    "                actual_preds = target.view_as(pred)[wrong_idx]\n",
    "                for i in range(len(wrong_idx)):\n",
    "                    sample = wrong_samples[i]\n",
    "                    wrong_pred = wrong_preds[i]\n",
    "                    actual_pred = actual_preds[i]\n",
    "                    sample = sample * 255.\n",
    "                    sample = sample.byte()\n",
    "                    img = TF.to_pil_image(sample)\n",
    "                    img.save('./wrong-mix/epoch{}_batch{}_idx{}_actual{}.png'.format(\n",
    "                        epoch,batch_idx,wrong_idx[i], actual_pred.item()))\n",
    "                    num = batch_idx * 64 + wrong_idx[i]\n",
    "                    img_ori = origin_dataset[num][0].numpy()\n",
    "                    plt.imsave('./wrong-mix/epoch{}_batch{}_idx{}_actual{}_ori.png'.format(\n",
    "                    epoch,batch_idx,wrong_idx[i], actual_pred.item()), img_ori[0], cmap = 'gray')\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    precision = result[1][1]/(result[1][1]+result[0][1])\n",
    "    recall = result[0][0]/(result[0][0]+result[1][0])\n",
    "    f1score = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Positive accuracy: {}/{} ({:.0f}%), Negative accuracy: {}/{} ({:.0f}%), f1 score: {:.4f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset), \n",
    "        result[1][1],result[1][1]+result[1][0],100. * result[1][1]/(result[1][1]+result[1][0]),\n",
    "        result[0][0],result[0][0]+result[0][1],100. * result[0][0]/(result[0][0]+result[0][1]),f1score))\n",
    "    return test_loss, correct\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=100, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'batch_size': args.batch_size}\n",
    "    if use_cuda:\n",
    "        kwargs.update({'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True},\n",
    "                     )\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    model = CNNTrainedGabor(pretrain_gabor_model).to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_accuracy_list = []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_result = test(model, device, test_loader,epoch,train_loss)\n",
    "        test_loss_list.append(test_result[0])\n",
    "        test_accuracy_list.append(test_result[1])\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "    plt.plot(train_loss_list, color='blue',label='train loss')  \n",
    "    plt.plot(test_loss_list, color='red',label='test loss')  \n",
    "    plt.legend()\n",
    "    print(test_accuracy_list)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9808aeb850>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD7CAYAAACG50QgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV1bn/8c9DMEKZFIiKDAIWuCACAgJaBywOoF7Rar1QZ72iIipa22prW4s/e3ttq3VotVSFKhUQKcO1OHEdWqNgAjKI1QsIFBARQZnH5Pn9sXbkEDKck5whyfm+X6/9ys7a03N2Tp6zz9prr2XujoiIZId6mQ5ARETSR0lfRCSLKOmLiGQRJX0RkSyipC8ikkWU9EVEskilSd/MupjZgphpi5mNNrOeZvaumS02s/8xs6bR+u3NbGfM+k/E7KtPtP4yM3vEzCyVL05ERA5kibTTN7McYC3QH3gBuNPd3zKza4EO7v5TM2sPvOju3cvY/j3gVmAuMAt4xN1fqvarEBGRuNRPcP1BwHJ3X2VmnYG/R+WvAa8APy1vQzNrBTR19znR788AFwIVJv2WLVt6+/btEwxTRCR7zZs37wt3zytrWaJJfxgwMZpfAgwFpgPfBdrGrNfBzN4HtgD3uPs/gNbAmph11kRlFWrfvj2FhYUJhikikr3MbFV5y+K+kWtmucAFwJSo6FpgpJnNA5oAe6LydUA7dz8BuAN4rqS+P4FjjTCzQjMr3LBhQyKbiohIBRJpvTMEmO/u6wHc/SN3P9vd+xCu/pdH5bvdfWM0Py8q70y4F9AmZn9torKDuPtYd+/r7n3z8sr8hiIiIlWQSNIfzv6qHczsiOhnPeAe4Ino97zohi9m1hHoBHzi7uuALWY2IGq1cyUwIymvQkRE4hJXnb6ZNQLOAm6IKR5uZjdH838FxkXzpwFjzGwvUAzc6O6bomUjgfFAQ8IN3Cq13Nm7dy9r1qxh165dVdk86zVo0IA2bdpwyCGHZDoUEUmzhJpsZkLfvn299I3cFStW0KRJE1q0aIGa+ifG3dm4cSNbt26lQ4cOmQ5HRFLAzOa5e9+yltXKJ3J37dqlhF9FZkaLFi30LUkkS9XKpA8o4VeDzp1I9qq1Sb8mmD59OmbGRx99lOlQapyXX4b58zMdhYiUpqRfDRMnTuSUU05h4sSJla9cRUVFRSnbd6q89Racdx4MHAiLF2c6GhGJpaRfRdu2bePtt9/mqaeeYtKkSUBI0HfeeSfdu3enR48ePProowAUFBRw8skn07NnT/r168fWrVsZP348o0aN+np/559/Pm+++SYAjRs35vvf/z49e/bk3XffZcyYMZx44ol0796dESNGUHLzfdmyZZx55pn07NmT3r17s3z5cq688kqmT5/+9X4vu+wyZsxIX8vY9eth+HA49lho0gTOPx/WrUvb4UWkEkr6VTRjxgwGDx5M586dadGiBfPmzWPs2LGsXLmSBQsWsGjRIi677DL27NnDf/zHf/Dwww+zcOFCZs+eTcOGDSvc9/bt2+nfvz8LFy7klFNOYdSoURQUFPDBBx+wc+dOXnzxRSAk9JtvvpmFCxfyzjvv0KpVK6677jrGjx8PwObNm3nnnXc477zzUn06ACgqgssugy+/hBdegP/5H9i4ES64ALZvT0sIIlKJRPveqXFGj4YFC5K7z1694He/q3idiRMncttttwEwbNgwJk6cyIoVK7jxxhupXz+c1ubNm7N48WJatWrFiSeeCEDTppX3SJGTk8PFF1/89e9vvPEGDzzwADt27GDTpk0cd9xxDBw4kLVr13LRRRcBoe09wOmnn87IkSPZsGEDU6dO5eKLL/46nlS77z743/+Fp56CHj1C2aRJMHRo+DCYOhVyctISioiUo9Yn/UzYtGkTr7/+OosXL8bMKCoqwsy+TuzxqF+/PsXFxV//HtuEskGDBuRE2XHXrl2MHDmSwsJC2rZty7333ltpc8srr7ySCRMmMGnSJMaNG1fhuskyezaMGQNXXgnXXLO//PzzwwforbfCD38Iv/1tWsIRkXLU+qRf2RV5KrzwwgtcccUV/PGPf/y67PTTT6dnz5788Y9/5IwzzqB+/fps2rSJLl26sG7dOgoKCjjxxBPZunUrDRs2pH379vzhD3+guLiYtWvX8t5775V5rJIE37JlS7Zt28YLL7zAJZdcQpMmTWjTpg3Tp0/nwgsvZPfu3RQVFfGNb3yDq6++mn79+nHUUUfRrVu3lJ+PTz+F730PunaFP/wBSrcIveUWWLoUHnwQvvlNuOmmlIckIuVQnX4VTJw48etqlRIXX3wx69ato127dvTo0YOePXvy3HPPkZuby+TJk7nlllvo2bMnZ511Frt27eJb3/oWHTp0oFu3btx666307t27zGMddthhXH/99XTv3p1zzjnngG8Tzz77LI888gg9evTg5JNP5rPPPgPgyCOPpGvXrlwTe8mdIvv2wbBhoc5+yhRo1Kjs9R56KFz1jxoFL2nYHJGMqZXdMPzzn/+ka9euGYqo5tuxYwfHH3888+fPp1mzZmWuk6xz+OMfw3/9Fzz7LFx+ecXrbtsGp50Wrvrffht69qz24UWkDHWuGwYp3+zZs+natSu33HJLuQk/WWbNCgn/+usrT/gAjRuHFj3NmoWr/k8/TWl4IlKGWl+nLwc688wzWbWq3EFzkmb1arjiinC1/vDD8W/XujW8+CKccgr8+7/D3/9efpVQphQXwxtvhPmBA9XiSOoWXelLwvbsgUsvhb174fnnoZLHDg7SqxdMnhya2n7ve6F9f02wYwc88QQcdxyceWaY2reHe+4JVVIidUGtvdJ3d3UcVkXVvY9z990wZ05I3J07V20f550HjzwSbuzeeWe40Zspa9fCY4/B2LGwaRP06QMTJkBuLowbF6qw7r8/fDu55hr47nfD08a1SVER7NqV2HTEEXDGGaFaLp127YJ//APWrIHDDjt4ato0ed++3GHrVvjqq4On3buhQYMwNWy4f76iqbxHYoqLEz//9euHJtDJViuTfoMGDdi4caO6V66Ckv70Sx7mStSMGaHp5c03h6v96rj5Zli2LDS7PfbY8AGQToWF4cPm+efDP+WFF8Ltt8O3vrW/2el3vxs+FJ59FsaPh+uuC01QL7kkfACcdhrUq0Hfl4uL4W9/C1VuCxfuTyD79lVtf4ccAqeeCoMHw5Ah4VtQKv7lli0LnfS99FKoWtu5s+L1mzY9+MPg8MMP/D0np+xk/uWX++c3bw7nLFlycvZ/ANSrt//8792b+L6OOCI1Sb/S1jtm1gWYHFPUEfgZ8AZhiMTGwErgMnffEm1zN3AdUATc6u6vROWDgYeBHOBJd/9VZQGW1XpHI2dVT1VHzvrkE+jdO7S1z8+HQw+tfixFRfCd74R6/pkzwzeAVCoqgunTQ7LPzw9X7NddFx4eq2xMGffwDWfcuPAtZ8uWsM1VV4WpffvUxl6RbdvCh9LDD4cE2rYtnHtuuF9S2dVpWVexhx56YCIu6TivTZvwATB4cKj+qmpbgR074M03w75ffjkcC8J7a8iQsP+uXcM5Li9hl5fQt2w58FiNGpX/wVDRlJsbrvYTvUIvmYqKEvuWUHq9hg2hZcuqnd+KWu/g7nFPhGT9GXAMUACcHpVfC9wXzXcDFgKHAh0IA6PnRNNywodGbrROt8qO2adPH5fM27XLvU8f92bN3D/5JLn73rbNvXdv90aN3N9/P7n7LrF5s/uDD7q3b+8O7h06uD/0UCiviu3b3SdMcB80yN0s7PPb33Z/5pmwLF1WrXL/wQ/cDzssxNC/v/ukSe579iT3OGvWuD/5pPvFF7s3bRqOlZPjfuqp7r/8Zfi7FReXv31xsfs//xnO+dlnux96aNhHw4bu553n/uij7kuXJifWffvcN21y//zz5J+H2gIo9PLyeHkLylwZzgbyo/nN7P+m0Bb4MJq/G7g7ZptXgJOi6ZWY8gPWK29S0q8ZRo0K75Zp01Kz/7Vr3du0cW/dOiSYZFm+3P2229ybNAnxn3KK+9SpITEky8qV7mPGuHfsGI7RpIn7f/6n+8svu3/5ZfKOE+vdd90vvTQk3pycMP/uu6k5Vml79rj//e/uP/6x+wknhNcM7kcd5X711eFDZ+NG961b3WfMcL/xxv0ftuDetav77be7v/qq+86d6Yk52yQz6T8NjIrm3wEujObvALZG848Bl8ds8xRwSTQ9GVN+BfBYZcdU0s+8yZPDO+WOO1J7nIUL3Rs3du/VKySMRG3b5r5smfvbb4eYL7rIvV499/r13S+7zL2gIPkxxyoqcn/zzZD4GjU6MMlde6372LHuixZV/QNn797wugYMCPtt1sz9zjvD1X4mrVvnPn68+7Bh7s2bh9jq1XM/5JAw37ix+9Ch7o8/7r5iRWZjzRYVJf24n8g1s1zgU+A4d19vZv8GPAK0AGYS6u5bmNljwBx3nxBt9xRQ8uD9YHf/z6j8CqC/ux90+87MRgAjANq1a9cnHe3OpWxLl4bWLMcdF9rUJ3gbIGEvvxwe3Bo8ONw0docNG+Czz0K//J99dvBUUr5t24H7at4cbrgh3DBu3Tq1cZe2fXuo/58zB959N/zcuDEsa9IE+vWDk06CAQOgf/+K626/+gqefBIefRT+9a9w0/u22+Dqq2teK6KiIigoCHX1u3fDOeeEG+O5uZmOLLtUVKefSNIfCtzs7meXsawzMMHd+0U3cXH3/4qWvQLcG616r7ufE5UfsF55yrqRK+mxc2dITKtXw/vvQ7t26Tnu44/DyJHhZtrmzSHxl9asGRx11MFTq1b75zt3TvwZglRxh+XL938AzJkTWteUPKPQqVP4ACj5IDj+eFi5MtyYHTcufIgMHBhaF513nh4Yk4pVlPQTabI5HPh6XEAzO8LdPzezesA9hJY8EK76nzOzB4GjgU7Ae4ABncysA7AWGAZ8L9EXI+lz220hMf3tb+lL+BB64axXL4yxW1ZiP+qompPM42UWWqZ885vhSWYIiXzevP0fBK++GpqGAnzjG+FDt379MBLZ6NFwwgmZi1/qjriSvpk1As4CbogpHm5mN0fzfwXGAbj7EjN7HvgQ2Ef4dlAU7WcU4cZuDvC0uy9JyquQpJswAf70J7jrrtD0L91uuKHydWq7Ro1CO//TTgu/u8OqVfu/CRx+OIwYEb69iCRLrexlU1Jr9uxQr96vH7z+evlPGYpIzaReNiVu//hHGNO2c2eYNk0JX6SuUdKXr82dG6pyjjkGXnsNWrTIdEQikmxK+gKE1jmDB4f+PmbPhiOPzHREIpIKSvrCkiVw9tmhzffrr6e/TbuIpI+SfpZbujR0nHXIISHhH3NMpiMSkVTSbbostnIlDBoUut19663QhlxE6jYl/Sy1di18+9thAIk33oBu3TIdkYikg5J+Flq/Plzhf/FFuGnbq1emIxKRdFHSzzIbN4Y6/NWr4ZVXwgNYIpI9lPSzyFdfhVY6S5eG/nROOSXTEYlIuinpZ4mtW8MwdIsXh+ECBw3KdEQikglK+llgx47QtUJBQRgEPBMdqIlIzaCkX8ft3g0XXRSaZE6YEAYhF5HspaRfh+3dC5deGvppf+op+J5GLxDJenoit47atw8uvxxmzoTHHoNrr810RCJSEyjp11E33RTq73/96zBGrIgIKOnXSa+/HgbS/uEP4c47Mx2NiNQklSZ9M+tiZgtipi1mNtrMepnZnKis0Mz6ResPNLPNMev/LGZfg83sYzNbZmZ3pfKFZat9+8J4qsccA/fem+loRKSmqfRGrrt/DPQCMLMcwqDm04A/Ab9w95fM7FzgAWBgtNk/3P382P1E2/6eMNbuGqDAzGa6+4dJei1CuMJfvBimTKl9g4eLSOolWr0zCFju7qsAB5pG5c2ATyvZth+wzN0/cfc9wCRgaILHlwp8+SXccw+cfjpcfHGmoxGRmijRJpvDgInR/GjgFTP7DeHD4+SY9U4ys4WED4I73X0J0BpYHbPOGqB/laKWMv3iFyHx/+53YJbpaESkJor7St/McoELgClR0U3A7e7eFrgdeCoqnw8c4+49gUeB6YkGZWYjovsEhRs2bEh086z04Yehaeb116vXTBEpXyLVO0OA+e6+Pvr9KuCv0fwUQvUN7r7F3bdF87OAQ8ysJeFeQNuY/bWJyg7i7mPdva+7983Ly0sgxOzkDrffDo0bw333ZToaEanJEkn6w9lftQOh6ub0aP7bwFIAMzvKLFQuRC166gEbgQKgk5l1iL41DANmVi98gdBj5quvhtY6+owUkYrEVadvZo0IrW5uiCm+HnjYzOoDu4ARUfklwE1mtg/YCQxzdwf2mdko4BUgB3g6quuXatizB+64A/7t3/QQlohULq6k7+7bgRalyt4G+pSx7mPAY+XsZxYwK/EwpTyPPhr6x581KwxuLiJSET2RmyJFRak/xvr1MGZM6Cp5yJDUH09Eaj8l/RRwhz59QpfGu3en7jj33BP6yn/wwdQdQ0TqFiX9FFixAhYuDCNUXXJJahL//Pmhu+Rbb4UuXZK/fxGpm5T0UyA/P/y85RZ48cXQp/2ePcnbvzvcdhu0bAk//Wny9isidZ+Sfgrk50PTpvDQQ/D734c+7ZOZ+KdMgbffhvvvh8MOS84+RSQ7KOmnQH4+nHQS5OTAyJGhhc2MGTBsWBjNqjp27IAf/CA8dauBUUQkUUr6SfbVV7BkCXzrW/vLRo2Chx+GadNg+PDqJf7f/Ab+9a+wv5yc6scrItlFST/J3n031LnHJn0IN1wfegimTg1j1VYl8a9eDb/6FXz3u3DaacmJV0SyiwZGT7L8/HAF3r+M/kNHjw4fCHfcEXrBfO45qJ/AX+BHPwrbP/BA8uIVkeyipJ9k+fmhvr1Ro7KX3347FBeHYQzr1YMJE+JL/G+/DRMnhtY67dsnNWQRySJK+km0dy/MnRu6N67I978fEv8Pfxiu+J99tuLEX1wcviW0bh2u9kVEqkpJP4kWLICdOw+uzy/LD34Qqmp+9KNwxf/MM+XfmP3zn2HePPjLX8r/BiEiEg8l/SQqeSgrnqQP4Uq/uBjuvjsk/vHjD078W7aE5SefHFr+iIhUh5J+EuXnwzHHhGqYeN11V0j8P/lJqOoZN+7AxH///aFjtRdf1BCIIlJ9SvpJ4h6S/sCBiW/74x+HxP/Tn4Yr/qeeCol/6dLQzPPqq6Fv32RHLCLZSEk/SVauhHXr4q/aKe2ee0Li//nPQ+J/8snQwufQQ+GXv0xqqCKSxSpN+mbWBZgcU9QR+BnwJvAE0ADYB4x09/eioRIfBs4FdgBXu/v8aF9XAfdE+/l/7v7nJL2OjEu0Pr8sP/tZSPy/+EXoqfPNN8PDWK1aJSVEEZHKk767fwz0AjCzHMJg5tOAPwG/cPeXzOxc4AFgIGEA9U7R1B94HOhvZs2BnwN9AQfmmdlMd/8y2S8qE955B5o0geOPr95+fv7zkPjvuw+OPTY01RQRSZZEq3cGAcvdfZWZOdA0Km9GGCgdYCjwTDQu7hwzO8zMWhE+EF5z900AZvYaMJgDB1uvtfLzYcCA6veHYxau9Dt3hh49QvWOiEiyJJr0h7E/SY8GXjGz3xD68Dk5Km8NrI7ZZk1UVl55rbd5MyxeDN/5TnL2ZwaXX56cfYmIxIq7wzUzywUuAKZERTcBt7t7W+B24KlkBWVmI8ys0MwKN2zYkKzdpsycOWV3siYiUtMk0svmEGC+u6+Pfr8K+Gs0PwXoF82vBdrGbNcmKiuv/CDuPtbd+7p737y8vARCzIz8/NDipqxO1kREapJEkv5wDqx//xQ4PZr/NrA0mp8JXGnBAGCzu68DXgHONrPDzexw4OyorNbLz4eePcONXBGRmiyuOn0zawScBdwQU3w98LCZ1Qd2ASOi8lmE5prLCE02rwFw901mdh9QEK03puSmbm22b1/oZO2aazIdiYhI5eJK+u6+HWhRquxtoE8Z6zpwczn7eRp4OvEwa66FC2H7dtXni0jtoJGzqikZD2WJiKSLkn415edD27ZhEhGp6ZT0q6GkkzVd5YtIbaGkXw3/+hesXRv6uhcRqQ2U9KtB9fkiUtso6VdDfn4YvrBHj0xHIiISHyX9aijpZK2iQc1FRGoSJf0q2rIldLKmqh0RqU2U9Kto7tzQ772SvojUJkr6VVTSydqAAZmOREQkfkr6VZSfH0bJatq08nVFRGoKJf0q2Lcv9KGvqh0RqW2U9Ktg8WLYtk1JX0RqHyX9KtBDWSJSWynpV0F+PrRuDe3aZToSEZHEKOlXQUkna2aZjkREJDGVJn0z62JmC2KmLWY22swmx5StNLMF0frtzWxnzLInYvbVx8wWm9kyM3vErPalzdWrw6SqHRGpjSrtQMDdPwZ6AZhZDmEw82nu/ruSdczst8DmmM2Wu3uvMnb3OGGYxbmEYRUHAy9VOfoMKKnPV8+aIlIbJVq9M4iQ0FeVFERX65dy4KDpBzGzVkBTd58TDan4DHBhgsfPuPx8+MY3wkDoIiK1TaJJfxgHJ/dTgfXuvjSmrIOZvW9mb5nZqVFZa2BNzDprorJaJT8f+veHQw7JdCQiIomLO+mbWS5wATCl1KLhHPhBsA5o5+4nAHcAz5lZQs+tmtkIMys0s8INGzYksmlKbd0aBkJXfb6I1FaJXOkPAea7+/qSAjOrD3wHmFxS5u673X1jND8PWA50JtwLaBOzvzZR2UHcfay793X3vnl5eQmEmFrqZE1EartEkn7pK3qAM4GP3P3rahszy4tu+GJmHYFOwCfuvg7YYmYDovsAVwIzqhV9muXnh2aaJ52U6UhERKomruE/zKwRcBZwQ6lFZdXxnwaMMbO9QDFwo7tvipaNBMYDDQmtdmpdy53u3aFZs0xHIiJSNXElfXffDrQoo/zqMsqmAlPL2U8h0D2xEGuGoqLQydpll2U6EhGRqtMTuXH64INwI1f1+SJSmynpx0mdrIlIXaCkH6f8fGjVCtq3z3QkIiJVp6QfJ3WyJiJ1gZJ+HNauhVWrVLUjIrWfkn4c1MmaiNQVSvpxyM+Hhg3hhBMyHYmISPUo6cchPx/69VMnayJS+ynpV2LbNliwQPX5IlI3KOlX4r33wtO4SvoiUhco6Vei5CauOlkTkbpASb8S+flw3HFw+OGZjkREpPqU9CtQVATvvquqHRGpO5T0K7BkCWzZoqQvInWHkn4F1MmaiNQ1SvoVyM+HI4+Ejh0zHYmISHJUmvTNrIuZLYiZtpjZaDObHFO20swWxGxzt5ktM7OPzeycmPLBUdkyM7srVS8qWd55R52siUjdUunIWe7+MdALIBr7di0wzd1/V7KOmf0W2BzNdyMMo3gccDQw28w6R6v+njDs4hqgwMxmuvuHyXs5ybNuHaxYAaNGZToSEZHkiWu4xBiDgOXuvqqkIBrk/FLg21HRUGCSu+8GVpjZMqBftGyZu38SbTcpWrdGJn3V54tIXZRonX5ZA6GfCqx396XR762B1THL10Rl5ZXXSPn50KCBOlkTkbol7qRvZrnABcCUUouGc/AHQbWY2QgzKzSzwg0bNiRz13HLz4cTT4Tc3IwcXkQkJRK50h8CzHf39SUFZlYf+A4wOWa9tUDbmN/bRGXllR/E3ce6e19375uXl5dAiMmxaxe8/776zxeRuieRpF/WFf2ZwEfuviambCYwzMwONbMOQCfgPaAA6GRmHaJvDcOidWucRYtg375wpS8iUpfEdSPXzBoRWt3cUGrRQXX87r7EzJ4n3KDdB9zs7kXRfkYBrwA5wNPuvqR64adGYWH42bdvZuMQEUm2uJK+u28HWpRRfnU5698P3F9G+SxgVmIhpl9hIeTlQbt2mY5ERCS59ERuGQoLw1W+HsoSkbpGSb+U7dtDR2uq2hGRukhJv5QFC6C4WElfROomJf1SdBNXROoyJf1SCguhVSs4+uhMRyIiknxK+qWU3MQVEamLlPRjbNkCH3+spC8idZeSfoz33wd3PYkrInWXkn6Mkpu4ffpkNg4RkVRR0o9RUBCewj3iiExHIiKSGkr6MXQTV0TqOiX9yJdfwvLlSvoiUrcp6UfmzQs/lfRFpC5T0o/oJq6IZAMl/UhhIRx7LDRvnulIRERSR0k/opu4IpINKk36ZtbFzBbETFvMbHS07BYz+8jMlpjZA1FZezPbGbP+EzH76mNmi81smZk9YlYzeqzfsAFWrVLSF5G6r9KRs9z9Y6AXgJnlEAYzn2ZmZwBDgZ7uvtvMYlu3L3f3XmXs7nHgemAuYQStwcBL1XsJ1aebuCKSLRKt3hlESOirgJuAX7n7bgB3/7yiDc2sFdDU3ee4uwPPABdWIeakKygIP3v3zmwcIiKplmjSjx0IvTNwqpnNNbO3zCy2x5oOZvZ+VH5qVNYaWBOzzpqoLOMKC6FLF2jaNNORiIikVlwDowOYWS5wAXB3zLbNgQHAicDzZtYRWAe0c/eNZtYHmG5mxyUSlJmNAEYAtEvD6OSFhXDGGSk/jIhIxiVypT8EmO/u66Pf1wB/9eA9oBho6e673X0jgLvPA5YTvhWsBdrE7K9NVHYQdx/r7n3dvW9eXl5iryhBn34aJtXni0g2SCTpD2d/1Q7AdOAMADPrDOQCX5hZXnTDl+jKvxPwibuvA7aY2YCo1c6VwIwkvIZqKbmJq+6URSQbxFW9Y2aNgLOAG2KKnwaeNrMPgD3AVe7uZnYaMMbM9hKu/m90903RNiOB8UBDQqudjLfcKSyEevWgV1ltjURE6pi4kr67bwdalCrbA1xexrpTganl7KcQ6J54mKlTWAjdukGjRpmOREQk9bL6iVx3PYkrItklq5P+6tXw+edK+iKSPbI66Zf0rKmkLyLZIuuTfv360LNnpiMREUmPrE/6xx8PDRpkOhIRkfTI2qSvm7giko2yNumvWBHGxVXSF5FskrVJXzdxRSQbZXXSz82F7jXqUTERkdTK2qRfUBBa7eTmZjoSEZH0ycqkX1wcOlpT1Y6IZJusTPpLl8LWrepZU0SyT1Ymfd3EFZFslbVJv2FD6No105GIiKRX1ib9E04IXTCIiGSTrEv6RUUwf76qdkQkO2Vd0pe9OkQAAAoBSURBVP/oI9ixQ0lfRLJTpUnfzLqY2YKYaYuZjY6W3WJmH5nZEjN7IGabu81smZl9bGbnxJQPjsqWmdldqXlJFSsoCD+V9EUkG1Vaq+3uHwO9AKIBz9cC08zsDGAo0NPdd5vZEdE63YBhwHHA0cDsaOB0gN8TxtpdAxSY2Ux3/zDJr6lChYXQuDF07lz5uiIidU2itzIHAcvdfZWZ/Rr4lbvvBnD3z6N1hgKTovIVZrYM6BctW+bunwCY2aRo3bQn/T59ICcnnUcVEakZEq3THwZMjOY7A6ea2Vwze8vMSh51ag2sjtlmTVRWXnna7N0LCxaoakdEslfcSd/McoELgClRUX2gOTAA+AHwvJlZMoIysxFmVmhmhRs2bEjGLgFYsgR271bSF5HslciV/hBgvruvj35fA/zVg/eAYqAloc6/bcx2baKy8soP4u5j3b2vu/fNy8tLIMSK6UlcEcl2iST94eyv2gGYDpwBEN2ozQW+AGYCw8zsUDPrAHQC3gMKgE5m1iH61jAsWjdtCguhWTM49th0HlVEpOaI60aumTUitLq5Iab4aeBpM/sA2ANc5e4OLDGz5wk3aPcBN7t7UbSfUcArQA7wtLsvSdoriUPJ8IjJqYQSEal94kr67r4daFGqbA9weTnr3w/cX0b5LGBW4mFW3+7dsGgR3HFHJo4uIlIzZM0TuYsWhdY76k5ZRLJZ1iR93cQVEcmypN+yJbRrl+lIREQyJ6uSvm7iiki2y4qkv2NHeDBLVTsiku2yIukvXBj60VfSF5FslxVJXzdxRUSCrEn6Rx0FRx+d6UhERDIrK5J+QUFon6+buCKS7ep80t+6NQyRqKodEZEsSPrvvw/uSvoiIpAFSb/kJm6fPpmNQ0SkJsiKpN+2LRx5ZKYjERHJvKxI+qraEREJ6nTS/+orWLpUSV9EpESdTvrz54ef6k5ZRCSoNOmbWRczWxAzbTGz0WZ2r5mtjSk/N1q/vZntjCl/ImZffcxssZktM7NHkjWQenkKCsJP3cQVEQkqHTnL3T8GegGYWQ5hMPNpwDXAQ+7+mzI2W+7uvcoofxy4HphLGEFrMPBS1UKvXGEhdOwIzZun6ggiIrVLotU7gwgJfVWiBzKzVkBTd58TjaX7DHBhovtJhG7iiogcKNGkPwyYGPP7KDNbZGZPm9nhMeUdzOx9M3vLzE6NyloDa2LWWROVpcQXX8DKlUr6IiKx4k76ZpYLXABMiYoeB44lVP2sA34bla8D2rn7CcAdwHNm1jSRoMxshJkVmlnhhg0bEtn0a/PmhZ9K+iIi+yVypT8EmO/u6wHcfb27F7l7MfAnoF9UvtvdN0bz84DlQGfCvYA2MftrE5UdxN3Huntfd++bl5eX6GsC9j+J27t3lTYXEamTEkn6w4mp2onq6EtcBHwQledFN3wxs45AJ+ATd18HbDGzAVGrnSuBGdWMv1yFhdC5MzRrlqojiIjUPpW23gEws0bAWcANMcUPmFkvwIGVMctOA8aY2V6gGLjR3TdFy0YC44GGhFY7KW25c/rpqdq7iEjtFFfSd/ftQItSZVeUs+5UYGo5ywqB7gnGmLA9e+DMM8MkIiL7xZX0a5vcXBg3LtNRiIjUPHW6GwYRETmQkr6ISBZR0hcRySJK+iIiWURJX0Qkiyjpi4hkESV9EZEsoqQvIpJFLHRtX3OZ2QYg4f77Iy2BL5IYTrIpvupRfNWj+KqnJsd3jLuX2VtljU/61WFmhe5eYztXVnzVo/iqR/FVT02Przyq3hERySJK+iIiWaSuJ/2xmQ6gEoqvehRf9Si+6qnp8ZWpTtfpi4jIger6lb6IiMSoE0nfzAab2cdmtszM7ipj+aFmNjlaPtfM2qcxtrZm9oaZfWhmS8zstjLWGWhmm81sQTT9LF3xRcdfaWaLo2MXlrHczOyR6PwtMrO0jTxsZl1izssCM9tiZqNLrZPW82dmT5vZ52b2QUxZczN7zcyWRj8PL2fbq6J1lprZVWmM79dm9lH095tmZoeVs22F74UUxnevma2N+RueW862Ff6vpzC+yTGxrTSzBeVsm/LzV23uXqsnIIcw+HpHIBdYCHQrtc5I4IlofhgwOY3xtQJ6R/NNgP8rI76BwIsZPIcrgZYVLD+XMLSlAQOAuRn8W39GaIOcsfNHGBK0N/BBTNkDwF3R/F3Af5exXXPgk+jn4dH84WmK72ygfjT/32XFF897IYXx3QvcGcffv8L/9VTFV2r5b4GfZer8VXeqC1f6/YBl7v6Ju+8BJgFDS60zFPhzNP8CMCganD3l3H2du8+P5rcC/wRap+PYSTQUeMaDOcBhZtYqA3EMApa7e1Uf1ksKd/87sKlUcex77M/AhWVseg7wmrtvcvcvgdeAwemIz91fdfd90a9zgDbJPm68yjl/8Yjnf73aKoovyhuXAhOTfdx0qQtJvzWwOub3NRycVL9eJ3rjb6bUmL/pEFUrnQDMLWPxSWa20MxeMrPj0hpYGNz+VTObZ2YjylgezzlOh2GU/8+WyfMHcKS7r4vmPwOOLGOdmnIeryV8cytLZe+FVBoVVT89XU71WE04f6cC6919aTnLM3n+4lIXkn6tYGaNCQPGj3b3LaUWzydUWfQEHgWmpzm8U9y9NzAEuNnMTkvz8StlZrnABcCUMhZn+vwdwMP3/BrZLM7MfgLsA/5SziqZei88DhwL9ALWEapQaqLhVHyVX+P/l+pC0l8LtI35vU1UVuY6ZlYfaAZsTEt04ZiHEBL+X9z9r6WXu/sWd98Wzc8CDjGzlumKz93XRj8/B6YRvkbHiuccp9oQYL67ry+9INPnL7K+pMor+vl5Getk9Dya2dXA+cBl0QfTQeJ4L6SEu6939yJ3Lwb+VM5xM33+6gPfASaXt06mzl8i6kLSLwA6mVmH6GpwGDCz1DozgZKWEpcAr5f3pk+2qA7wKeCf7v5gOescVXKPwcz6Ef4uaflQMrNGZtakZJ5ww++DUqvNBK6MWvEMADbHVGWkS7lXWJk8fzFi32NXATPKWOcV4GwzOzyqvjg7Kks5MxsM/BC4wN13lLNOPO+FVMUXe4/oonKOG8//eiqdCXzk7mvKWpjJ85eQTN9JTsZEaF3yf4Q7+z+JysYQ3uAADQjVAsuA94COaYztFMJX/UXAgmg6F7gRuDFaZxSwhNAaYQ5wchrj6xgdd2EUQ8n5i43PgN9H53cx0DfNf99GhCTeLKYsY+eP8OGzDthLqFe+jnCP6H+BpcBsoHm0bl/gyZhtr43eh8uAa9IY3zJCfXjJe7CkNdvRwKyK3gtpiu/Z6L21iJDIW5WOL/r9oP/1dMQXlY8vec/FrJv281fdSU/kiohkkbpQvSMiInFS0hcRySJK+iIiWURJX0Qkiyjpi4hkESV9EZEsoqQvIpJFlPRFRLLI/wc1qljKhzkEQgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [7542, 7698, 7836, 7804, 7859, 7892, 7934, 7897, 7900, 7914, 7902, 7895, 7909, 7910, 7901, 7910, 7908, 7910, 7909, 7910]\n",
    "plt.plot(x, color='blue',label='Accuracy')  \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/71432 (0%)]\tLoss: 0.723132\n",
      "Train Epoch: 1 [6400/71432 (9%)]\tLoss: 0.181368\n",
      "Train Epoch: 1 [12800/71432 (18%)]\tLoss: 0.164086\n",
      "Train Epoch: 1 [19200/71432 (27%)]\tLoss: 0.299986\n",
      "Train Epoch: 1 [25600/71432 (36%)]\tLoss: 0.116206\n",
      "Train Epoch: 1 [32000/71432 (45%)]\tLoss: 0.142081\n",
      "Train Epoch: 1 [38400/71432 (54%)]\tLoss: 0.098413\n",
      "Train Epoch: 1 [44800/71432 (63%)]\tLoss: 0.165801\n",
      "Train Epoch: 1 [51200/71432 (72%)]\tLoss: 0.157562\n",
      "Train Epoch: 1 [57600/71432 (81%)]\tLoss: 0.093857\n",
      "Train Epoch: 1 [64000/71432 (90%)]\tLoss: 0.143686\n",
      "Train Epoch: 1 [70400/71432 (98%)]\tLoss: 0.099544\n",
      "\n",
      "Test set: Average loss: 0.1291, Accuracy: 7689/8141 (94%), Positive accuracy: 3794/4091 (93%), Negative accuracy: 3895/4050 (96%), f1 score: 0.9447\n",
      "\n",
      "Train Epoch: 2 [0/71432 (0%)]\tLoss: 0.172338\n",
      "Train Epoch: 2 [6400/71432 (9%)]\tLoss: 0.093837\n",
      "Train Epoch: 2 [12800/71432 (18%)]\tLoss: 0.047705\n",
      "Train Epoch: 2 [19200/71432 (27%)]\tLoss: 0.102612\n",
      "Train Epoch: 2 [25600/71432 (36%)]\tLoss: 0.120843\n",
      "Train Epoch: 2 [32000/71432 (45%)]\tLoss: 0.053516\n",
      "Train Epoch: 2 [38400/71432 (54%)]\tLoss: 0.099203\n",
      "Train Epoch: 2 [44800/71432 (63%)]\tLoss: 0.140209\n",
      "Train Epoch: 2 [51200/71432 (72%)]\tLoss: 0.076230\n",
      "Train Epoch: 2 [57600/71432 (81%)]\tLoss: 0.057741\n",
      "Train Epoch: 2 [64000/71432 (90%)]\tLoss: 0.089737\n",
      "Train Epoch: 2 [70400/71432 (98%)]\tLoss: 0.108630\n",
      "\n",
      "Test set: Average loss: 0.1351, Accuracy: 7686/8141 (94%), Positive accuracy: 3955/4091 (97%), Negative accuracy: 3731/4050 (92%), f1 score: 0.9447\n",
      "\n",
      "Train Epoch: 3 [0/71432 (0%)]\tLoss: 0.161428\n",
      "Train Epoch: 3 [6400/71432 (9%)]\tLoss: 0.076482\n",
      "Train Epoch: 3 [12800/71432 (18%)]\tLoss: 0.081578\n",
      "Train Epoch: 3 [19200/71432 (27%)]\tLoss: 0.129778\n",
      "Train Epoch: 3 [25600/71432 (36%)]\tLoss: 0.104735\n",
      "Train Epoch: 3 [32000/71432 (45%)]\tLoss: 0.041063\n",
      "Train Epoch: 3 [38400/71432 (54%)]\tLoss: 0.110686\n",
      "Train Epoch: 3 [44800/71432 (63%)]\tLoss: 0.086706\n",
      "Train Epoch: 3 [51200/71432 (72%)]\tLoss: 0.107425\n",
      "Train Epoch: 3 [57600/71432 (81%)]\tLoss: 0.130207\n",
      "Train Epoch: 3 [64000/71432 (90%)]\tLoss: 0.112593\n",
      "Train Epoch: 3 [70400/71432 (98%)]\tLoss: 0.083332\n",
      "\n",
      "Test set: Average loss: 0.1083, Accuracy: 7771/8141 (95%), Positive accuracy: 3877/4091 (95%), Negative accuracy: 3894/4050 (96%), f1 score: 0.9546\n",
      "\n",
      "Train Epoch: 4 [0/71432 (0%)]\tLoss: 0.057797\n",
      "Train Epoch: 4 [6400/71432 (9%)]\tLoss: 0.101003\n",
      "Train Epoch: 4 [12800/71432 (18%)]\tLoss: 0.108890\n",
      "Train Epoch: 4 [19200/71432 (27%)]\tLoss: 0.166451\n",
      "Train Epoch: 4 [25600/71432 (36%)]\tLoss: 0.055861\n",
      "Train Epoch: 4 [32000/71432 (45%)]\tLoss: 0.050115\n",
      "Train Epoch: 4 [38400/71432 (54%)]\tLoss: 0.012048\n",
      "Train Epoch: 4 [44800/71432 (63%)]\tLoss: 0.053490\n",
      "Train Epoch: 4 [51200/71432 (72%)]\tLoss: 0.052022\n",
      "Train Epoch: 4 [57600/71432 (81%)]\tLoss: 0.042562\n",
      "Train Epoch: 4 [64000/71432 (90%)]\tLoss: 0.061815\n",
      "Train Epoch: 4 [70400/71432 (98%)]\tLoss: 0.117868\n",
      "\n",
      "Test set: Average loss: 0.1151, Accuracy: 7779/8141 (96%), Positive accuracy: 3799/4091 (93%), Negative accuracy: 3980/4050 (98%), f1 score: 0.9561\n",
      "\n",
      "Train Epoch: 5 [0/71432 (0%)]\tLoss: 0.141853\n",
      "Train Epoch: 5 [6400/71432 (9%)]\tLoss: 0.039673\n",
      "Train Epoch: 5 [12800/71432 (18%)]\tLoss: 0.056235\n",
      "Train Epoch: 5 [19200/71432 (27%)]\tLoss: 0.059102\n",
      "Train Epoch: 5 [25600/71432 (36%)]\tLoss: 0.058170\n",
      "Train Epoch: 5 [32000/71432 (45%)]\tLoss: 0.129347\n",
      "Train Epoch: 5 [38400/71432 (54%)]\tLoss: 0.112623\n",
      "Train Epoch: 5 [44800/71432 (63%)]\tLoss: 0.027189\n",
      "Train Epoch: 5 [51200/71432 (72%)]\tLoss: 0.053099\n",
      "Train Epoch: 5 [57600/71432 (81%)]\tLoss: 0.075908\n",
      "Train Epoch: 5 [64000/71432 (90%)]\tLoss: 0.149355\n",
      "Train Epoch: 5 [70400/71432 (98%)]\tLoss: 0.028692\n",
      "\n",
      "Test set: Average loss: 0.0977, Accuracy: 7816/8141 (96%), Positive accuracy: 3867/4091 (95%), Negative accuracy: 3949/4050 (98%), f1 score: 0.9602\n",
      "\n",
      "Train Epoch: 6 [0/71432 (0%)]\tLoss: 0.128027\n",
      "Train Epoch: 6 [6400/71432 (9%)]\tLoss: 0.064306\n",
      "Train Epoch: 6 [12800/71432 (18%)]\tLoss: 0.054031\n",
      "Train Epoch: 6 [19200/71432 (27%)]\tLoss: 0.085537\n",
      "Train Epoch: 6 [25600/71432 (36%)]\tLoss: 0.065953\n",
      "Train Epoch: 6 [32000/71432 (45%)]\tLoss: 0.101847\n",
      "Train Epoch: 6 [38400/71432 (54%)]\tLoss: 0.108428\n",
      "Train Epoch: 6 [44800/71432 (63%)]\tLoss: 0.088830\n",
      "Train Epoch: 6 [51200/71432 (72%)]\tLoss: 0.048968\n",
      "Train Epoch: 6 [57600/71432 (81%)]\tLoss: 0.087203\n",
      "Train Epoch: 6 [64000/71432 (90%)]\tLoss: 0.068856\n",
      "Train Epoch: 6 [70400/71432 (98%)]\tLoss: 0.059755\n",
      "\n",
      "Test set: Average loss: 0.0876, Accuracy: 7863/8141 (97%), Positive accuracy: 3896/4091 (95%), Negative accuracy: 3967/4050 (98%), f1 score: 0.9660\n",
      "\n",
      "Train Epoch: 7 [0/71432 (0%)]\tLoss: 0.040751\n",
      "Train Epoch: 7 [6400/71432 (9%)]\tLoss: 0.042035\n",
      "Train Epoch: 7 [12800/71432 (18%)]\tLoss: 0.044927\n",
      "Train Epoch: 7 [19200/71432 (27%)]\tLoss: 0.124835\n",
      "Train Epoch: 7 [25600/71432 (36%)]\tLoss: 0.054911\n",
      "Train Epoch: 7 [32000/71432 (45%)]\tLoss: 0.078016\n",
      "Train Epoch: 7 [38400/71432 (54%)]\tLoss: 0.053384\n",
      "Train Epoch: 7 [44800/71432 (63%)]\tLoss: 0.051804\n",
      "Train Epoch: 7 [51200/71432 (72%)]\tLoss: 0.080125\n",
      "Train Epoch: 7 [57600/71432 (81%)]\tLoss: 0.054541\n",
      "Train Epoch: 7 [64000/71432 (90%)]\tLoss: 0.015894\n",
      "Train Epoch: 7 [70400/71432 (98%)]\tLoss: 0.117955\n",
      "\n",
      "Test set: Average loss: 0.0851, Accuracy: 7865/8141 (97%), Positive accuracy: 3930/4091 (96%), Negative accuracy: 3935/4050 (97%), f1 score: 0.9661\n",
      "\n",
      "Train Epoch: 8 [0/71432 (0%)]\tLoss: 0.059791\n",
      "Train Epoch: 8 [6400/71432 (9%)]\tLoss: 0.095556\n",
      "Train Epoch: 8 [12800/71432 (18%)]\tLoss: 0.060632\n",
      "Train Epoch: 8 [19200/71432 (27%)]\tLoss: 0.054446\n",
      "Train Epoch: 8 [25600/71432 (36%)]\tLoss: 0.044928\n",
      "Train Epoch: 8 [32000/71432 (45%)]\tLoss: 0.122400\n",
      "Train Epoch: 8 [38400/71432 (54%)]\tLoss: 0.092461\n",
      "Train Epoch: 8 [44800/71432 (63%)]\tLoss: 0.044232\n",
      "Train Epoch: 8 [51200/71432 (72%)]\tLoss: 0.091392\n",
      "Train Epoch: 8 [57600/71432 (81%)]\tLoss: 0.100112\n",
      "Train Epoch: 8 [64000/71432 (90%)]\tLoss: 0.053660\n",
      "Train Epoch: 8 [70400/71432 (98%)]\tLoss: 0.120569\n",
      "\n",
      "Test set: Average loss: 0.0871, Accuracy: 7860/8141 (97%), Positive accuracy: 3899/4091 (95%), Negative accuracy: 3961/4050 (98%), f1 score: 0.9656\n",
      "\n",
      "Train Epoch: 9 [0/71432 (0%)]\tLoss: 0.062622\n",
      "Train Epoch: 9 [6400/71432 (9%)]\tLoss: 0.161946\n",
      "Train Epoch: 9 [12800/71432 (18%)]\tLoss: 0.011290\n",
      "Train Epoch: 9 [19200/71432 (27%)]\tLoss: 0.084209\n",
      "Train Epoch: 9 [25600/71432 (36%)]\tLoss: 0.052149\n",
      "Train Epoch: 9 [32000/71432 (45%)]\tLoss: 0.090034\n",
      "Train Epoch: 9 [38400/71432 (54%)]\tLoss: 0.062903\n",
      "Train Epoch: 9 [44800/71432 (63%)]\tLoss: 0.203646\n",
      "Train Epoch: 9 [51200/71432 (72%)]\tLoss: 0.030909\n",
      "Train Epoch: 9 [57600/71432 (81%)]\tLoss: 0.046202\n",
      "Train Epoch: 9 [64000/71432 (90%)]\tLoss: 0.121775\n",
      "Train Epoch: 9 [70400/71432 (98%)]\tLoss: 0.083456\n",
      "\n",
      "Test set: Average loss: 0.0874, Accuracy: 7862/8141 (97%), Positive accuracy: 3895/4091 (95%), Negative accuracy: 3967/4050 (98%), f1 score: 0.9658\n",
      "\n",
      "Train Epoch: 10 [0/71432 (0%)]\tLoss: 0.059940\n",
      "Train Epoch: 10 [6400/71432 (9%)]\tLoss: 0.208733\n",
      "Train Epoch: 10 [12800/71432 (18%)]\tLoss: 0.095850\n",
      "Train Epoch: 10 [19200/71432 (27%)]\tLoss: 0.073014\n",
      "Train Epoch: 10 [25600/71432 (36%)]\tLoss: 0.068109\n",
      "Train Epoch: 10 [32000/71432 (45%)]\tLoss: 0.200666\n",
      "Train Epoch: 10 [38400/71432 (54%)]\tLoss: 0.048412\n",
      "Train Epoch: 10 [44800/71432 (63%)]\tLoss: 0.045530\n",
      "Train Epoch: 10 [51200/71432 (72%)]\tLoss: 0.019100\n",
      "Train Epoch: 10 [57600/71432 (81%)]\tLoss: 0.022705\n",
      "Train Epoch: 10 [64000/71432 (90%)]\tLoss: 0.070446\n",
      "Train Epoch: 10 [70400/71432 (98%)]\tLoss: 0.067664\n",
      "\n",
      "Test set: Average loss: 0.0834, Accuracy: 7883/8141 (97%), Positive accuracy: 3912/4091 (96%), Negative accuracy: 3971/4050 (98%), f1 score: 0.9684\n",
      "\n",
      "Train Epoch: 11 [0/71432 (0%)]\tLoss: 0.153892\n",
      "Train Epoch: 11 [6400/71432 (9%)]\tLoss: 0.096314\n",
      "Train Epoch: 11 [12800/71432 (18%)]\tLoss: 0.043817\n",
      "Train Epoch: 11 [19200/71432 (27%)]\tLoss: 0.063673\n",
      "Train Epoch: 11 [25600/71432 (36%)]\tLoss: 0.115786\n",
      "Train Epoch: 11 [32000/71432 (45%)]\tLoss: 0.008975\n",
      "Train Epoch: 11 [38400/71432 (54%)]\tLoss: 0.195075\n",
      "Train Epoch: 11 [44800/71432 (63%)]\tLoss: 0.115904\n",
      "Train Epoch: 11 [51200/71432 (72%)]\tLoss: 0.119196\n",
      "Train Epoch: 11 [57600/71432 (81%)]\tLoss: 0.043590\n",
      "Train Epoch: 11 [64000/71432 (90%)]\tLoss: 0.012064\n",
      "Train Epoch: 11 [70400/71432 (98%)]\tLoss: 0.042925\n",
      "\n",
      "Test set: Average loss: 0.0845, Accuracy: 7887/8141 (97%), Positive accuracy: 3925/4091 (96%), Negative accuracy: 3962/4050 (98%), f1 score: 0.9688\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [0/71432 (0%)]\tLoss: 0.067205\n",
      "Train Epoch: 12 [6400/71432 (9%)]\tLoss: 0.158040\n",
      "Train Epoch: 12 [12800/71432 (18%)]\tLoss: 0.067922\n",
      "Train Epoch: 12 [19200/71432 (27%)]\tLoss: 0.045800\n",
      "Train Epoch: 12 [25600/71432 (36%)]\tLoss: 0.114880\n",
      "Train Epoch: 12 [32000/71432 (45%)]\tLoss: 0.177880\n",
      "Train Epoch: 12 [38400/71432 (54%)]\tLoss: 0.051337\n",
      "Train Epoch: 12 [44800/71432 (63%)]\tLoss: 0.042167\n",
      "Train Epoch: 12 [51200/71432 (72%)]\tLoss: 0.074252\n",
      "Train Epoch: 12 [57600/71432 (81%)]\tLoss: 0.121884\n",
      "Train Epoch: 12 [64000/71432 (90%)]\tLoss: 0.085744\n",
      "Train Epoch: 12 [70400/71432 (98%)]\tLoss: 0.107819\n",
      "\n",
      "Test set: Average loss: 0.0838, Accuracy: 7880/8141 (97%), Positive accuracy: 3920/4091 (96%), Negative accuracy: 3960/4050 (98%), f1 score: 0.9680\n",
      "\n",
      "Train Epoch: 13 [0/71432 (0%)]\tLoss: 0.098931\n",
      "Train Epoch: 13 [6400/71432 (9%)]\tLoss: 0.127376\n",
      "Train Epoch: 13 [12800/71432 (18%)]\tLoss: 0.050912\n",
      "Train Epoch: 13 [19200/71432 (27%)]\tLoss: 0.069185\n",
      "Train Epoch: 13 [25600/71432 (36%)]\tLoss: 0.030214\n",
      "Train Epoch: 13 [32000/71432 (45%)]\tLoss: 0.067661\n",
      "Train Epoch: 13 [38400/71432 (54%)]\tLoss: 0.017610\n",
      "Train Epoch: 13 [44800/71432 (63%)]\tLoss: 0.067203\n",
      "Train Epoch: 13 [51200/71432 (72%)]\tLoss: 0.059360\n",
      "Train Epoch: 13 [57600/71432 (81%)]\tLoss: 0.058473\n",
      "Train Epoch: 13 [64000/71432 (90%)]\tLoss: 0.077697\n",
      "Train Epoch: 13 [70400/71432 (98%)]\tLoss: 0.037194\n",
      "\n",
      "Test set: Average loss: 0.0839, Accuracy: 7881/8141 (97%), Positive accuracy: 3909/4091 (96%), Negative accuracy: 3972/4050 (98%), f1 score: 0.9682\n",
      "\n",
      "Train Epoch: 14 [0/71432 (0%)]\tLoss: 0.039094\n",
      "Train Epoch: 14 [6400/71432 (9%)]\tLoss: 0.058109\n",
      "Train Epoch: 14 [12800/71432 (18%)]\tLoss: 0.014546\n",
      "Train Epoch: 14 [19200/71432 (27%)]\tLoss: 0.134238\n",
      "Train Epoch: 14 [25600/71432 (36%)]\tLoss: 0.196273\n",
      "Train Epoch: 14 [32000/71432 (45%)]\tLoss: 0.082800\n",
      "Train Epoch: 14 [38400/71432 (54%)]\tLoss: 0.057058\n",
      "Train Epoch: 14 [44800/71432 (63%)]\tLoss: 0.049863\n",
      "Train Epoch: 14 [51200/71432 (72%)]\tLoss: 0.059140\n",
      "Train Epoch: 14 [57600/71432 (81%)]\tLoss: 0.106783\n",
      "Train Epoch: 14 [64000/71432 (90%)]\tLoss: 0.064519\n",
      "Train Epoch: 14 [70400/71432 (98%)]\tLoss: 0.029841\n",
      "\n",
      "Test set: Average loss: 0.0834, Accuracy: 7879/8141 (97%), Positive accuracy: 3915/4091 (96%), Negative accuracy: 3964/4050 (98%), f1 score: 0.9679\n",
      "\n",
      "Train Epoch: 15 [0/71432 (0%)]\tLoss: 0.049567\n",
      "Train Epoch: 15 [6400/71432 (9%)]\tLoss: 0.146748\n",
      "Train Epoch: 15 [12800/71432 (18%)]\tLoss: 0.016687\n",
      "Train Epoch: 15 [19200/71432 (27%)]\tLoss: 0.027481\n",
      "Train Epoch: 15 [25600/71432 (36%)]\tLoss: 0.100509\n",
      "Train Epoch: 15 [32000/71432 (45%)]\tLoss: 0.060045\n",
      "Train Epoch: 15 [38400/71432 (54%)]\tLoss: 0.044702\n",
      "Train Epoch: 15 [44800/71432 (63%)]\tLoss: 0.135751\n",
      "Train Epoch: 15 [51200/71432 (72%)]\tLoss: 0.158854\n",
      "Train Epoch: 15 [57600/71432 (81%)]\tLoss: 0.095332\n",
      "Train Epoch: 15 [64000/71432 (90%)]\tLoss: 0.086078\n",
      "Train Epoch: 15 [70400/71432 (98%)]\tLoss: 0.098147\n",
      "\n",
      "Test set: Average loss: 0.0838, Accuracy: 7879/8141 (97%), Positive accuracy: 3916/4091 (96%), Negative accuracy: 3963/4050 (98%), f1 score: 0.9679\n",
      "\n",
      "Train Epoch: 16 [0/71432 (0%)]\tLoss: 0.015018\n",
      "Train Epoch: 16 [6400/71432 (9%)]\tLoss: 0.049293\n",
      "Train Epoch: 16 [12800/71432 (18%)]\tLoss: 0.022479\n",
      "Train Epoch: 16 [19200/71432 (27%)]\tLoss: 0.043834\n",
      "Train Epoch: 16 [25600/71432 (36%)]\tLoss: 0.130988\n",
      "Train Epoch: 16 [32000/71432 (45%)]\tLoss: 0.090611\n",
      "Train Epoch: 16 [38400/71432 (54%)]\tLoss: 0.036616\n",
      "Train Epoch: 16 [44800/71432 (63%)]\tLoss: 0.021027\n",
      "Train Epoch: 16 [51200/71432 (72%)]\tLoss: 0.090138\n",
      "Train Epoch: 16 [57600/71432 (81%)]\tLoss: 0.147830\n",
      "Train Epoch: 16 [64000/71432 (90%)]\tLoss: 0.141443\n",
      "Train Epoch: 16 [70400/71432 (98%)]\tLoss: 0.098716\n",
      "\n",
      "Test set: Average loss: 0.0831, Accuracy: 7882/8141 (97%), Positive accuracy: 3912/4091 (96%), Negative accuracy: 3970/4050 (98%), f1 score: 0.9683\n",
      "\n",
      "Train Epoch: 17 [0/71432 (0%)]\tLoss: 0.071339\n",
      "Train Epoch: 17 [6400/71432 (9%)]\tLoss: 0.062115\n",
      "Train Epoch: 17 [12800/71432 (18%)]\tLoss: 0.025618\n",
      "Train Epoch: 17 [19200/71432 (27%)]\tLoss: 0.030233\n",
      "Train Epoch: 17 [25600/71432 (36%)]\tLoss: 0.073048\n",
      "Train Epoch: 17 [32000/71432 (45%)]\tLoss: 0.083643\n",
      "Train Epoch: 17 [38400/71432 (54%)]\tLoss: 0.108850\n",
      "Train Epoch: 17 [44800/71432 (63%)]\tLoss: 0.047220\n",
      "Train Epoch: 17 [51200/71432 (72%)]\tLoss: 0.130635\n",
      "Train Epoch: 17 [57600/71432 (81%)]\tLoss: 0.052141\n",
      "Train Epoch: 17 [64000/71432 (90%)]\tLoss: 0.107931\n",
      "Train Epoch: 17 [70400/71432 (98%)]\tLoss: 0.072241\n",
      "\n",
      "Test set: Average loss: 0.0832, Accuracy: 7879/8141 (97%), Positive accuracy: 3918/4091 (96%), Negative accuracy: 3961/4050 (98%), f1 score: 0.9679\n",
      "\n",
      "Train Epoch: 18 [0/71432 (0%)]\tLoss: 0.062748\n",
      "Train Epoch: 18 [6400/71432 (9%)]\tLoss: 0.032877\n",
      "Train Epoch: 18 [12800/71432 (18%)]\tLoss: 0.064217\n",
      "Train Epoch: 18 [19200/71432 (27%)]\tLoss: 0.099729\n",
      "Train Epoch: 18 [25600/71432 (36%)]\tLoss: 0.025871\n",
      "Train Epoch: 18 [32000/71432 (45%)]\tLoss: 0.059450\n",
      "Train Epoch: 18 [38400/71432 (54%)]\tLoss: 0.103248\n",
      "Train Epoch: 18 [44800/71432 (63%)]\tLoss: 0.075347\n",
      "Train Epoch: 18 [51200/71432 (72%)]\tLoss: 0.097110\n",
      "Train Epoch: 18 [57600/71432 (81%)]\tLoss: 0.138375\n",
      "Train Epoch: 18 [64000/71432 (90%)]\tLoss: 0.279850\n",
      "Train Epoch: 18 [70400/71432 (98%)]\tLoss: 0.133631\n",
      "\n",
      "Test set: Average loss: 0.0838, Accuracy: 7881/8141 (97%), Positive accuracy: 3913/4091 (96%), Negative accuracy: 3968/4050 (98%), f1 score: 0.9681\n",
      "\n",
      "Train Epoch: 19 [0/71432 (0%)]\tLoss: 0.043196\n",
      "Train Epoch: 19 [6400/71432 (9%)]\tLoss: 0.018599\n",
      "Train Epoch: 19 [12800/71432 (18%)]\tLoss: 0.107174\n",
      "Train Epoch: 19 [19200/71432 (27%)]\tLoss: 0.097892\n",
      "Train Epoch: 19 [25600/71432 (36%)]\tLoss: 0.192776\n",
      "Train Epoch: 19 [32000/71432 (45%)]\tLoss: 0.052790\n",
      "Train Epoch: 19 [38400/71432 (54%)]\tLoss: 0.028024\n",
      "Train Epoch: 19 [44800/71432 (63%)]\tLoss: 0.037966\n",
      "Train Epoch: 19 [51200/71432 (72%)]\tLoss: 0.056427\n",
      "Train Epoch: 19 [57600/71432 (81%)]\tLoss: 0.086358\n",
      "Train Epoch: 19 [64000/71432 (90%)]\tLoss: 0.076566\n",
      "Train Epoch: 19 [70400/71432 (98%)]\tLoss: 0.068317\n",
      "\n",
      "Test set: Average loss: 0.0836, Accuracy: 7885/8141 (97%), Positive accuracy: 3913/4091 (96%), Negative accuracy: 3972/4050 (98%), f1 score: 0.9686\n",
      "\n",
      "Train Epoch: 20 [0/71432 (0%)]\tLoss: 0.035243\n",
      "Train Epoch: 20 [6400/71432 (9%)]\tLoss: 0.076107\n",
      "Train Epoch: 20 [12800/71432 (18%)]\tLoss: 0.128075\n",
      "Train Epoch: 20 [19200/71432 (27%)]\tLoss: 0.078648\n",
      "Train Epoch: 20 [25600/71432 (36%)]\tLoss: 0.175563\n",
      "Train Epoch: 20 [32000/71432 (45%)]\tLoss: 0.052084\n",
      "Train Epoch: 20 [38400/71432 (54%)]\tLoss: 0.069332\n",
      "Train Epoch: 20 [44800/71432 (63%)]\tLoss: 0.074219\n",
      "Train Epoch: 20 [51200/71432 (72%)]\tLoss: 0.057262\n",
      "Train Epoch: 20 [57600/71432 (81%)]\tLoss: 0.047674\n",
      "Train Epoch: 20 [64000/71432 (90%)]\tLoss: 0.049745\n",
      "Train Epoch: 20 [70400/71432 (98%)]\tLoss: 0.083092\n",
      "\n",
      "Test set: Average loss: 0.0835, Accuracy: 7881/8141 (97%), Positive accuracy: 3914/4091 (96%), Negative accuracy: 3967/4050 (98%), f1 score: 0.9681\n",
      "\n",
      "[7689, 7686, 7771, 7779, 7816, 7863, 7865, 7860, 7862, 7883, 7887, 7880, 7881, 7879, 7879, 7882, 7879, 7881, 7885, 7881]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CNNTrainedGabor(nn.Module):\n",
    "    def __init__(self, pretrain_gabor_model):\n",
    "        super(CNNTrainedGabor, self).__init__()\n",
    "        self.pretrain_gabor_model = pretrain_gabor_model\n",
    "        self.conv1 = nn.Conv2d(1, 4, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 5, 1)\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "#         self.dropout2 = nn.Dropout2d(0.5)\n",
    "        # self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc1 = nn.Linear(225, 128)\n",
    "#         self.fc2 = nn.Linear(128, 2)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 16)\n",
    "        self.fc4 = nn.Linear(16, 6)\n",
    "        self.fc5 = nn.Linear(6, 2)\n",
    "#         self.fc_gabor1 = nn.Linear(96, 36)\n",
    "#         self.fc_gabor2 = nn.Linear(36, 6)\n",
    "\n",
    "        \n",
    "   \n",
    "    def forward(self, x):\n",
    "        pc, x_cos, x_sin = self.pc(x)\n",
    "        pc = pc.view(x.shape[0],1)\n",
    "#         print(x_cos.shape,x_sin.shape)\n",
    "        cos, sin = self.mean_std(x_cos,x_sin)\n",
    "#         print(cos.shape,sin.shape)\n",
    "        comb = torch.cat((cos,sin,pc),1)\n",
    "#         x_comb = torch.cat((x_cos, x_sin), 3)\n",
    "#         x_comb_fc = self.fc_gabor1(x_comb)\n",
    "#         x_comb_fc = self.fc_gabor2(x_comb_fc)\n",
    "#         x_comb_fc = torch.squeeze(x_comb_fc)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "# #         x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        comb_cnn = torch.cat((comb,x),1)\n",
    "        x = self.fc1(comb_cnn)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x)\n",
    "\n",
    "#         x = self.dropout2(x)\n",
    "#         print(pc.shape, x.shape)\n",
    "#         print(x_comb_fc.shape, x.shape)\n",
    "#         x = torch.cat((x_comb_fc,pc, x), 1)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def pc(self, x):\n",
    "        filter_cos = self.pretrain_gabor_model['filter_cos']\n",
    "        filter_sin = self.pretrain_gabor_model['filter_sin']\n",
    "        bias1 = self.pretrain_gabor_model['bias1']\n",
    "        bias2 = self.pretrain_gabor_model['bias2']\n",
    "        weights = self.pretrain_gabor_model['weights']\n",
    "        w = self.pretrain_gabor_model['w']\n",
    "        b = self.pretrain_gabor_model['b']\n",
    "        \n",
    "        x_cos = F.conv2d(x, filter_cos, bias=bias1)\n",
    "        x_sin = F.conv2d(x, filter_sin, bias=bias2)\n",
    "        x_comb = torch.cat((x_cos, x_sin), 2)\n",
    "\n",
    "        x_cos = x_cos.view(len(x), 1, 1, 48)\n",
    "        x_sin = x_sin.view(len(x), 1, 1, 48)\n",
    "        weighted_cos = (torch.matmul(x_cos, weights)).view(len(x), 1)\n",
    "        weighted_sin = (torch.matmul(x_sin, weights)).view(len(x), 1)\n",
    "\n",
    "        numerator = torch.norm(torch.cat([weighted_cos, weighted_sin], 1), dim=1)\n",
    "    #         print(\"numerator\", numerator.size())\n",
    "        x_comb_norm = torch.norm(x_comb, dim=2)\n",
    "        x_comb_norm = x_comb_norm.view(len(x), 1, 48)\n",
    "    #         print(\"x_comb_norm\", x_comb_norm.size())\n",
    "        denominator = torch.matmul(x_comb_norm, torch.abs(weights))\n",
    "        denominator = denominator.view(len(x))\n",
    "    #         print(\"size:\", numerator.size(), denominator.size())\n",
    "        pc = numerator / denominator                \n",
    "        return torch.sigmoid(w * pc + b),x_cos, x_sin\n",
    "    \n",
    "    def mean_std(self, x_cos, x_sin):\n",
    "        cos_mean = torch.FloatTensor([])\n",
    "        cos_std = torch.FloatTensor([])\n",
    "        sin_mean = torch.FloatTensor([])\n",
    "        sin_std = torch.FloatTensor([])\n",
    "        for i in range(6):\n",
    "            cos_curr = torch.FloatTensor([])\n",
    "            sin_curr = torch.FloatTensor([])\n",
    "            for j in range(i,48,6):\n",
    "                cos_curr = torch.cat((cos_curr,x_cos[:,:,:,j]),1)\n",
    "                sin_curr = torch.cat((sin_curr,x_sin[:,:,:,j]),1)\n",
    "            cos_mean = torch.cat((cos_mean,torch.mean(cos_curr,1)),1)\n",
    "            cos_std = torch.cat((cos_std,torch.std(cos_curr,1)),1)\n",
    "            sin_mean = torch.cat((sin_mean,torch.mean(sin_curr,1)),1)\n",
    "            sin_std = torch.cat((sin_std,torch.std(sin_curr,1)),1)\n",
    "#         print(cos_mean.shape,cos_std.shape,\"mean_std\")\n",
    "        cos = torch.cat((cos_mean,cos_std),1)\n",
    "        sin = torch.cat((sin_mean,sin_std),1)\n",
    "        return cos, sin\n",
    "            \n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "    train_loss = sum(loss_list)/len(loss_list)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "# def test(model, device, test_loader):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             output = model(data)\n",
    "#             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "#             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "#             correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "\n",
    "#     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         test_loss, correct, len(test_loader.dataset),\n",
    "#         100. * correct / len(test_loader.dataset)))\n",
    "def test(model, device, test_loader,epoch,train_loss):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    result= [[0,0], [0,0]] \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            cmat = confusion_matrix(target.view_as(pred), pred, labels=[0, 1]) \n",
    "            result = [[result[i][j] + cmat[i][j]  for j in range(len(result[0]))] for i in range(len(result))] \n",
    "            \n",
    "            # Store wrongly predicted images\n",
    "            if epoch == 11:\n",
    "                wrong_idx = (pred != target.view_as(pred)).nonzero()[:, 0]\n",
    "                wrong_samples = data[wrong_idx]\n",
    "                wrong_preds = pred[wrong_idx]\n",
    "                actual_preds = target.view_as(pred)[wrong_idx]\n",
    "                for i in range(len(wrong_idx)):\n",
    "                    sample = wrong_samples[i]\n",
    "                    wrong_pred = wrong_preds[i]\n",
    "                    actual_pred = actual_preds[i]\n",
    "                    sample = sample * 255.\n",
    "                    sample = sample.byte()\n",
    "                    img = TF.to_pil_image(sample)\n",
    "                    img.save('./wrong-final/batch{}_idx{}_actual{}.png'.format(\n",
    "                        batch_idx,wrong_idx[i], actual_pred.item()))\n",
    "                    num = batch_idx * 64 + wrong_idx[i]\n",
    "                    img_ori = origin_dataset[num][0].numpy()\n",
    "                    plt.imsave('./wrong-final/batch{}_idx{}_actual{}_ori.png'.format(\n",
    "                        batch_idx,wrong_idx[i], actual_pred.item()), img_ori[0], cmap = 'gray')\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    precision = result[1][1]/(result[1][1]+result[0][1])\n",
    "    recall = result[0][0]/(result[0][0]+result[1][0])\n",
    "    f1score = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Positive accuracy: {}/{} ({:.0f}%), Negative accuracy: {}/{} ({:.0f}%), f1 score: {:.4f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset), \n",
    "        result[1][1],result[1][1]+result[1][0],100. * result[1][1]/(result[1][1]+result[1][0]),\n",
    "        result[0][0],result[0][0]+result[0][1],100. * result[0][0]/(result[0][0]+result[0][1]),f1score))\n",
    "    return test_loss, correct\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=100, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'batch_size': args.batch_size}\n",
    "    if use_cuda:\n",
    "        kwargs.update({'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True},\n",
    "                     )\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    model = CNNTrainedGabor(pretrain_gabor_model).to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_accuracy_list = []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_result = test(model, device, test_loader,epoch,train_loss)\n",
    "        test_loss_list.append(test_result[0])\n",
    "        test_accuracy_list.append(test_result[1])\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "    plt.plot(train_loss_list, color='blue',label='train loss')  \n",
    "    plt.plot(test_loss_list, color='red',label='test loss')  \n",
    "    plt.legend()\n",
    "    print(test_accuracy_list)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f92e6ec9590>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhU9b3H8fc3ISGsYUnCviqCCIgYFmlxKdiKtVCtVrjeWrBKW2tr9VavXrv42NrV3qu1eHvRS7XWHZdyFUsN2qoYhICsCUsIWyAkYYAECCHL/O4fM9AYEjLAzJyZyef1PHmYmfNL5vscDp/8+M45v2POOUREJP4leV2AiIiEhwJdRCRBKNBFRBKEAl1EJEEo0EVEEkQbr944IyPDDRw40Ku3FxGJSytXrtznnMtsaptngT5w4EDy8vK8ensRkbhkZjua26aWi4hIglCgi4gkCAW6iEiCUKCLiCQIBbqISIJoMdDNbL6ZlZnZ+ma2m5n9zswKzWytmY0Jf5kiItKSUGboTwNXnWL7VGBI8GsO8N9nX5aIiJyuFs9Dd869b2YDTzFkOvAnF1iHd5mZdTGzXs65kjDVKCIeWbljP//YVO51GQln8vk9uLBfl7D/3HBcWNQH2NXgeXHwtZMC3czmEJjF079//zC8tYhEgt/vmPteIf+Vsxm/AzOvK0osWZ3TYjbQQ+acmwfMA8jOztadNURikO/wMb7/0mo+2LKPL4/uzcPXjqRDW88uKpfTEI6/pd1AvwbP+wZfE5E4s3zbfr77wioOVNXyi+tGMmNsP0zT87gRjtMWFwI3B892mQBUqH8uEl/8fscTfy9k5pPLaJeSzOu3T2TmuP4K8zjT4gzdzF4ALgcyzKwY+AmQAuCc+wOwCLgaKASqgNmRKlZEwu/AkRrufnk1720q54ujevHL60bSKS3F67LkDIRylsvMFrY74Dthq0hEombljv3c8fwn+A7X8NPpF/CvEwZoVh7H9EmHSCvknOPJD4r49V830btLO1799kRG9k33uiw5Swp0kVbmYFUNP3hlDTkFZVx1QU9+fcMoOqvFkhAU6CKtyCc7D3DH859Qdqian3xpOLMmDlSLJYEo0EVaAeccf1y6nV+8XUBWpzRe+dZERkfgwhbxlgJdJMFVHK3l3xes5a8b9jLl/B789oYLSW+vFksiUqCLJLB1xRXc/vxKSg5W88Mvns83PjtILZYEpkAXSVDvbizlW8+uIqNjKi998xIuHtDV65IkwhToIgnqqQ+20TM9jb985zN07ZDqdTkSBbpjkUgCqjhay/Jt+/niqF4K81ZEgS6SgP6xuZw6v2PK+VlelyJRpEAXSUA5+aV075DK6H7qm7cmCnSRBFNb7+fvm8r43LAskpN0RktrokAXSTArtu+nsrqOyef38LoUiTIFukiCWVJQRmqbJCYNyfC6FIkyBbpIAnHOkVNQysRzuuu2ca2QAl0kgWwtP8wOXxVT1G5plRToIgnknfwyACbrdMVWSYEukkByCkoZ0aczvdLbeV2KeECBLpIgfIePsWrnAbVbWjEFukiCeHdjGc6hQG/FFOgiCWJJQRk9O6dxQe/OXpciHlGgiySA6tp63t9SzuTzs7TeeSumQBdJAMuKfFTV1DNluNotrZkCXSQB5BSU0j41mUsGd/e6FPGQAl0kzjnnWFJQxqQhGaSlJHtdjnhIgS4S5zbsqaSkolpnt4gCXSTe5RSUYgZXDNPVoa2dAl0kzi0pKGNM/65kdGzrdSniMQW6SBwrqTjKut0VWrtFAAW6SFxbUhBYjOtK9c8FBbpIXFtSUMqA7u05N6uj16VIDFCgi8Spqpo6lm71MXlYD10dKkCIgW5mV5nZJjMrNLP7mtg+wMyWmNlaM/u7mfUNf6ki0tAHW/ZRU+dnynD1zyWgxUA3s2RgLjAVGA7MNLPhjYY9AvzJOTcKeAj4RbgLFZFPy8kvpVNaG8YO7OZ1KRIjQpmhjwMKnXNFzrka4EVgeqMxw4F3g4/fa2K7iIRRvd/x7sYyrhiaRUqyOqcSEMqR0AfY1eB5cfC1htYA1wUfXwt0MrOTFpUwszlmlmdmeeXl5WdSr4gAq3cdxHekRqcryqeE61f7D4DLzOwT4DJgN1DfeJBzbp5zLts5l52ZmRmmtxZpfXIKSmmTZFx+ngJd/qlNCGN2A/0aPO8bfO0E59wegjN0M+sIfMU5dzBcRYrIpy0pKGXcoG6kt0/xuhSJIaHM0FcAQ8xskJmlAjOAhQ0HmFmGmR3/WfcD88Nbpogct9NXxebSw0zWxUTSSIuB7pyrA+4AFgMFwMvOuQ1m9pCZTQsOuxzYZGabgR7AwxGqV6TVyykoBWCK+ufSSCgtF5xzi4BFjV77cYPHC4AF4S1NRJqSU1DKkKyODOjewetSJMbofCeROFJxtJbl2/brVnPSJAW6SBz5x+Zy6vxON7OQJinQReJITn4p3TukMrpfF69LkRikQBeJE7X1fv6+qYzPDcsiOUmLccnJFOgicWLF9v1UVtfpdEVplgJdJE4sKSgjtU0Sk4ZkeF2KxCgFukgccM6RU1DKxHO606FtSGcbSyukQBeJA4Vlh9nhq9LZLXJKCnSROJATvHeoVleUU1Ggi8SBnIJSRvTpTK/0dl6XIjFMgS4S43yHj7Fq5wG1W6RFCnSRGPfuxjKcQ4EuLVKgi8S4nIJSenZO44Lenb0uRWKcAl0khlXX1vPBln1MPj8LM10dKqemQBeJYblFPqpq6rW6ooREgS4Sw5YUlNI+NZlLBp90z3WRkyjQRWKUc44lBWVMGpJBWkqy1+VIHFCgi8SoDXsqKamo1tktEjIFukiMyikoxQyuGKarQyU0CnSRGJVTUMqY/l3J6NjW61IkTijQRWJQScVR1u+uVLtFTosCXSQGvbsxsBjXFC3GJadBgS4Sgz4q9NErPY1zszp6XYrEEQW6SIxxzrGsyMclg7vr6lA5LQp0kRizpewwviM1TNDFRHKaFOgiMSZ3qw+AS85RoMvpUaCLxJhlRT76dGlHv27tvS5F4owCXSSG+P2B/rnaLXImFOgiMWRT6SEOVNWq3SJnRIEuEkOWFQX65xMGd/O4EolHCnSRGJK71Ue/bu3o21X9czl9IQW6mV1lZpvMrNDM7mtie38ze8/MPjGztWZ2dfhLFUlsfr/j4237tfa5nLEWA93MkoG5wFRgODDTzIY3GvZD4GXn3EXADOCJcBcqkugK9lZScVT9czlzoczQxwGFzrki51wN8CIwvdEYBxy/g206sCd8JYq0DsfPP9cZLnKm2oQwpg+wq8HzYmB8ozEPAn8zs+8CHYApYalOpBVZVuRjYPf29Epv53UpEqfC9aHoTOBp51xf4GrgWTM76Web2RwzyzOzvPLy8jC9tUj8qz/eP1e7Rc5CKIG+G+jX4Hnf4GsNfQN4GcA5lwukARmNf5Bzbp5zLts5l52ZmXlmFYskoPw9lRyqrlO7Rc5KKIG+AhhiZoPMLJXAh54LG43ZCUwGMLPzCQS6puAiIcot2gegM1zkrLQY6M65OuAOYDFQQOBslg1m9pCZTQsO+zfgNjNbA7wAzHLOuUgVLZJolhXtZ3BmB7I6p3ldisSxUD4UxTm3CFjU6LUfN3icD3wmvKWJtA519X6Wb9vPtNG9vS5F4pyuFBXx2IY9lRw+Vqd2i5w1BbqIx3KD67eM1/otcpYU6CIey93q49ysjmR1Uv9czo4CXcRDtfV+8rZr/RYJDwW6iIfW7a7gSE29zj+XsFCgi3jon+u3qH8uZ0+BLuKhZUU+hvboRPeObb0uRRKAAl3EIzV1fvK2H9DsXMJGgS7ikbXFBzlaW68FuSRsFOgiHjl+/9DxgxToEh4KdBGP5Bb5GNazE107pHpdiiQIBbqIB47V1ZO3/YDaLRJWCnQRD6zZVcGxOr8uKJKwUqCLeCB3qw8z9c8lvBToIh7ILdrH8F6dSW+f4nUpkkAU6CJRVl1bz6qdB9VukbBToItE2Sc7D1JT59f6LRJ2CnSRKMst8pFkME5XiEqYKdBFomxZkY8RfdLpnKb+uYSXAl0kiqpr61m986DaLRIRCnSRKFq54wA19Tr/XCJDgS4SRcuKfCQnGdkDu3pdiiQgBbpIFOVuDfTPO6l/LhGgQBeJkqqaOtYU6/xziRwFukiUrNxxgNp6pwW5JGIU6CJRkrvVR5skI3uA+ucSGQp0kSjJLfIxqm86Hdq28boUSVAKdJEoOHKsjrXFFWq3SEQp0EWiYMX2/dT7nS4okohSoItEQW6Rj5RkI3uA1m+RyFGgi0TBsqL9jO7XhXapyV6XIglMgS4SYYeqa1m/u0LtFom4kALdzK4ys01mVmhm9zWx/b/MbHXwa7OZHQx/qSLxKW/7Aer9ThcUScS1eP6UmSUDc4ErgWJghZktdM7lHx/jnLurwfjvAhdFoFaRuJRb5CM1OYkxOv9cIiyUGfo4oNA5V+ScqwFeBKafYvxM4IVwFCeSCHK3+hjdvwtpKeqfS2SFEuh9gF0NnhcHXzuJmQ0ABgHvNrN9jpnlmVleeXn56dYqEncqjtayYU+F2i0SFeH+UHQGsMA5V9/URufcPOdctnMuOzMzM8xvLRJ7Vmzbj9+hC4okKkIJ9N1AvwbP+wZfa8oM1G4ROSG3yEdqmyRG9+vidSnSCoQS6CuAIWY2yMxSCYT2wsaDzGwY0BXIDW+JIvFrWZGPi/t3Vf9coqLFQHfO1QF3AIuBAuBl59wGM3vIzKY1GDoDeNE55yJTqkh8OVhVQ35Jpc4/l6gJadk359wiYFGj137c6PmD4StLJP59vG0/Tv1ziSJdKSoSIcuKfKSlJHFhv3SvS5FWQoEuEiG5W31cPKArbduofy7RoUAXiYD9R2rYuPeQzj+XqFKgi0TA8m0+QP1ziS4FukgE5G710S4lmZF9dP65RI8CXSQCcot8ZA/sSmob/ROT6NHRJhJm+w4fY3PpYbVbJOp0+3GRMCqtrOahNwMrS+uCIok2BbpIGBytqWfe+0X84R9bqfc7vnPFOVyk9VskyhToImfB73f8Zc1ufv3XTZRUVDN1RE/umzqMAd07eF2atEIKdJEzlLd9Pz99M581xRWM7JPOozeOZrzaLOIhBbrIadq1v4pfvr2Rt9aV0KNzW357w4Vce1EfkpLM69KklVOgi4ToUHUtc9/byvyl20gyuHPyEL552WDap+qfkcQGHYkiLaj3O15asYv/fGcT+w7XcN2YPtzzhaH0Sm/ndWkin6JAFzmFD7aU8/BbBWzce4ixA7syf9ZYRvXV2SsSmxToIk0oLDvMzxcV8O7GMvp1a8cTN41h6oiemKlPLrFLgS7SwOFjdTyyeBN/XraDdinJ3D91GF+fOFC3kJO4oEAXCSooqeQ7z61iu+8IM8f1564rzyOjY1uvyxIJmQJdWj3nAh96/mThBtLbpfDCbRN0PrnEJQW6tGpHjtXxwzfW8/onu/nsuRk8OmO0ZuUStxTo0mptLj3Et/+8kqJ9R7hrynnc8blzSdbFQRLHFOjSKr2St4sf/WU9Hdum8Nw3xjPx3AyvSxI5a3EX6M45dviqGJihxY/k9B2tqedHf1nPgpXFXDK4O4/NHE1WpzSvyxIJi7i7wcXv3y3kmsc/ZMOeCq9LkThTWHaI6XM/5NVVxXzvc+fy51vHK8wlocRdoF+f3ZdOaW2Y/ccV7D541OtyJE68/kkx036/FN/hGp6ZPY67Pz9U/XJJOHEX6L3S2/H07HEcra1n1vzlVFTVel2SxLDq2nruf20td720hhG903nre5O49LxMr8sSiYi4C3SAoT07Me9r2ezwVXHbs3lU19Z7XZLEoKLyw1z7xEe8sHwXt19+Ds/fNp6e6WqxSOKKy0AHuOSc7vzmhlEs37aff3tlDX6/87okiSH/t2YPX3r8Q/ZWHOWPs8dy71XDaJMct4e7SEji7iyXhqaP7sPeimp+8fZGeqen8cAXh3tdknisuraen72Vz5+X7eTiAV15fOZF9O6iZW6ldYjrQAeYc+lgSiqqefKDbfRKb8ctnx3kdUnikd0Hj/LNZ/NYv7uSOZcO5p4vDCVFs3JpReI+0M2MH10znJKKo/z0rXx6pqdx9cheXpclUbZ610FufSaPY7X1PHlzNlcO7+F1SSJRlxDTl+Qk47EZFzGmf1e+/9JqVmzf73VJEkWL1pVw4//k0i41iddun6gwl1YrpEA3s6vMbJOZFZrZfc2M+aqZ5ZvZBjN7PrxltiwtJZmnbs6mb5d23PpMHoVlh6JdgkSZc4657xVy+3OrGNEnnTdu/wxDenTyuiwRz7QY6GaWDMwFpgLDgZlmNrzRmCHA/cBnnHMXAN+PQK0t6tohlWduGUdKsvH1+Ssoq6z2ogyJgmN19fzglbX8ZvEmpo/uzXO3jqe7VkmUVi6UGfo4oNA5V+ScqwFeBKY3GnMbMNc5dwDAOVcW3jJD169be+bPGsuBqhpmP72Cw8fqvCpFIuTAkRq+9r/LeXVVMXdNOY9HbxytOwqJEFqg9wF2NXheHHytofOA88xsqZktM7OrmvpBZjbHzPLMLK+8vPzMKg7BqL5dmHvTGDbuPcTtz62itt4fsfeS6Npafphrn1jK6l0HeWzGaO6cMkT3+RQJCteHom2AIcDlwEzgSTM76dbozrl5zrls51x2ZmZkL7++YmgWP792BO9vLuf+19bhnC48incfbd3HtXOXcqi6jhduG8/00Y3nFSKtWyinLe4G+jV43jf4WkPFwMfOuVpgm5ltJhDwK8JS5Rm6cWx/9hys5rElW+jdpR13X3mel+XIWXh5xS7+4/V1DMrowPxZY+nXrb3XJYnEnFBm6CuAIWY2yMxSgRnAwkZj3iAwO8fMMgi0YIrCWOcZ+/6UIXw1uy+/W7KFF5fv9LocOU1+v+MXbxdw76trueSc7rx6+0SFuUgzWpyhO+fqzOwOYDGQDMx3zm0ws4eAPOfcwuC2z5tZPlAP3OOc80Wy8FCZGQ9fO5LSymM88MZ6enRO44phWV6XJSGoqqnjrpdWs3hDKV+bMICffGm41mMROQXzqrecnZ3t8vLyovZ+R47VceO8XLaWHeGlb05gVN+TWvwSQ0orq7n1mTw27KngR9cMZ9bEgfrwUwQws5XOueymtrWa6U6Htm2YP2ss3TumcsvTK9jpq/K6JGnG+t0VTP/9UorKD/PU17OZ/ZlBCnORELSaQAfI6pTG07PHUed33PS/y8jfU+l1SdLIO/mlfPV/ckkyWPDtiXxumC7jFwlVqwp0gHOzOvL07HEcq/Vz7RNLeWH5Tp3SGAOcczz5fhFzns1jSFZH3vjOZzi/V2evyxKJK60u0AFG9+vCojsnMW5QN+5/bR13v7yGI7qi1DPFB6q4ef5yHl5UwNQRPXlxziVkddadhUROV9wvn3umMjq25enZ45j7XiH/lbOZtcUHeeKmixnaU4s7RYvf73hu+U5+uagAgJ9+eQQ3jetPkm7eLHJGWuUM/bjkJON7k4fw3DfGU3G0julzP+SVvF0tf6OctZ2+Kv7lqWX86I31jBnQlcV3XcrXJgxQmIuchVYd6MdNPDeDRXd+lov6deWeBWv5wStrOFqjG09Hgt/v+OPSbXzh0ffZsLuSX143kj/dMo6+XXWxkMjZarUtl8ayOqXx51vH81jOZh5/rzDYghnDuVlqwYRLUflh7l2wlrwdB7h8aCa/uG4kvdJ1v0+RcNEMvYHkJOPuzw/lmdnj8B2uYdrvl/L6J8VelxX36v2BM1imPvYBm0sP8cgNF/LHWWMV5iJhpkBvwqXnZbLozkmM6J3OXS+t4f7X1lJdqxbMmSgsO8T1f/iIhxcVMGlIJu/cfRnXX9xXFwqJRIBaLs3o0TmN528bz3++s5kn/r6VT3YGWjCDMzt6XVpcqKv3M++DIh7N2UL71GQemzGaaRf2VpCLRJBm6KfQJjmJe68axh9nj6W0spovPf4h/7dmj9dlxbyNeyu59omP+PVfNzF5WBbv3HUZ00f3UZiLRJgCPQRXDM3ire9NYlivznz3hU/44Rvr1IJpQm29n8dytvClxz9kz8GjPHHTGP77Xy8ms5Pu9SkSDWq5hKh3l3a8OGcCjyzexP+8X3SiBTOgewevS4sJ63dXcM+CtRSUVDLtwt48OO0CunVI9boskVZFM/TTkJKcxP1Xn89TN2dTfOAoVz/2Ac/mbsfvb71rwRyrq+e3f9vEl+cuZd/hY8z72sX8buZFCnMRD7Sa9dDDbffBo9z36lo+2LKPCYO78auvjGp1s/U1uw5yz4I1bC49zHVj+vDja4bTpb2CXCSSTrUeugL9LDjneDlvFz97s4A6v+OeLwxl1sSBCX/5enVtPY/mbGHe+1vJ6pTGz68boWVuRaLkVIGuHvpZMDNuHNufS8/L5D9eW8dDb+azaF0Jv75+VMKe3rhyxwHuXbCGreVHuDG7Hw9ccz6d01K8LktEUA89LHqlt2P+rLH89oYL2Vx6iKmPfcC897dSn0C99aM19fzszXyu/8NHVNf6+dMt4/jV9aMU5iIxRDP0MDEzvnJxXyYNyeCBN9bz80UbeWvdXh65fhRDesT3ejAfF/n491fXst1XxU3j+3Pf1GF0UpCLxBzN0MMsq3PaiTM9dvqO8MXffcjc9wqpq/d7Xdppq6qp48GFG7hx3jLqneP5W8fz8LUjFeYiMUoz9AgwM6Zd2JtLBnfnJwvX85vFm3h7fQm/uf7CuLmt2kdb9/Hvr65l1/6jzJo4kHu+MJQObXW4iMQyzdAjKLNTW5646WKeuGkMJQermfb7D3k0ZzM1dbE7Wz98rI4HXl/Hvzz5MclmvPzNS3hw2gUKc5E4oH+lUXD1yF5MGNydBxdu4NGcLfx1/V4eueFCRvRJ97q0T3l/czn3v7aOPRVHuW3SIO6+cijtUpO9LktEQqQZepR065DK72ZexLyvXYzvSA3T5y7lkcWbOFbn/ZowFUdruXfBGm6ev5y0lCRe/fZEHvjicIW5SJzRhUUeqKiq5aE383l1VTEd27ZheK/ODO/d+cSfQ3p0pG2byIRpVU0dG/ceIn9PJRv2VJJfUsnGkkpq6/1887JzuHPyENJSFOQisUpXisaoD7fs42/5e9mwp5KCkkqqgvcxbZNknJvVkQt6p/8z6Ht1Jr396Z1dUn7oGPklleQHg3vDngq27TvC8b/y9HYpJ36JfHl0H0b2ja0WkIicTIEeB/x+x479VWzYU3EigPP3VFJ26NiJMX27tjtpNt+nSzucg+2+I58K75a+94I+6fROT9Ma5SJxRpf+x4GkJGNQRgcGZXTgmlG9T7ze1Cz7nYLST82ya+v9J83uJw3JPKvZvYjEHwV6jMvs1JbLOmVy2XmZJ15r2AfPL6kkJclOtGci2X8XkdimQI9D7VPbMKZ/V8b07+p1KSISQ0I6bdHMrjKzTWZWaGb3NbF9lpmVm9nq4Net4S9VREROpcUZupklA3OBK4FiYIWZLXTO5Tca+pJz7o4I1CgiIiEIZYY+Dih0zhU552qAF4HpkS1LREROVyiB3gfY1eB5cfC1xr5iZmvNbIGZ9WvqB5nZHDPLM7O88vLyMyhXRESaE65L//8PGOicGwW8AzzT1CDn3DznXLZzLjszM7OpISIicoZCCfTdQMMZd9/gayc453zOueNXsTwFXBye8kREJFShBPoKYIiZDTKzVGAGsLDhADPr1eDpNKAgfCWKiEgoWjzLxTlXZ2Z3AIuBZGC+c26DmT0E5DnnFgLfM7NpQB2wH5gVwZpFRKQJnq3lYmblwI4z/PYMYF8Yy4mkeKlVdYZXvNQJ8VOr6gwY4Jxr8kNIzwL9bJhZXnOL08SaeKlVdYZXvNQJ8VOr6myZbnAhIpIgFOgiIgkiXgN9ntcFnIZ4qVV1hle81AnxU6vqbEFc9tBFRORk8TpDFxGRRhToIiIJIqYDPYR12Nua2UvB7R+b2UAPauxnZu+ZWb6ZbTCzO5sYc7mZVTRYL/7H0a6zQS3bzWxdsI6TbupqAb8L7tO1ZjbGgxqHNthXq82s0sy+32iMJ/vUzOabWZmZrW/wWjcze8fMtgT/bPLOI2b29eCYLWb2dY9q/Y2ZbQz+3b5uZl2a+d5THidRqPNBM9vd4O/36ma+95QZEYU6X2pQ43YzW93M90ZnfzrnYvKLwFWpW4HBQCqwBhjeaMztwB+Cj2cQWJM92nX2AsYEH3cCNjdR5+XAm17v02At24GMU2y/GngbMGAC8HEMHAd7CVxM4fk+BS4FxgDrG7z2a+C+4OP7gF818X3dgKLgn12Dj7t6UOvngTbBx79qqtZQjpMo1Pkg8IMQjo1TZkSk62y0/bfAj73cn7E8Qw9lHfbp/HNlxwXAZIvybeydcyXOuVXBx4cIrGPT1PLC8WI68CcXsAzo0mitnmibDGx1zp3pVcVh5Zx7n8DyFg01PA6fAb7cxLd+AXjHObffOXeAwKqkV0WsUJqu1Tn3N+dcXfDpMgKL7XmqmX0aiqjeq+FUdQZz56vAC5F6/1DEcqCHsg77iTHBg7QC6B6V6poQbPlcBHzcxOZLzGyNmb1tZhdEtbBPc8DfzGylmc1pYnuo699Hywya/0cSK/u0h3OuJPh4L9CjiTGxtl8BbiHwv7GmtHScRMMdwdbQ/GbaWLG0TycBpc65Lc1sj8r+jOVAjytm1hF4Ffi+c66y0eZVBFoGFwKPA29Eu74GPuucGwNMBb5jZpd6WMspBVf3nAa80sTmWNqnJ7jA/69j/lxgM3uAwGJ6zzUzxOvj5L+Bc4DRQAmBdkYsm8mpZ+dR2Z+xHOgtrsPecIyZtQHSAV9UqmvAzFIIhPlzzrnXGm93zlU65w4HHy8CUswsI8plHq9ld/DPMuB1Av9tbSiU/R4tU4FVzrnSxhtiaZ8CpcfbUsE/y5oYEzP71cxmAdcANwV/AZ0khOMkopxzpc65euecH3iymfePiX0azJ7rgJeaGxOt/RnLgd7iOuzB58fPFrgeeLe5AzRSgr2z/wUKnHP/2cyYnsd7+2Y2jsB+9+IXTwcz63T8MYEPyNY3GrYQuDl4tssEoKJBOyHamp31xMo+DWp4HH4d+EsTYxYDn1Y3kYsAAAEdSURBVDezrsH2weeDr0WVmV0F3AtMc85VNTMmlOMkohp9bnNtM+8fSkZEwxRgo3OuuKmNUd2fkf7U9Wy+CJxxsZnAJ9kPBF97iMDBCJBG4L/jhcByYLAHNX6WwH+x1wKrg19XA98CvhUccwewgcCn8MuAiR7tz8HBGtYE6zm+TxvWasDc4D5fB2R7VGsHAgGd3uA1z/cpgV8wJUAtgZ7tNwh8brME2ALkAN2CY7OBpxp87y3BY7UQmO1RrYUE+s7Hj9XjZ4n1Bhad6jiJcp3PBo+/tQRCulfjOoPPT8qIaNYZfP3p48dlg7Ge7E9d+i8ikiBiueUiIiKnQYEuIpIgFOgiIglCgS4ikiAU6CIiCUKBLiKSIBToIiIJ4v8B5gmuyxWP3ZQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = plt.imread(\"./batch27_i55_actual0.png\")\n",
    "change = np.transpose(img)\n",
    "plt.plot(change[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
