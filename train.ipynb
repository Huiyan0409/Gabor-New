{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim  \n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import argparse\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from skimage.util import random_noise\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "import kornia.augmentation.functional as FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./\"\n",
    "def default_loader(path):\n",
    "    return Image.open(path).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self,root, datatxt, transform=None, target_transform=None,loader=default_loader):\n",
    "        super(MyDataset,self).__init__()\n",
    "        fh = open(root + datatxt, 'r') \n",
    "        imgs = []     \n",
    "        data = []\n",
    "        label = []\n",
    "        for line in fh:                \n",
    "            line = line.rstrip()       \n",
    "            data.append(line)\n",
    "        for line in range(len(data)-1):\n",
    "            words = data[line].split()  \n",
    "            imgs.append((words[0])) \n",
    "            label.append(int(words[1]))\n",
    "            \n",
    "        \n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.label = torch.LongTensor(label)\n",
    " \n",
    "    # def __getitem__(self, index):    \n",
    "    def __getitem__(self, idx):    \n",
    "        image = Image.open(str(self.imgs[idx]))\n",
    "        # image = image.convert('RGB')\n",
    "        image = image.convert('L')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        label = self.label[idx]\n",
    "        return image, label\n",
    "    def __len__(self): \n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_trainData: 71432\n",
      "num_of_trainNewData: 89827\n",
      "num_of_testData: 8141\n",
      "num_of_testNewData: 10299\n",
      "num_of_originData: 8141\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(root='./',datatxt='train.txt', transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#         AddGaussianNoise(0., 0.05)\n",
    "        ]))\n",
    "trainNew_dataset = MyDataset(root='./',datatxt='train-new.txt', transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#         AddGaussianNoise(0., 0.05)\n",
    "        ]))\n",
    "test_dataset = MyDataset(root='./',datatxt='test.txt', transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#         AddGaussianNoise(0., 0.05)\n",
    "        ]))\n",
    "testNew_dataset = MyDataset(root='./',datatxt='test-new.txt', transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#         AddGaussianNoise(0., 0.05)\n",
    "        ]))\n",
    "origin_dataset = MyDataset(root='./',datatxt='origin.txt', transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#         AddGaussianNoise(0., 0.05)\n",
    "        ]))\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True,num_workers=1)\n",
    "trainNew_loader = DataLoader(dataset=trainNew_dataset, batch_size=64, shuffle=True,num_workers=1)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False,num_workers=1)\n",
    "testNew_loader = DataLoader(dataset=testNew_dataset, batch_size=64, shuffle=False,num_workers=1)\n",
    "origin_loader = DataLoader(dataset=origin_dataset, batch_size=64, shuffle=False,num_workers=1)\n",
    "print('num_of_trainData:', len(train_dataset))\n",
    "print('num_of_trainNewData:', len(trainNew_dataset))\n",
    "print('num_of_testData:', len(test_dataset))\n",
    "print('num_of_testNewData:', len(testNew_dataset))\n",
    "print('num_of_originData:', len(origin_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label： tensor(0) shape: (1, 19, 19)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "idx = 11\n",
    "img = test_dataset[idx][0].numpy()\n",
    "plt.imshow(img[0], cmap = 'gray')\n",
    "plt.imsave('test.png', img[0], cmap = 'gray')\n",
    "# figure, b = plt.subplots()\n",
    "# figure.set_size_inches(0.19, 0.19)\n",
    "plt.axis('off') \n",
    "print('label：',train_dataset[idx][1], 'shape:', img.shape)\n",
    "print(type(img[0]))\n",
    "matplotlib.image.imsave('name.png',img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative:  36885\n",
      "positive:  34546\n"
     ]
    }
   ],
   "source": [
    "positive = 0\n",
    "negative = 0\n",
    "for idx in range(1,71432):\n",
    "    if train_dataset[idx][1].item() == 0:\n",
    "        negative = negative+1\n",
    "    else:\n",
    "        positive = positive+1\n",
    "print('negative: ', negative)\n",
    "print('positive: ', positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaborConvPC(nn.Module):\n",
    "#     def __init__(self, kernel_size, in_channels, num_orientations, num_scales):\n",
    "#         super(GaborConvPC, self).__init__()\n",
    "#         self.sigma1, self.theta1, self.Lambda1, self.psi1, self.gamma1, self.bias1, self.weights1, self.w, self.b = self.generate_parameters(num_orientations*num_scales, in_channels)\n",
    "#         self.sigma2, self.theta2, self.Lambda2, self.psi2, self.gamma2, self.bias2, self.weights2, self.w, self.b = self.generate_parameters(num_orientations*num_scales, in_channels)\n",
    "#         # self.filter1 = self.whole_filter(in_channels, channel1, kernel_size, self.sigma1, self.theta1, self.Lambda1, self.psi1, self.gamma1)\n",
    "#         self.filter_cos = self.whole_filter(in_channels, num_orientations, num_scales, kernel_size, self.sigma1, self.theta1, self.Lambda1, self.psi1, self.gamma1, True)\n",
    "#         self.filter_sin = self.whole_filter(in_channels, num_orientations, num_scales, kernel_size, self.sigma1, self.theta1, self.Lambda1, self.psi1, self.gamma1, False)       \n",
    "        \n",
    "#         self.fc1 = nn.Linear(1*1*48, 24)\n",
    "#         self.fc2 = nn.Linear(24, 2)\n",
    "\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x_cos = F.conv2d(x, self.filter_cos, bias=self.bias1)\n",
    "#         x_sin = F.conv2d(x, self.filter_sin, bias=self.bias2)\n",
    "#         x_comb = torch.cat((x_cos, x_sin), 2)\n",
    "\n",
    "#         x_cos = x_cos.view(len(x), 1, 1, 48)\n",
    "#         x_sin = x_sin.view(len(x), 1, 1, 48)\n",
    "#         weighted_cos = (torch.matmul(x_cos, self.weights1)).view(len(x), 1)\n",
    "#         weighted_sin = (torch.matmul(x_sin, self.weights1)).view(len(x), 1)\n",
    "\n",
    "#         numerator = torch.norm(torch.cat([weighted_cos, weighted_sin], 1), dim=1)\n",
    "# #         print(\"numerator\", numerator.size())\n",
    "#         x_comb_norm = torch.norm(x_comb, dim=2)\n",
    "#         x_comb_norm = x_comb_norm.view(len(x), 1, 48)\n",
    "# #         print(\"x_comb_norm\", x_comb_norm.size())\n",
    "#         denominator = torch.matmul(x_comb_norm, torch.abs(self.weights1))\n",
    "#         denominator = denominator.view(len(x))\n",
    "\n",
    "#         pc = numerator / denominator                \n",
    "#         return torch.sigmoid(self.w * pc + self.b)\n",
    "\n",
    "\n",
    "#     def generate_parameters(self, dim_out, dim_in):\n",
    "#         sigma = nn.Parameter(torch.randn(1, 1))\n",
    "#         theta = nn.Parameter(torch.randn(1, 1))\n",
    "#         Lambda = nn.Parameter(torch.randn(1, 1))\n",
    "#         psi = nn.Parameter(torch.randn(1, 1))\n",
    "#         gamma = nn.Parameter(torch.randn(1, 1))\n",
    "#         bias = nn.Parameter(torch.randn(dim_out))\n",
    "#         weights = nn.Parameter(torch.randn(1, 48, 1))\n",
    "#         w = nn.Parameter(torch.randn(1, 1))\n",
    "#         b = nn.Parameter(torch.randn(1, 1))\n",
    "#         return sigma, theta, Lambda, psi, gamma, bias, weights, w, b\n",
    "    def __init__(self, kernel_size, in_channels, num_orientations, num_scales):\n",
    "        super(GaborConvPC, self).__init__()\n",
    "        self.sigma, self.theta, self.Lambda, self.psi, self.gamma, self.bias1, self.bias2, self.weights, self.w, self.b = self.generate_parameters(num_orientations*num_scales, in_channels)\n",
    "        self.filter_cos = self.whole_filter(in_channels, num_orientations, num_scales, kernel_size, self.sigma, self.theta, self.Lambda, self.psi, self.gamma, True)\n",
    "        self.filter_sin = self.whole_filter(in_channels, num_orientations, num_scales, kernel_size, self.sigma, self.theta, self.Lambda, self.psi, self.gamma, False)\n",
    "\n",
    "        self.fc1 = nn.Linear(1*1*48, 24)\n",
    "        self.fc2 = nn.Linear(24, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cos = F.conv2d(x, self.filter_cos, bias=self.bias1)\n",
    "        x_sin = F.conv2d(x, self.filter_sin, bias=self.bias2)\n",
    "        x_comb = torch.cat((x_cos, x_sin), 2)\n",
    "        x_cos = x_cos.view(len(x), 1, 1, 48)\n",
    "        x_sin = x_sin.view(len(x), 1, 1, 48)\n",
    "        weighted_cos = (torch.matmul(x_cos, self.weights)).view(len(x), 1)\n",
    "        weighted_sin = (torch.matmul(x_sin, self.weights)).view(len(x), 1)\n",
    "\n",
    "        numerator = torch.norm(torch.cat([weighted_cos, weighted_sin], 1), dim=1)\n",
    "        x_comb_norm = torch.norm(x_comb, dim=2)\n",
    "        x_comb_norm = x_comb_norm.view(len(x), 1, 48)\n",
    "        denominator = torch.matmul(x_comb_norm, torch.abs(self.weights))\n",
    "        denominator = denominator.view(len(x))\n",
    "        pc = numerator / denominator                \n",
    "        return torch.sigmoid(self.w * pc + self.b)\n",
    "\n",
    "\n",
    "    def generate_parameters(self, dim_out, dim_in):\n",
    "        sigma = nn.Parameter(torch.randn(1, 1))\n",
    "        theta = nn.Parameter(torch.randn(1, 1))\n",
    "        Lambda = nn.Parameter(torch.randn(1, 1))\n",
    "        psi = nn.Parameter(torch.randn(1, 1))\n",
    "        gamma = nn.Parameter(torch.randn(1, 1))\n",
    "        bias1 = nn.Parameter(torch.randn(dim_out))\n",
    "        bias2 = nn.Parameter(torch.randn(dim_out))\n",
    "        weights = nn.Parameter(torch.randn(1, 48, 1))\n",
    "        w = nn.Parameter(torch.randn(1, 1))\n",
    "        b = nn.Parameter(torch.randn(1, 1))\n",
    "        return sigma, theta, Lambda, psi, gamma, bias1, bias2, weights, w, b\n",
    "\n",
    "\n",
    "    def whole_filter(self, in_channels, num_orientations, num_scales, kernel_size, sigma, theta, Lambda, psi, gamma, cos):\n",
    "        result = torch.zeros(num_orientations*num_scales, in_channels, kernel_size, kernel_size) # \\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW\n",
    "        for i in range(num_orientations):\n",
    "            for j in range(num_scales):\n",
    "                result[i*num_scales + j] = self.one_filter(in_channels, kernel_size, sigma[0]*(2.1**j), theta[0]+i*np.pi/num_orientations, Lambda[0], psi[0], gamma[0], cos)\n",
    "        return nn.Parameter(result)\n",
    "\n",
    "\n",
    "    def one_filter(self, in_channels, kernel_size, sigma, theta, Lambda, psi, gamma, cos):\n",
    "        result = torch.zeros(in_channels, kernel_size, kernel_size)\n",
    "        for i in range(in_channels):\n",
    "            result[i] = self.gabor_fn(sigma, theta, Lambda, psi, gamma, kernel_size, cos)\n",
    "        return nn.Parameter(result)\n",
    "\n",
    "\n",
    "    def gabor_fn(self, sigma, theta, Lambda, psi, gamma, kernel_size, cos):\n",
    "        sigma_x = sigma\n",
    "        # sigma_y = float(sigma) / gamma\n",
    "        sigma_y = sigma / gamma\n",
    "\n",
    "        # Bounding box\n",
    "        half_size = (kernel_size - 1) // 2\n",
    "        ymin, xmin = -half_size, -half_size\n",
    "        ymax, xmax = half_size, half_size\n",
    "    #     (y, x) = np.meshgrid(np.arange(ymin, ymax + 1), np.arange(xmin, xmax + 1))\n",
    "        y, x = torch.meshgrid([torch.arange(ymin, ymax+1), torch.arange(xmin,xmax+1)])\n",
    "\n",
    "        if cos:\n",
    "            gb = torch.exp(-.5 * (x**2 / sigma_x**2 + y**2 / sigma_y**2)) * torch.cos(2 * np.pi / Lambda * x + psi)\n",
    "        else:\n",
    "            gb = torch.exp(-.5 * (x**2 / sigma_x**2 + y**2 / sigma_y**2)) * torch.sin(2 * np.pi / Lambda * x + psi)\n",
    "\n",
    "        # Rotation\n",
    "        degrees = theta * 180 / np.pi\n",
    "        gb = FF.apply_rotation(gb, {'degrees': torch.tensor(degrees)}, {'interpolation': torch.tensor([1]), 'align_corners': torch.tensor(True)})\n",
    "        gb = gb.squeeze()\n",
    "        return gb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> torch.Size([48])\n",
      "<class 'torch.Tensor'> torch.Size([48])\n",
      "<class 'torch.Tensor'> torch.Size([1, 48, 1])\n",
      "<class 'torch.Tensor'> torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> torch.Size([1, 1])\n",
      "<class 'torch.Tensor'> torch.Size([48, 1, 19, 19])\n",
      "<class 'torch.Tensor'> torch.Size([48, 1, 19, 19])\n",
      "<class 'torch.Tensor'> torch.Size([24, 48])\n",
      "<class 'torch.Tensor'> torch.Size([24])\n",
      "<class 'torch.Tensor'> torch.Size([2, 24])\n",
      "<class 'torch.Tensor'> torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/71432 (0%)]\tLoss: 48.603153\n",
      "Train Epoch: 1 [6400/71432 (9%)]\tLoss: 35.009838\n",
      "Train Epoch: 1 [12800/71432 (18%)]\tLoss: 20.915850\n",
      "Train Epoch: 1 [19200/71432 (27%)]\tLoss: 18.323053\n",
      "Train Epoch: 1 [25600/71432 (36%)]\tLoss: 14.125262\n",
      "Train Epoch: 1 [32000/71432 (45%)]\tLoss: 12.656539\n",
      "Train Epoch: 1 [38400/71432 (54%)]\tLoss: 17.845667\n",
      "Train Epoch: 1 [44800/71432 (63%)]\tLoss: 16.184490\n",
      "Train Epoch: 1 [51200/71432 (72%)]\tLoss: 18.283365\n",
      "Train Epoch: 1 [57600/71432 (81%)]\tLoss: 15.357291\n",
      "Train Epoch: 1 [64000/71432 (90%)]\tLoss: 9.544128\n",
      "Train Epoch: 1 [70400/71432 (98%)]\tLoss: 12.535745\n",
      "\n",
      "Test set: Average loss: 0.2239, Accuracy: 7435/8141 (91%), Positive accuracy: 3558/4091 (87%), Negative accuracy: 3877/4050 (96%), train loss: 0.3036\n",
      "\n",
      "Train Epoch: 2 [0/71432 (0%)]\tLoss: 14.142234\n",
      "Train Epoch: 2 [6400/71432 (9%)]\tLoss: 17.528366\n",
      "Train Epoch: 2 [12800/71432 (18%)]\tLoss: 19.495396\n",
      "Train Epoch: 2 [19200/71432 (27%)]\tLoss: 20.777678\n",
      "Train Epoch: 2 [25600/71432 (36%)]\tLoss: 9.979182\n",
      "Train Epoch: 2 [32000/71432 (45%)]\tLoss: 11.304832\n",
      "Train Epoch: 2 [38400/71432 (54%)]\tLoss: 12.378254\n",
      "Train Epoch: 2 [44800/71432 (63%)]\tLoss: 20.189289\n",
      "Train Epoch: 2 [51200/71432 (72%)]\tLoss: 8.403931\n",
      "Train Epoch: 2 [57600/71432 (81%)]\tLoss: 14.401905\n",
      "Train Epoch: 2 [64000/71432 (90%)]\tLoss: 10.057652\n",
      "Train Epoch: 2 [70400/71432 (98%)]\tLoss: 9.452750\n",
      "\n",
      "Test set: Average loss: 0.2018, Accuracy: 7527/8141 (92%), Positive accuracy: 3703/4091 (91%), Negative accuracy: 3824/4050 (94%), train loss: 0.2141\n",
      "\n",
      "Train Epoch: 3 [0/71432 (0%)]\tLoss: 17.356974\n",
      "Train Epoch: 3 [6400/71432 (9%)]\tLoss: 26.465014\n",
      "Train Epoch: 3 [12800/71432 (18%)]\tLoss: 18.550686\n",
      "Train Epoch: 3 [19200/71432 (27%)]\tLoss: 6.265908\n",
      "Train Epoch: 3 [25600/71432 (36%)]\tLoss: 15.318336\n",
      "Train Epoch: 3 [32000/71432 (45%)]\tLoss: 21.957382\n",
      "Train Epoch: 3 [38400/71432 (54%)]\tLoss: 10.599272\n",
      "Train Epoch: 3 [44800/71432 (63%)]\tLoss: 9.167578\n",
      "Train Epoch: 3 [51200/71432 (72%)]\tLoss: 15.493443\n",
      "Train Epoch: 3 [57600/71432 (81%)]\tLoss: 7.803523\n",
      "Train Epoch: 3 [64000/71432 (90%)]\tLoss: 13.470689\n",
      "Train Epoch: 3 [70400/71432 (98%)]\tLoss: 18.927956\n",
      "\n",
      "Test set: Average loss: 0.1916, Accuracy: 7538/8141 (93%), Positive accuracy: 3716/4091 (91%), Negative accuracy: 3822/4050 (94%), train loss: 0.1986\n",
      "\n",
      "Train Epoch: 4 [0/71432 (0%)]\tLoss: 4.525685\n",
      "Train Epoch: 4 [6400/71432 (9%)]\tLoss: 11.413948\n",
      "Train Epoch: 4 [12800/71432 (18%)]\tLoss: 10.337744\n",
      "Train Epoch: 4 [19200/71432 (27%)]\tLoss: 13.829277\n",
      "Train Epoch: 4 [25600/71432 (36%)]\tLoss: 10.648000\n",
      "Train Epoch: 4 [32000/71432 (45%)]\tLoss: 9.183097\n",
      "Train Epoch: 4 [38400/71432 (54%)]\tLoss: 8.687589\n",
      "Train Epoch: 4 [44800/71432 (63%)]\tLoss: 7.233505\n",
      "Train Epoch: 4 [51200/71432 (72%)]\tLoss: 7.977865\n",
      "Train Epoch: 4 [57600/71432 (81%)]\tLoss: 12.913610\n",
      "Train Epoch: 4 [64000/71432 (90%)]\tLoss: 7.191594\n",
      "Train Epoch: 4 [70400/71432 (98%)]\tLoss: 15.340242\n",
      "\n",
      "Test set: Average loss: 0.1946, Accuracy: 7568/8141 (93%), Positive accuracy: 3730/4091 (91%), Negative accuracy: 3838/4050 (95%), train loss: 0.1911\n",
      "\n",
      "Train Epoch: 5 [0/71432 (0%)]\tLoss: 11.228333\n",
      "Train Epoch: 5 [6400/71432 (9%)]\tLoss: 8.029409\n",
      "Train Epoch: 5 [12800/71432 (18%)]\tLoss: 7.156737\n",
      "Train Epoch: 5 [19200/71432 (27%)]\tLoss: 11.244967\n",
      "Train Epoch: 5 [25600/71432 (36%)]\tLoss: 22.286898\n",
      "Train Epoch: 5 [32000/71432 (45%)]\tLoss: 12.147220\n",
      "Train Epoch: 5 [38400/71432 (54%)]\tLoss: 10.312797\n",
      "Train Epoch: 5 [44800/71432 (63%)]\tLoss: 15.073043\n",
      "Train Epoch: 5 [51200/71432 (72%)]\tLoss: 11.375280\n",
      "Train Epoch: 5 [57600/71432 (81%)]\tLoss: 9.181170\n",
      "Train Epoch: 5 [64000/71432 (90%)]\tLoss: 30.953157\n",
      "Train Epoch: 5 [70400/71432 (98%)]\tLoss: 7.275300\n",
      "\n",
      "Test set: Average loss: 0.1780, Accuracy: 7587/8141 (93%), Positive accuracy: 3758/4091 (92%), Negative accuracy: 3829/4050 (95%), train loss: 0.1867\n",
      "\n",
      "Train Epoch: 6 [0/71432 (0%)]\tLoss: 12.656019\n",
      "Train Epoch: 6 [6400/71432 (9%)]\tLoss: 12.505353\n",
      "Train Epoch: 6 [12800/71432 (18%)]\tLoss: 15.405834\n",
      "Train Epoch: 6 [19200/71432 (27%)]\tLoss: 11.881155\n",
      "Train Epoch: 6 [25600/71432 (36%)]\tLoss: 11.198609\n",
      "Train Epoch: 6 [32000/71432 (45%)]\tLoss: 19.696980\n",
      "Train Epoch: 6 [38400/71432 (54%)]\tLoss: 11.070728\n",
      "Train Epoch: 6 [44800/71432 (63%)]\tLoss: 15.453032\n",
      "Train Epoch: 6 [51200/71432 (72%)]\tLoss: 8.923892\n",
      "Train Epoch: 6 [57600/71432 (81%)]\tLoss: 8.488992\n",
      "Train Epoch: 6 [64000/71432 (90%)]\tLoss: 18.881777\n",
      "Train Epoch: 6 [70400/71432 (98%)]\tLoss: 12.786618\n",
      "\n",
      "Test set: Average loss: 0.1751, Accuracy: 7611/8141 (93%), Positive accuracy: 3765/4091 (92%), Negative accuracy: 3846/4050 (95%), train loss: 0.1793\n",
      "\n",
      "Train Epoch: 7 [0/71432 (0%)]\tLoss: 8.927872\n",
      "Train Epoch: 7 [6400/71432 (9%)]\tLoss: 12.002681\n",
      "Train Epoch: 7 [12800/71432 (18%)]\tLoss: 16.686029\n",
      "Train Epoch: 7 [19200/71432 (27%)]\tLoss: 7.779758\n",
      "Train Epoch: 7 [25600/71432 (36%)]\tLoss: 5.392008\n",
      "Train Epoch: 7 [32000/71432 (45%)]\tLoss: 10.716162\n",
      "Train Epoch: 7 [38400/71432 (54%)]\tLoss: 16.224810\n",
      "Train Epoch: 7 [44800/71432 (63%)]\tLoss: 6.802735\n",
      "Train Epoch: 7 [51200/71432 (72%)]\tLoss: 11.243481\n",
      "Train Epoch: 7 [57600/71432 (81%)]\tLoss: 14.110431\n",
      "Train Epoch: 7 [64000/71432 (90%)]\tLoss: 15.860814\n",
      "Train Epoch: 7 [70400/71432 (98%)]\tLoss: 8.977641\n",
      "\n",
      "Test set: Average loss: 0.1761, Accuracy: 7627/8141 (94%), Positive accuracy: 3845/4091 (94%), Negative accuracy: 3782/4050 (93%), train loss: 0.1785\n",
      "\n",
      "Train Epoch: 8 [0/71432 (0%)]\tLoss: 5.503403\n",
      "Train Epoch: 8 [6400/71432 (9%)]\tLoss: 7.995509\n",
      "Train Epoch: 8 [12800/71432 (18%)]\tLoss: 14.070179\n",
      "Train Epoch: 8 [19200/71432 (27%)]\tLoss: 9.854252\n",
      "Train Epoch: 8 [25600/71432 (36%)]\tLoss: 17.035320\n",
      "Train Epoch: 8 [32000/71432 (45%)]\tLoss: 19.898281\n",
      "Train Epoch: 8 [38400/71432 (54%)]\tLoss: 7.924694\n",
      "Train Epoch: 8 [44800/71432 (63%)]\tLoss: 10.455941\n",
      "Train Epoch: 8 [51200/71432 (72%)]\tLoss: 7.597138\n",
      "Train Epoch: 8 [57600/71432 (81%)]\tLoss: 6.941180\n",
      "Train Epoch: 8 [64000/71432 (90%)]\tLoss: 19.657032\n",
      "Train Epoch: 8 [70400/71432 (98%)]\tLoss: 16.437561\n",
      "\n",
      "Test set: Average loss: 0.1734, Accuracy: 7570/8141 (93%), Positive accuracy: 3699/4091 (90%), Negative accuracy: 3871/4050 (96%), train loss: 0.1751\n",
      "\n",
      "Train Epoch: 9 [0/71432 (0%)]\tLoss: 18.733356\n",
      "Train Epoch: 9 [6400/71432 (9%)]\tLoss: 17.814951\n",
      "Train Epoch: 9 [12800/71432 (18%)]\tLoss: 10.477026\n",
      "Train Epoch: 9 [19200/71432 (27%)]\tLoss: 14.028068\n",
      "Train Epoch: 9 [25600/71432 (36%)]\tLoss: 7.695834\n",
      "Train Epoch: 9 [32000/71432 (45%)]\tLoss: 9.000176\n",
      "Train Epoch: 9 [38400/71432 (54%)]\tLoss: 8.873530\n",
      "Train Epoch: 9 [44800/71432 (63%)]\tLoss: 12.880066\n",
      "Train Epoch: 9 [51200/71432 (72%)]\tLoss: 13.494879\n",
      "Train Epoch: 9 [57600/71432 (81%)]\tLoss: 13.134168\n",
      "Train Epoch: 9 [64000/71432 (90%)]\tLoss: 10.401440\n",
      "Train Epoch: 9 [70400/71432 (98%)]\tLoss: 15.607653\n",
      "\n",
      "Test set: Average loss: 0.2045, Accuracy: 7470/8141 (92%), Positive accuracy: 3698/4091 (90%), Negative accuracy: 3772/4050 (93%), train loss: 0.1750\n",
      "\n",
      "Train Epoch: 10 [0/71432 (0%)]\tLoss: 20.252439\n",
      "Train Epoch: 10 [6400/71432 (9%)]\tLoss: 9.382166\n",
      "Train Epoch: 10 [12800/71432 (18%)]\tLoss: 15.376451\n",
      "Train Epoch: 10 [19200/71432 (27%)]\tLoss: 13.227932\n",
      "Train Epoch: 10 [25600/71432 (36%)]\tLoss: 11.506997\n",
      "Train Epoch: 10 [32000/71432 (45%)]\tLoss: 14.068561\n",
      "Train Epoch: 10 [38400/71432 (54%)]\tLoss: 11.032742\n",
      "Train Epoch: 10 [44800/71432 (63%)]\tLoss: 12.207366\n",
      "Train Epoch: 10 [51200/71432 (72%)]\tLoss: 13.479105\n",
      "Train Epoch: 10 [57600/71432 (81%)]\tLoss: 7.141129\n",
      "Train Epoch: 10 [64000/71432 (90%)]\tLoss: 8.098031\n",
      "Train Epoch: 10 [70400/71432 (98%)]\tLoss: 8.891722\n",
      "\n",
      "Test set: Average loss: 0.1667, Accuracy: 7618/8141 (94%), Positive accuracy: 3735/4091 (91%), Negative accuracy: 3883/4050 (96%), train loss: 0.1719\n",
      "\n",
      "Train Epoch: 11 [0/71432 (0%)]\tLoss: 9.210414\n",
      "Train Epoch: 11 [6400/71432 (9%)]\tLoss: 9.353351\n",
      "Train Epoch: 11 [12800/71432 (18%)]\tLoss: 16.877169\n",
      "Train Epoch: 11 [19200/71432 (27%)]\tLoss: 8.854151\n",
      "Train Epoch: 11 [25600/71432 (36%)]\tLoss: 9.458814\n",
      "Train Epoch: 11 [32000/71432 (45%)]\tLoss: 7.894894\n",
      "Train Epoch: 11 [38400/71432 (54%)]\tLoss: 17.107529\n",
      "Train Epoch: 11 [44800/71432 (63%)]\tLoss: 9.189207\n",
      "Train Epoch: 11 [51200/71432 (72%)]\tLoss: 6.933424\n",
      "Train Epoch: 11 [57600/71432 (81%)]\tLoss: 11.677713\n",
      "Train Epoch: 11 [64000/71432 (90%)]\tLoss: 12.811179\n",
      "Train Epoch: 11 [70400/71432 (98%)]\tLoss: 3.962044\n",
      "\n",
      "Test set: Average loss: 0.1899, Accuracy: 7508/8141 (92%), Positive accuracy: 3604/4091 (88%), Negative accuracy: 3904/4050 (96%), train loss: 0.1698\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [0/71432 (0%)]\tLoss: 7.344242\n",
      "Train Epoch: 12 [6400/71432 (9%)]\tLoss: 15.782540\n",
      "Train Epoch: 12 [12800/71432 (18%)]\tLoss: 9.478770\n",
      "Train Epoch: 12 [19200/71432 (27%)]\tLoss: 15.699148\n",
      "Train Epoch: 12 [25600/71432 (36%)]\tLoss: 19.821827\n",
      "Train Epoch: 12 [32000/71432 (45%)]\tLoss: 9.793143\n",
      "Train Epoch: 12 [38400/71432 (54%)]\tLoss: 10.953063\n",
      "Train Epoch: 12 [44800/71432 (63%)]\tLoss: 7.606368\n",
      "Train Epoch: 12 [51200/71432 (72%)]\tLoss: 11.750005\n",
      "Train Epoch: 12 [57600/71432 (81%)]\tLoss: 13.582916\n",
      "Train Epoch: 12 [64000/71432 (90%)]\tLoss: 6.380159\n",
      "Train Epoch: 12 [70400/71432 (98%)]\tLoss: 15.152920\n",
      "\n",
      "Test set: Average loss: 0.2355, Accuracy: 7374/8141 (91%), Positive accuracy: 3474/4091 (85%), Negative accuracy: 3900/4050 (96%), train loss: 0.1692\n",
      "\n",
      "Train Epoch: 13 [0/71432 (0%)]\tLoss: 10.149737\n",
      "Train Epoch: 13 [6400/71432 (9%)]\tLoss: 10.663799\n",
      "Train Epoch: 13 [12800/71432 (18%)]\tLoss: 9.421299\n",
      "Train Epoch: 13 [19200/71432 (27%)]\tLoss: 16.689062\n",
      "Train Epoch: 13 [25600/71432 (36%)]\tLoss: 7.014350\n",
      "Train Epoch: 13 [32000/71432 (45%)]\tLoss: 7.631852\n",
      "Train Epoch: 13 [38400/71432 (54%)]\tLoss: 7.429485\n",
      "Train Epoch: 13 [44800/71432 (63%)]\tLoss: 12.930496\n",
      "Train Epoch: 13 [51200/71432 (72%)]\tLoss: 13.336860\n",
      "Train Epoch: 13 [57600/71432 (81%)]\tLoss: 5.598822\n",
      "Train Epoch: 13 [64000/71432 (90%)]\tLoss: 4.667440\n",
      "Train Epoch: 13 [70400/71432 (98%)]\tLoss: 13.857121\n",
      "\n",
      "Test set: Average loss: 0.1631, Accuracy: 7636/8141 (94%), Positive accuracy: 3786/4091 (93%), Negative accuracy: 3850/4050 (95%), train loss: 0.1683\n",
      "\n",
      "Train Epoch: 14 [0/71432 (0%)]\tLoss: 10.898941\n",
      "Train Epoch: 14 [6400/71432 (9%)]\tLoss: 2.728095\n",
      "Train Epoch: 14 [12800/71432 (18%)]\tLoss: 9.181990\n",
      "Train Epoch: 14 [19200/71432 (27%)]\tLoss: 20.171225\n",
      "Train Epoch: 14 [25600/71432 (36%)]\tLoss: 11.842731\n",
      "Train Epoch: 14 [32000/71432 (45%)]\tLoss: 11.497828\n",
      "Train Epoch: 14 [38400/71432 (54%)]\tLoss: 6.955735\n",
      "Train Epoch: 14 [44800/71432 (63%)]\tLoss: 19.061993\n",
      "Train Epoch: 14 [51200/71432 (72%)]\tLoss: 8.988092\n",
      "Train Epoch: 14 [57600/71432 (81%)]\tLoss: 11.167157\n",
      "Train Epoch: 14 [64000/71432 (90%)]\tLoss: 16.597351\n",
      "Train Epoch: 14 [70400/71432 (98%)]\tLoss: 14.609114\n",
      "\n",
      "Test set: Average loss: 0.2125, Accuracy: 7443/8141 (91%), Positive accuracy: 3524/4091 (86%), Negative accuracy: 3919/4050 (97%), train loss: 0.1669\n",
      "\n",
      "Train Epoch: 15 [0/71432 (0%)]\tLoss: 9.780563\n",
      "Train Epoch: 15 [6400/71432 (9%)]\tLoss: 22.115625\n",
      "Train Epoch: 15 [12800/71432 (18%)]\tLoss: 4.978778\n",
      "Train Epoch: 15 [19200/71432 (27%)]\tLoss: 12.791647\n",
      "Train Epoch: 15 [25600/71432 (36%)]\tLoss: 9.499985\n",
      "Train Epoch: 15 [32000/71432 (45%)]\tLoss: 10.858666\n",
      "Train Epoch: 15 [38400/71432 (54%)]\tLoss: 8.961700\n",
      "Train Epoch: 15 [44800/71432 (63%)]\tLoss: 10.336700\n",
      "Train Epoch: 15 [51200/71432 (72%)]\tLoss: 11.724525\n",
      "Train Epoch: 15 [57600/71432 (81%)]\tLoss: 6.971073\n",
      "Train Epoch: 15 [64000/71432 (90%)]\tLoss: 8.148758\n",
      "Train Epoch: 15 [70400/71432 (98%)]\tLoss: 9.422707\n",
      "\n",
      "Test set: Average loss: 0.1829, Accuracy: 7530/8141 (92%), Positive accuracy: 3635/4091 (89%), Negative accuracy: 3895/4050 (96%), train loss: 0.1651\n",
      "\n",
      "Train Epoch: 16 [0/71432 (0%)]\tLoss: 3.326846\n",
      "Train Epoch: 16 [6400/71432 (9%)]\tLoss: 11.020884\n",
      "Train Epoch: 16 [12800/71432 (18%)]\tLoss: 10.604300\n",
      "Train Epoch: 16 [19200/71432 (27%)]\tLoss: 10.053862\n",
      "Train Epoch: 16 [25600/71432 (36%)]\tLoss: 10.738915\n",
      "Train Epoch: 16 [32000/71432 (45%)]\tLoss: 9.486262\n",
      "Train Epoch: 16 [38400/71432 (54%)]\tLoss: 15.183279\n",
      "Train Epoch: 16 [44800/71432 (63%)]\tLoss: 10.149918\n",
      "Train Epoch: 16 [51200/71432 (72%)]\tLoss: 29.103237\n",
      "Train Epoch: 16 [57600/71432 (81%)]\tLoss: 12.129298\n",
      "Train Epoch: 16 [64000/71432 (90%)]\tLoss: 8.262820\n",
      "Train Epoch: 16 [70400/71432 (98%)]\tLoss: 6.039827\n",
      "\n",
      "Test set: Average loss: 0.1654, Accuracy: 7658/8141 (94%), Positive accuracy: 3885/4091 (95%), Negative accuracy: 3773/4050 (93%), train loss: 0.1647\n",
      "\n",
      "Train Epoch: 17 [0/71432 (0%)]\tLoss: 12.343002\n",
      "Train Epoch: 17 [6400/71432 (9%)]\tLoss: 6.728675\n",
      "Train Epoch: 17 [12800/71432 (18%)]\tLoss: 21.326153\n",
      "Train Epoch: 17 [19200/71432 (27%)]\tLoss: 10.129022\n",
      "Train Epoch: 17 [25600/71432 (36%)]\tLoss: 3.894568\n",
      "Train Epoch: 17 [32000/71432 (45%)]\tLoss: 6.813775\n",
      "Train Epoch: 17 [38400/71432 (54%)]\tLoss: 12.364668\n",
      "Train Epoch: 17 [44800/71432 (63%)]\tLoss: 9.326601\n",
      "Train Epoch: 17 [51200/71432 (72%)]\tLoss: 19.456722\n",
      "Train Epoch: 17 [57600/71432 (81%)]\tLoss: 12.551371\n",
      "Train Epoch: 17 [64000/71432 (90%)]\tLoss: 10.418339\n",
      "Train Epoch: 17 [70400/71432 (98%)]\tLoss: 10.542188\n",
      "\n",
      "Test set: Average loss: 0.1905, Accuracy: 7508/8141 (92%), Positive accuracy: 3599/4091 (88%), Negative accuracy: 3909/4050 (97%), train loss: 0.1637\n",
      "\n",
      "Train Epoch: 18 [0/71432 (0%)]\tLoss: 9.135499\n",
      "Train Epoch: 18 [6400/71432 (9%)]\tLoss: 9.275992\n",
      "Train Epoch: 18 [12800/71432 (18%)]\tLoss: 9.276555\n",
      "Train Epoch: 18 [19200/71432 (27%)]\tLoss: 7.561688\n",
      "Train Epoch: 18 [25600/71432 (36%)]\tLoss: 18.356808\n",
      "Train Epoch: 18 [32000/71432 (45%)]\tLoss: 8.763074\n",
      "Train Epoch: 18 [38400/71432 (54%)]\tLoss: 13.855391\n",
      "Train Epoch: 18 [44800/71432 (63%)]\tLoss: 8.927876\n",
      "Train Epoch: 18 [51200/71432 (72%)]\tLoss: 8.061585\n",
      "Train Epoch: 18 [57600/71432 (81%)]\tLoss: 15.106682\n",
      "Train Epoch: 18 [64000/71432 (90%)]\tLoss: 8.048877\n",
      "Train Epoch: 18 [70400/71432 (98%)]\tLoss: 7.480063\n",
      "\n",
      "Test set: Average loss: 0.1977, Accuracy: 7500/8141 (92%), Positive accuracy: 3567/4091 (87%), Negative accuracy: 3933/4050 (97%), train loss: 0.1642\n",
      "\n",
      "Train Epoch: 19 [0/71432 (0%)]\tLoss: 16.305235\n",
      "Train Epoch: 19 [6400/71432 (9%)]\tLoss: 7.011967\n",
      "Train Epoch: 19 [12800/71432 (18%)]\tLoss: 22.955692\n",
      "Train Epoch: 19 [19200/71432 (27%)]\tLoss: 9.344751\n",
      "Train Epoch: 19 [25600/71432 (36%)]\tLoss: 9.915250\n",
      "Train Epoch: 19 [32000/71432 (45%)]\tLoss: 8.811098\n",
      "Train Epoch: 19 [38400/71432 (54%)]\tLoss: 20.054977\n",
      "Train Epoch: 19 [44800/71432 (63%)]\tLoss: 4.999605\n",
      "Train Epoch: 19 [51200/71432 (72%)]\tLoss: 6.266148\n",
      "Train Epoch: 19 [57600/71432 (81%)]\tLoss: 6.759766\n",
      "Train Epoch: 19 [64000/71432 (90%)]\tLoss: 5.513367\n",
      "Train Epoch: 19 [70400/71432 (98%)]\tLoss: 9.196118\n",
      "\n",
      "Test set: Average loss: 0.1761, Accuracy: 7557/8141 (93%), Positive accuracy: 3684/4091 (90%), Negative accuracy: 3873/4050 (96%), train loss: 0.1625\n",
      "\n",
      "Train Epoch: 20 [0/71432 (0%)]\tLoss: 7.233092\n",
      "Train Epoch: 20 [6400/71432 (9%)]\tLoss: 12.382372\n",
      "Train Epoch: 20 [12800/71432 (18%)]\tLoss: 12.722471\n",
      "Train Epoch: 20 [19200/71432 (27%)]\tLoss: 18.176592\n",
      "Train Epoch: 20 [25600/71432 (36%)]\tLoss: 18.616720\n",
      "Train Epoch: 20 [32000/71432 (45%)]\tLoss: 7.771194\n",
      "Train Epoch: 20 [38400/71432 (54%)]\tLoss: 5.234972\n",
      "Train Epoch: 20 [44800/71432 (63%)]\tLoss: 10.715703\n",
      "Train Epoch: 20 [51200/71432 (72%)]\tLoss: 14.558449\n",
      "Train Epoch: 20 [57600/71432 (81%)]\tLoss: 20.263721\n",
      "Train Epoch: 20 [64000/71432 (90%)]\tLoss: 6.211475\n",
      "Train Epoch: 20 [70400/71432 (98%)]\tLoss: 6.730323\n",
      "\n",
      "Test set: Average loss: 0.1738, Accuracy: 7566/8141 (93%), Positive accuracy: 3684/4091 (90%), Negative accuracy: 3882/4050 (96%), train loss: 0.1613\n",
      "\n",
      "[7435, 7527, 7538, 7568, 7587, 7611, 7627, 7570, 7470, 7618, 7508, 7374, 7636, 7443, 7530, 7658, 7508, 7500, 7557, 7566]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def myLoss(output, target):\n",
    "#     print(\"size,size:\",output.size(), target.size())\n",
    "#     print(\"size,\", ((1-2*target) * torch.log(output)).size())\n",
    "#     return -torch.sum(target * torch.log(output) + (1-target) * torch.log(1-output)) / len(output)\n",
    "    return -torch.sum((36885/34546)*target * torch.log(output) + (1-target) * torch.log(1-output)) / len(output)\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "#         loss = F.nll_loss(output, target)\n",
    "        loss = myLoss(output, target)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item()/64)\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    train_loss = sum(loss_list)/len(loss_list)\n",
    "    return train_loss\n",
    "\n",
    "        \n",
    "def test(args, model, device, test_loader,count,epoch,train_loss):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    result= [[0,0], [0,0]] \n",
    "    with torch.no_grad():\n",
    "        for batch_idx,(data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "#             test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            test_loss += myLoss(output, target)\n",
    "#             pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            t = Variable(torch.Tensor([0.5]))\n",
    "            pred = (output > t) * 1\n",
    "            pred = torch.reshape(pred, (len(target), 1))\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            cmat = confusion_matrix(target.view_as(pred), pred, labels=[0, 1]) \n",
    "            result = [[result[i][j] + cmat[i][j]  for j in range(len(result[0]))] for i in range(len(result))] \n",
    "#              # Store wrongly predicted images\n",
    "#             if epoch == 9:\n",
    "#                 wrong_idx = (pred != target.view_as(pred)).nonzero()[:, 0]\n",
    "#                 wrong_samples = data[wrong_idx]\n",
    "#                 wrong_preds = pred[wrong_idx]\n",
    "#                 actual_preds = target.view_as(pred)[wrong_idx]\n",
    "#                 for i in range(len(wrong_idx)):\n",
    "#                     sample = wrong_samples[i]\n",
    "#                     wrong_pred = wrong_preds[i]\n",
    "#                     actual_pred = actual_preds[i]\n",
    "#                     # Undo normalization\n",
    "#             #         sample = sample * 0.3081\n",
    "#             #         sample = sample + 0.1307\n",
    "#                     sample = sample * 255.\n",
    "#                     sample = sample.byte()\n",
    "#                     img = TF.to_pil_image(sample)\n",
    "#                     count = count+1\n",
    "#                     img.save('./wrong-gabor/batch{}_i{}_actual{}_pc{:.4f}.png'.format(\n",
    "#                     batch_idx,wrong_idx[i], actual_pred.item(),output[wrong_idx[i]]))\n",
    "#                     num = batch_idx * 64 + wrong_idx[i]\n",
    "#     #                 print(batch_idx,wrong_idx[i])\n",
    "#                     img_ori = origin_dataset[num][0].numpy()\n",
    "#                     plt.imsave('./wrong-gabor/batch{}_i{}_actual{}_ori.png'.format(\n",
    "#                     batch_idx,wrong_idx[i], actual_pred.item()), img_ori[0], cmap = 'gray')\n",
    "            \n",
    "                \n",
    "                    \n",
    "                \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Positive accuracy: {}/{} ({:.0f}%), Negative accuracy: {}/{} ({:.0f}%), train loss: {:.4f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset), \n",
    "        result[1][1],result[1][1]+result[1][0],100. * result[1][1]/(result[1][1]+result[1][0]),\n",
    "        result[0][0],result[0][0]+result[0][1],100. * result[0][0]/(result[0][0]+result[0][1]),train_loss))\n",
    "    return test_loss, correct\n",
    "    \n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=True,\n",
    "                        help='For Saving the current Model')\n",
    "    parser.add_argument('--std', type=float, default=0, metavar='STD',\n",
    "                        help='noise-std (default: 0)')\n",
    "    parser.add_argument('--mean', type=float, default=0, metavar='MEAN',\n",
    "                        help='noise-std (default: 0)')\n",
    "#     args = parser.parse_args()\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "#     transform=transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,))\n",
    "#         ])\n",
    "    model = GaborConvPC(19, 1, 8, 6).to(device)\n",
    "    # if torch.cuda.is_available():\n",
    "    #     torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    count = 0\n",
    "    for param in model.parameters():\n",
    "        print(type(param.data), param.size())\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_accuracy_list = []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_result = test(args, model, device, test_loader,count,epoch,train_loss)\n",
    "        test_loss_list.append(test_result[0])\n",
    "        test_accuracy_list.append(test_result[1])\n",
    "        # for param in model.parameters():\n",
    "        #     print(param.size(), param.data)\n",
    "        # print(model.state_dict())\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    if (args.save_model):\n",
    "        torch.save(model.state_dict(),\"pretrain_gabor.pt\")\n",
    "    plt.plot(train_loss_list, color='blue',label='train loss')  \n",
    "    plt.plot(test_loss_list, color='red',label='test loss')  \n",
    "    plt.legend()\n",
    "    print(test_accuracy_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/71432 (0%)]\tLoss: 0.718485\n",
      "Train Epoch: 1 [6400/71432 (9%)]\tLoss: 0.691527\n",
      "Train Epoch: 1 [12800/71432 (18%)]\tLoss: 0.689359\n",
      "Train Epoch: 1 [19200/71432 (27%)]\tLoss: 0.687260\n",
      "Train Epoch: 1 [25600/71432 (36%)]\tLoss: 0.653592\n",
      "Train Epoch: 1 [32000/71432 (45%)]\tLoss: 0.615134\n",
      "Train Epoch: 1 [38400/71432 (54%)]\tLoss: 0.576575\n",
      "Train Epoch: 1 [44800/71432 (63%)]\tLoss: 0.442903\n",
      "Train Epoch: 1 [51200/71432 (72%)]\tLoss: 0.427494\n",
      "Train Epoch: 1 [57600/71432 (81%)]\tLoss: 0.300409\n",
      "Train Epoch: 1 [64000/71432 (90%)]\tLoss: 0.450209\n",
      "Train Epoch: 1 [70400/71432 (98%)]\tLoss: 0.308525\n",
      "\n",
      "Test set: Average loss: 0.3336, Accuracy: 6975/8141 (86%), Positive accuracy: 3514/4091 (86%), Negative accuracy: 3461/4050 (85%), f1 score: 0.8568, Train loss: 0.5551\n",
      "\n",
      "Train Epoch: 2 [0/71432 (0%)]\tLoss: 0.299476\n",
      "Train Epoch: 2 [6400/71432 (9%)]\tLoss: 0.445150\n",
      "Train Epoch: 2 [12800/71432 (18%)]\tLoss: 0.365812\n",
      "Train Epoch: 2 [19200/71432 (27%)]\tLoss: 0.251055\n",
      "Train Epoch: 2 [25600/71432 (36%)]\tLoss: 0.320019\n",
      "Train Epoch: 2 [32000/71432 (45%)]\tLoss: 0.303056\n",
      "Train Epoch: 2 [38400/71432 (54%)]\tLoss: 0.422739\n",
      "Train Epoch: 2 [44800/71432 (63%)]\tLoss: 0.361774\n",
      "Train Epoch: 2 [51200/71432 (72%)]\tLoss: 0.293799\n",
      "Train Epoch: 2 [57600/71432 (81%)]\tLoss: 0.251018\n",
      "Train Epoch: 2 [64000/71432 (90%)]\tLoss: 0.343587\n",
      "Train Epoch: 2 [70400/71432 (98%)]\tLoss: 0.228442\n",
      "\n",
      "Test set: Average loss: 0.2174, Accuracy: 7443/8141 (91%), Positive accuracy: 3850/4091 (94%), Negative accuracy: 3593/4050 (89%), f1 score: 0.9150, Train loss: 0.3438\n",
      "\n",
      "Train Epoch: 3 [0/71432 (0%)]\tLoss: 0.443341\n",
      "Train Epoch: 3 [6400/71432 (9%)]\tLoss: 0.219584\n",
      "Train Epoch: 3 [12800/71432 (18%)]\tLoss: 0.277727\n",
      "Train Epoch: 3 [19200/71432 (27%)]\tLoss: 0.262271\n",
      "Train Epoch: 3 [25600/71432 (36%)]\tLoss: 0.301710\n",
      "Train Epoch: 3 [32000/71432 (45%)]\tLoss: 0.246973\n",
      "Train Epoch: 3 [38400/71432 (54%)]\tLoss: 0.157004\n",
      "Train Epoch: 3 [44800/71432 (63%)]\tLoss: 0.176041\n",
      "Train Epoch: 3 [51200/71432 (72%)]\tLoss: 0.208051\n",
      "Train Epoch: 3 [57600/71432 (81%)]\tLoss: 0.213538\n",
      "Train Epoch: 3 [64000/71432 (90%)]\tLoss: 0.256698\n",
      "Train Epoch: 3 [70400/71432 (98%)]\tLoss: 0.175492\n",
      "\n",
      "Test set: Average loss: 0.1414, Accuracy: 7681/8141 (94%), Positive accuracy: 3962/4091 (97%), Negative accuracy: 3719/4050 (92%), f1 score: 0.9442, Train loss: 0.2273\n",
      "\n",
      "Train Epoch: 4 [0/71432 (0%)]\tLoss: 0.180314\n",
      "Train Epoch: 4 [6400/71432 (9%)]\tLoss: 0.230155\n",
      "Train Epoch: 4 [12800/71432 (18%)]\tLoss: 0.197733\n",
      "Train Epoch: 4 [19200/71432 (27%)]\tLoss: 0.231904\n",
      "Train Epoch: 4 [25600/71432 (36%)]\tLoss: 0.191186\n",
      "Train Epoch: 4 [32000/71432 (45%)]\tLoss: 0.115184\n",
      "Train Epoch: 4 [38400/71432 (54%)]\tLoss: 0.177887\n",
      "Train Epoch: 4 [44800/71432 (63%)]\tLoss: 0.086036\n",
      "Train Epoch: 4 [51200/71432 (72%)]\tLoss: 0.211784\n",
      "Train Epoch: 4 [57600/71432 (81%)]\tLoss: 0.141040\n",
      "Train Epoch: 4 [64000/71432 (90%)]\tLoss: 0.148401\n",
      "Train Epoch: 4 [70400/71432 (98%)]\tLoss: 0.036484\n",
      "\n",
      "Test set: Average loss: 0.1230, Accuracy: 7734/8141 (95%), Positive accuracy: 3904/4091 (95%), Negative accuracy: 3830/4050 (95%), f1 score: 0.9500, Train loss: 0.1776\n",
      "\n",
      "Train Epoch: 5 [0/71432 (0%)]\tLoss: 0.210174\n",
      "Train Epoch: 5 [6400/71432 (9%)]\tLoss: 0.286700\n",
      "Train Epoch: 5 [12800/71432 (18%)]\tLoss: 0.117264\n",
      "Train Epoch: 5 [19200/71432 (27%)]\tLoss: 0.088031\n",
      "Train Epoch: 5 [25600/71432 (36%)]\tLoss: 0.259243\n",
      "Train Epoch: 5 [32000/71432 (45%)]\tLoss: 0.194355\n",
      "Train Epoch: 5 [38400/71432 (54%)]\tLoss: 0.172431\n",
      "Train Epoch: 5 [44800/71432 (63%)]\tLoss: 0.112972\n",
      "Train Epoch: 5 [51200/71432 (72%)]\tLoss: 0.260724\n",
      "Train Epoch: 5 [57600/71432 (81%)]\tLoss: 0.203275\n",
      "Train Epoch: 5 [64000/71432 (90%)]\tLoss: 0.043151\n",
      "Train Epoch: 5 [70400/71432 (98%)]\tLoss: 0.146890\n",
      "\n",
      "Test set: Average loss: 0.1225, Accuracy: 7739/8141 (95%), Positive accuracy: 3894/4091 (95%), Negative accuracy: 3845/4050 (95%), f1 score: 0.9506, Train loss: 0.1645\n",
      "\n",
      "Train Epoch: 6 [0/71432 (0%)]\tLoss: 0.078387\n",
      "Train Epoch: 6 [6400/71432 (9%)]\tLoss: 0.148791\n",
      "Train Epoch: 6 [12800/71432 (18%)]\tLoss: 0.206266\n",
      "Train Epoch: 6 [19200/71432 (27%)]\tLoss: 0.140756\n",
      "Train Epoch: 6 [25600/71432 (36%)]\tLoss: 0.203682\n",
      "Train Epoch: 6 [32000/71432 (45%)]\tLoss: 0.164077\n",
      "Train Epoch: 6 [38400/71432 (54%)]\tLoss: 0.221746\n",
      "Train Epoch: 6 [44800/71432 (63%)]\tLoss: 0.058069\n",
      "Train Epoch: 6 [51200/71432 (72%)]\tLoss: 0.176698\n",
      "Train Epoch: 6 [57600/71432 (81%)]\tLoss: 0.276012\n",
      "Train Epoch: 6 [64000/71432 (90%)]\tLoss: 0.153779\n",
      "Train Epoch: 6 [70400/71432 (98%)]\tLoss: 0.148153\n",
      "\n",
      "Test set: Average loss: 0.1238, Accuracy: 7737/8141 (95%), Positive accuracy: 3841/4091 (94%), Negative accuracy: 3896/4050 (96%), f1 score: 0.9505, Train loss: 0.1613\n",
      "\n",
      "Train Epoch: 7 [0/71432 (0%)]\tLoss: 0.184694\n",
      "Train Epoch: 7 [6400/71432 (9%)]\tLoss: 0.103054\n",
      "Train Epoch: 7 [12800/71432 (18%)]\tLoss: 0.157064\n",
      "Train Epoch: 7 [19200/71432 (27%)]\tLoss: 0.113281\n",
      "Train Epoch: 7 [25600/71432 (36%)]\tLoss: 0.130682\n",
      "Train Epoch: 7 [32000/71432 (45%)]\tLoss: 0.159866\n",
      "Train Epoch: 7 [38400/71432 (54%)]\tLoss: 0.162941\n",
      "Train Epoch: 7 [44800/71432 (63%)]\tLoss: 0.145084\n",
      "Train Epoch: 7 [51200/71432 (72%)]\tLoss: 0.261466\n",
      "Train Epoch: 7 [57600/71432 (81%)]\tLoss: 0.191512\n",
      "Train Epoch: 7 [64000/71432 (90%)]\tLoss: 0.269213\n",
      "Train Epoch: 7 [70400/71432 (98%)]\tLoss: 0.106283\n",
      "\n",
      "Test set: Average loss: 0.1199, Accuracy: 7742/8141 (95%), Positive accuracy: 3867/4091 (95%), Negative accuracy: 3875/4050 (96%), f1 score: 0.9510, Train loss: 0.1553\n",
      "\n",
      "Train Epoch: 8 [0/71432 (0%)]\tLoss: 0.243291\n",
      "Train Epoch: 8 [6400/71432 (9%)]\tLoss: 0.083588\n",
      "Train Epoch: 8 [12800/71432 (18%)]\tLoss: 0.197477\n",
      "Train Epoch: 8 [19200/71432 (27%)]\tLoss: 0.149264\n",
      "Train Epoch: 8 [25600/71432 (36%)]\tLoss: 0.093644\n",
      "Train Epoch: 8 [32000/71432 (45%)]\tLoss: 0.093769\n",
      "Train Epoch: 8 [38400/71432 (54%)]\tLoss: 0.165935\n",
      "Train Epoch: 8 [44800/71432 (63%)]\tLoss: 0.217689\n",
      "Train Epoch: 8 [51200/71432 (72%)]\tLoss: 0.152986\n",
      "Train Epoch: 8 [57600/71432 (81%)]\tLoss: 0.228670\n",
      "Train Epoch: 8 [64000/71432 (90%)]\tLoss: 0.059225\n",
      "Train Epoch: 8 [70400/71432 (98%)]\tLoss: 0.095983\n",
      "\n",
      "Test set: Average loss: 0.1217, Accuracy: 7735/8141 (95%), Positive accuracy: 3859/4091 (94%), Negative accuracy: 3876/4050 (96%), f1 score: 0.9501, Train loss: 0.1565\n",
      "\n",
      "Train Epoch: 9 [0/71432 (0%)]\tLoss: 0.297120\n",
      "Train Epoch: 9 [6400/71432 (9%)]\tLoss: 0.140483\n",
      "Train Epoch: 9 [12800/71432 (18%)]\tLoss: 0.106994\n",
      "Train Epoch: 9 [19200/71432 (27%)]\tLoss: 0.157391\n",
      "Train Epoch: 9 [25600/71432 (36%)]\tLoss: 0.139814\n",
      "Train Epoch: 9 [32000/71432 (45%)]\tLoss: 0.049767\n",
      "Train Epoch: 9 [38400/71432 (54%)]\tLoss: 0.190448\n",
      "Train Epoch: 9 [44800/71432 (63%)]\tLoss: 0.117452\n",
      "Train Epoch: 9 [51200/71432 (72%)]\tLoss: 0.090019\n",
      "Train Epoch: 9 [57600/71432 (81%)]\tLoss: 0.234828\n",
      "Train Epoch: 9 [64000/71432 (90%)]\tLoss: 0.141566\n",
      "Train Epoch: 9 [70400/71432 (98%)]\tLoss: 0.080786\n",
      "\n",
      "Test set: Average loss: 0.1210, Accuracy: 7738/8141 (95%), Positive accuracy: 3855/4091 (94%), Negative accuracy: 3883/4050 (96%), f1 score: 0.9505, Train loss: 0.1545\n",
      "\n",
      "Train Epoch: 10 [0/71432 (0%)]\tLoss: 0.065929\n",
      "Train Epoch: 10 [6400/71432 (9%)]\tLoss: 0.120226\n",
      "Train Epoch: 10 [12800/71432 (18%)]\tLoss: 0.083925\n",
      "Train Epoch: 10 [19200/71432 (27%)]\tLoss: 0.130272\n",
      "Train Epoch: 10 [25600/71432 (36%)]\tLoss: 0.113653\n",
      "Train Epoch: 10 [32000/71432 (45%)]\tLoss: 0.261310\n",
      "Train Epoch: 10 [38400/71432 (54%)]\tLoss: 0.296257\n",
      "Train Epoch: 10 [44800/71432 (63%)]\tLoss: 0.168307\n",
      "Train Epoch: 10 [51200/71432 (72%)]\tLoss: 0.182790\n",
      "Train Epoch: 10 [57600/71432 (81%)]\tLoss: 0.131973\n",
      "Train Epoch: 10 [64000/71432 (90%)]\tLoss: 0.131828\n",
      "Train Epoch: 10 [70400/71432 (98%)]\tLoss: 0.108853\n",
      "\n",
      "Test set: Average loss: 0.1237, Accuracy: 7729/8141 (95%), Positive accuracy: 3822/4091 (93%), Negative accuracy: 3907/4050 (96%), f1 score: 0.9495, Train loss: 0.1539\n",
      "\n",
      "Train Epoch: 11 [0/71432 (0%)]\tLoss: 0.030613\n",
      "Train Epoch: 11 [6400/71432 (9%)]\tLoss: 0.157321\n",
      "Train Epoch: 11 [12800/71432 (18%)]\tLoss: 0.218577\n",
      "Train Epoch: 11 [19200/71432 (27%)]\tLoss: 0.117208\n",
      "Train Epoch: 11 [25600/71432 (36%)]\tLoss: 0.122601\n",
      "Train Epoch: 11 [32000/71432 (45%)]\tLoss: 0.092960\n",
      "Train Epoch: 11 [38400/71432 (54%)]\tLoss: 0.106975\n",
      "Train Epoch: 11 [44800/71432 (63%)]\tLoss: 0.117219\n",
      "Train Epoch: 11 [51200/71432 (72%)]\tLoss: 0.283319\n",
      "Train Epoch: 11 [57600/71432 (81%)]\tLoss: 0.159295\n",
      "Train Epoch: 11 [64000/71432 (90%)]\tLoss: 0.080732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [70400/71432 (98%)]\tLoss: 0.180445\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 2484) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 2484) is killed by signal: Abort trap: 6. ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a060955c0963>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-a060955c0963>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mtest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0mtest_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mtest_accuracy_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-a060955c0963>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, device, test_loader, count, epoch, train_loss)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 2484) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, 1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.dropout3 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(6400, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 36)\n",
    "        self.fc3 = nn.Linear(36, 2)\n",
    "#         self.fc3 = nn.Linear(128, 2)\n",
    "#         self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 3, 1)\n",
    "# #         self.conv4 = nn.Conv2d(64, 128, 3, 1)\n",
    "# #         self.conv5 = nn.Conv2d(128, 256, 3, 1)\n",
    "#         # self.fc1 = nn.Linear(9216, 128)\n",
    "# #         self.fc2 = nn.Linear(128, 2)\n",
    "#         self.fc1 = nn.Linear(2304, 1024)\n",
    "#         self.fc2 = nn.Linear(1024, 128)\n",
    "#         self.fc3 = nn.Linear(128, 2)\n",
    "# #         self.fc4 = nn.Linear(256, 64)\n",
    "# # #         self.fc5 = nn.Linear(64, 16)\n",
    "# # #         self.fc6 = nn.Linear(16, 6)\n",
    "# #         self.fc5 = nn.Linear(64, 2)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x_ori):\n",
    "        x = self.conv1(x_ori)\n",
    "        conv1 = F.relu(x)  #store conv1\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv2 = F.relu(conv2) #store conv2\n",
    "#         x = F.max_pool2d(conv2, 2)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv3 = F.relu(conv3) #store conv2\n",
    "        conv4 = self.conv4(conv3)\n",
    "        conv4 = F.relu(conv4) #store conv2\n",
    "        x = F.max_pool2d(conv4, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc3(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dropout3(x)\n",
    "#         x = self.fc3(x)\n",
    "#         x = self.conv1(x_ori)\n",
    "#         conv1 = F.relu(x)  #store conv1\n",
    "#         conv2 = self.conv2(conv1)\n",
    "#         conv2 = F.relu(conv2) #store conv2\n",
    "#         conv3 = self.conv3(conv2)\n",
    "#         conv3 = F.relu(conv3) #store conv2\n",
    "# #         conv4 = self.conv4(conv3)\n",
    "# #         conv4 = F.relu(conv4) #store conv2\n",
    "# #         conv5 = self.conv5(conv4)\n",
    "# #         conv5 = F.relu(conv5) #store conv2\n",
    "#         x = F.max_pool2d(conv3, 2)\n",
    "# #         x = self.dropout1(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "# #         x = self.dropout2(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.fc3(x)\n",
    "# #         x = F.relu(x)\n",
    "# #         x = self.fc4(x)\n",
    "# #         x = F.relu(x)\n",
    "# # #         x = self.fc5(x)\n",
    "# # #         x = F.relu(x)\n",
    "# # #         x = self.fc6(x)\n",
    "# # #         x = F.relu(x)\n",
    "# #         x = self.fc5(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    num = 0\n",
    "    loss_list = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        weight = torch.tensor([34546/(36885+34546),36885/(36885+34546)])\n",
    "        loss = F.nll_loss(output, target, weight = weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "#         if epoch == 3 and batch_idx == 10:\n",
    "#             for i in range(64):\n",
    "#                 img_ori = output[1][i].numpy()\n",
    "#                 real = target[i].detach().numpy()\n",
    "#                 predict = output[0][i].detach().numpy()\n",
    "#                 plt.imsave('./CNN/real{}_{}_a.png'.format(real,num),img_ori[0],cmap = 'gray')\n",
    "#                 img_conv1 = output[2][i].detach().numpy()\n",
    "#                 for j in range(len(img_conv1)):\n",
    "#                     plt.imsave('./CNN/real{}_{}_conv1_{}.png'.format(real, num, j),img_conv1[0],cmap = 'gray')\n",
    "#                 img_conv2 = output[3][i].detach().numpy()\n",
    "#                 for k in range(len(img_conv2)):\n",
    "#                     plt.imsave('./CNN/real{}_{}_conv2_{}.png'.format(real,num,k),img_conv2[0],cmap = 'gray')\n",
    "#                 num = num+1\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "        \n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} '.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "            if args.dry_run:\n",
    "                break\n",
    "    train_loss = sum(loss_list)/len(loss_list)\n",
    "    return train_loss\n",
    "                \n",
    "\n",
    "\n",
    "def test(model, device, test_loader,count,epoch,train_loss):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    result= [[0,0], [0,0]] \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            cmat = confusion_matrix(target.view_as(pred), pred, labels=[0, 1]) \n",
    "            result = [[result[i][j] + cmat[i][j]  for j in range(len(result[0]))] for i in range(len(result))] \n",
    "            \n",
    "#             # Store wrongly predicted images\n",
    "#             wrong_idx = (pred != target.view_as(pred)).nonzero()[:, 0]\n",
    "#             wrong_samples = data[wrong_idx]\n",
    "#             wrong_preds = pred[wrong_idx]\n",
    "#             actual_preds = target.view_as(pred)[wrong_idx]\n",
    "#             for i in range(len(wrong_idx)):\n",
    "#                 sample = wrong_samples[i]\n",
    "#                 wrong_pred = wrong_preds[i]\n",
    "#                 actual_pred = actual_preds[i]\n",
    "#                 # Undo normalization\n",
    "#         #         sample = sample * 0.3081\n",
    "#         #         sample = sample + 0.1307\n",
    "#                 sample = sample * 255.\n",
    "#                 sample = sample.byte()\n",
    "#                 img = TF.to_pil_image(sample)\n",
    "#                 count = count+1\n",
    "#                 img.save('./wrong/epoch{}_batch{}_idx{}_actual{}.png'.format(\n",
    "#                 epoch,batch_idx,wrong_idx[i], actual_pred.item()))\n",
    "#                 num = batch_idx * 64 + wrong_idx[i]\n",
    "#                 img_ori = origin_dataset[num][0].numpy()\n",
    "#                 plt.imsave('./wrong/epoch{}_batch{}_idx{}_actual{}_ori.png'.format(\n",
    "#                 epoch,batch_idx,wrong_idx[i], actual_pred.item()), img_ori[0], cmap = 'gray')\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    precision = result[1][1]/(result[1][1]+result[0][1])\n",
    "    recall = result[0][0]/(result[0][0]+result[1][0])\n",
    "    f1score = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Positive accuracy: {}/{} ({:.0f}%), Negative accuracy: {}/{} ({:.0f}%), f1 score: {:.4f}, Train loss: {:.4f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset), \n",
    "        result[1][1],result[1][1]+result[1][0],100. * result[1][1]/(result[1][1]+result[1][0]),\n",
    "        result[0][0],result[0][0]+result[0][1],100. * result[0][0]/(result[0][0]+result[0][1]),f1score,train_loss))\n",
    "    return test_loss, correct\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'batch_size': args.batch_size}\n",
    "    if use_cuda:\n",
    "        kwargs.update({'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True},\n",
    "                     )\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "    count = 0\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_accuracy_list = []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_result = test(model, device, test_loader, count, epoch,train_loss)\n",
    "        test_loss_list.append(test_result[0])\n",
    "        test_accuracy_list.append(test_result[1])\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "    plt.plot(train_loss_list, color='blue',label='train loss')  \n",
    "    plt.plot(test_loss_list, color='red',label='test loss')  \n",
    "    plt.legend()\n",
    "    print(test_accuracy_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-14-d90b47fe309a>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-d90b47fe309a>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    scale = 0\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "                figure, b = plt.subplots()\n",
    "                figure.set_size_inches(19, 19)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(model.filter_cos[i].detach().numpy()[0], cmap='gray')\n",
    "#                 np.savetxt('train-coeff.txt', model.filter_cos[i].detach().numpy()[0], delimiter='    ',fmt='%1.2f')\n",
    "#                 img.save('./filter/{}_ori_{}scale_{}.png'.format('cos',ori,scale))\n",
    "                si = model.sigma1.detach().numpy()*(2.1**scale)\n",
    "                de = model.theta1.detach().numpy()+ori*np.pi/8\n",
    "                plt.savefig(\"./filter-11.3_9/%s_scale_%.2fdeg_%.2f.png\" % ('cos',si,de), dpi=1,pad_inches=0.0,bbox_inches='tight')\n",
    "                if scale == 5:\n",
    "                    scale = 0\n",
    "                    ori = ori+1\n",
    "                else:\n",
    "                    scale = scale + 1\n",
    "\n",
    "            scale = 0\n",
    "            ori = 0\n",
    "            for i in range(len(model.filter_sin)):\n",
    "#                 sample = model.filter_sin[i]\n",
    "#                 sample = sample * 255.\n",
    "#                 sample = sample.byte()\n",
    "#                 img = TF.to_pil_image(sample)\n",
    "                figure, b = plt.subplots()\n",
    "                figure.set_size_inches(0.19, 0.19)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(model.filter_sin[i].detach().numpy()[0], cmap='gray')\n",
    "#                 np.savetxt('train-coeff.txt', model.filter_sin[i].detach().numpy()[0], delimiter='    ',fmt='%1.2f')\n",
    "                si = model.sigma1.detach().numpy()*(2.1**scale)\n",
    "                de = model.theta1.detach().numpy()+ori*np.pi/8\n",
    "#                 img.save('./filter/{}_ori_{}scale_{}.png'.ormat('sin',ori,scale))\n",
    "                plt.savefig(\"./filter-11.3_9/%s_scale_%.2fdeg_%.2f.png\" % ('sin',si,de), dpi=100,pad_inches=0.0,bbox_inches='tight')\n",
    "                if scale == 5:\n",
    "                    scale = 0\n",
    "                    ori = ori + 1\n",
    "                else:\n",
    "                    scale = scale + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_gabor_model = torch.load('pretrain_gabor.pt')\n",
    "filter_cos = pretrain_gabor_model['filter_cos']\n",
    "filter_sin = pretrain_gabor_model['filter_sin']\n",
    "bias1 = pretrain_gabor_model['bias1']\n",
    "bias2 = pretrain_gabor_model['bias2']\n",
    "weights = pretrain_gabor_model['weights']\n",
    "w = pretrain_gabor_model['w']\n",
    "b = pretrain_gabor_model['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc(x): #x as a picture\n",
    "    x_cos = F.conv2d(x, filter_cos, bias=bias1)\n",
    "    x_sin = F.conv2d(x, filter_sin, bias=bias2)\n",
    "    x_comb = torch.cat((x_cos, x_sin), 2)\n",
    "\n",
    "    x_cos = x_cos.view(len(x), 1, 1, 24)\n",
    "    x_sin = x_sin.view(len(x), 1, 1, 24)\n",
    "    weighted_cos = (torch.matmul(x_cos, weights)).view(len(x), 1)\n",
    "    weighted_sin = (torch.matmul(x_sin, weights)).view(len(x), 1)\n",
    "\n",
    "    numerator = torch.norm(torch.cat([weighted_cos, weighted_sin], 1), dim=1)\n",
    "#         print(\"numerator\", numerator.size())\n",
    "    x_comb_norm = torch.norm(x_comb, dim=2)\n",
    "    x_comb_norm = x_comb_norm.view(len(x), 1, 24)\n",
    "#         print(\"x_comb_norm\", x_comb_norm.size())\n",
    "    denominator = torch.matmul(x_comb_norm, torch.abs(weights))\n",
    "    denominator = denominator.view(len(x))\n",
    "#         print(\"size:\", numerator.size(), denominator.size())\n",
    "    pc = numerator / denominator                \n",
    "#     return torch.sigmoid(w * pc + b)\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/71432 (0%)]\tLoss: 0.647960\n",
      "Train Epoch: 1 [6400/71432 (9%)]\tLoss: 0.251740\n",
      "Train Epoch: 1 [12800/71432 (18%)]\tLoss: 0.346312\n",
      "Train Epoch: 1 [19200/71432 (27%)]\tLoss: 0.193182\n",
      "Train Epoch: 1 [25600/71432 (36%)]\tLoss: 0.153147\n",
      "Train Epoch: 1 [32000/71432 (45%)]\tLoss: 0.264010\n",
      "Train Epoch: 1 [38400/71432 (54%)]\tLoss: 0.123752\n",
      "Train Epoch: 1 [44800/71432 (63%)]\tLoss: 0.157175\n",
      "Train Epoch: 1 [51200/71432 (72%)]\tLoss: 0.131665\n",
      "Train Epoch: 1 [57600/71432 (81%)]\tLoss: 0.072287\n",
      "Train Epoch: 1 [64000/71432 (90%)]\tLoss: 0.055243\n",
      "Train Epoch: 1 [70400/71432 (98%)]\tLoss: 0.115366\n",
      "\n",
      "Test set: Average loss: 0.1707, Accuracy: 7542/8141 (93%), Positive accuracy: 3570/4091 (87%), Negative accuracy: 3972/4050 (98%), f1 score: 0.9289\n",
      "\n",
      "Train Epoch: 2 [0/71432 (0%)]\tLoss: 0.085896\n",
      "Train Epoch: 2 [6400/71432 (9%)]\tLoss: 0.063816\n",
      "Train Epoch: 2 [12800/71432 (18%)]\tLoss: 0.232834\n",
      "Train Epoch: 2 [19200/71432 (27%)]\tLoss: 0.094140\n",
      "Train Epoch: 2 [25600/71432 (36%)]\tLoss: 0.086009\n",
      "Train Epoch: 2 [32000/71432 (45%)]\tLoss: 0.061543\n",
      "Train Epoch: 2 [38400/71432 (54%)]\tLoss: 0.101196\n",
      "Train Epoch: 2 [44800/71432 (63%)]\tLoss: 0.121557\n",
      "Train Epoch: 2 [51200/71432 (72%)]\tLoss: 0.109980\n",
      "Train Epoch: 2 [57600/71432 (81%)]\tLoss: 0.042661\n",
      "Train Epoch: 2 [64000/71432 (90%)]\tLoss: 0.101936\n",
      "Train Epoch: 2 [70400/71432 (98%)]\tLoss: 0.053546\n",
      "\n",
      "Test set: Average loss: 0.1683, Accuracy: 7698/8141 (95%), Positive accuracy: 3866/4091 (95%), Negative accuracy: 3832/4050 (95%), f1 score: 0.9456\n",
      "\n",
      "Train Epoch: 3 [0/71432 (0%)]\tLoss: 0.112622\n",
      "Train Epoch: 3 [6400/71432 (9%)]\tLoss: 0.051621\n",
      "Train Epoch: 3 [12800/71432 (18%)]\tLoss: 0.100049\n",
      "Train Epoch: 3 [19200/71432 (27%)]\tLoss: 0.103712\n",
      "Train Epoch: 3 [25600/71432 (36%)]\tLoss: 0.153880\n",
      "Train Epoch: 3 [32000/71432 (45%)]\tLoss: 0.081229\n",
      "Train Epoch: 3 [38400/71432 (54%)]\tLoss: 0.075977\n",
      "Train Epoch: 3 [44800/71432 (63%)]\tLoss: 0.062409\n",
      "Train Epoch: 3 [51200/71432 (72%)]\tLoss: 0.056035\n",
      "Train Epoch: 3 [57600/71432 (81%)]\tLoss: 0.096312\n",
      "Train Epoch: 3 [64000/71432 (90%)]\tLoss: 0.066721\n",
      "Train Epoch: 3 [70400/71432 (98%)]\tLoss: 0.049232\n",
      "\n",
      "Test set: Average loss: 0.0964, Accuracy: 7836/8141 (96%), Positive accuracy: 3854/4091 (94%), Negative accuracy: 3982/4050 (98%), f1 score: 0.9629\n",
      "\n",
      "Train Epoch: 4 [0/71432 (0%)]\tLoss: 0.119895\n",
      "Train Epoch: 4 [6400/71432 (9%)]\tLoss: 0.089348\n",
      "Train Epoch: 4 [12800/71432 (18%)]\tLoss: 0.055229\n",
      "Train Epoch: 4 [19200/71432 (27%)]\tLoss: 0.054992\n",
      "Train Epoch: 4 [25600/71432 (36%)]\tLoss: 0.073277\n",
      "Train Epoch: 4 [32000/71432 (45%)]\tLoss: 0.160695\n",
      "Train Epoch: 4 [38400/71432 (54%)]\tLoss: 0.072009\n",
      "Train Epoch: 4 [44800/71432 (63%)]\tLoss: 0.034077\n",
      "Train Epoch: 4 [51200/71432 (72%)]\tLoss: 0.059753\n",
      "Train Epoch: 4 [57600/71432 (81%)]\tLoss: 0.070264\n",
      "Train Epoch: 4 [64000/71432 (90%)]\tLoss: 0.081689\n",
      "Train Epoch: 4 [70400/71432 (98%)]\tLoss: 0.122434\n",
      "\n",
      "Test set: Average loss: 0.1028, Accuracy: 7804/8141 (96%), Positive accuracy: 3834/4091 (94%), Negative accuracy: 3970/4050 (98%), f1 score: 0.9590\n",
      "\n",
      "Train Epoch: 5 [0/71432 (0%)]\tLoss: 0.082068\n",
      "Train Epoch: 5 [6400/71432 (9%)]\tLoss: 0.040525\n",
      "Train Epoch: 5 [12800/71432 (18%)]\tLoss: 0.142328\n",
      "Train Epoch: 5 [19200/71432 (27%)]\tLoss: 0.082739\n",
      "Train Epoch: 5 [25600/71432 (36%)]\tLoss: 0.042756\n",
      "Train Epoch: 5 [32000/71432 (45%)]\tLoss: 0.088684\n",
      "Train Epoch: 5 [38400/71432 (54%)]\tLoss: 0.059672\n",
      "Train Epoch: 5 [44800/71432 (63%)]\tLoss: 0.116562\n",
      "Train Epoch: 5 [51200/71432 (72%)]\tLoss: 0.136567\n",
      "Train Epoch: 5 [57600/71432 (81%)]\tLoss: 0.103806\n",
      "Train Epoch: 5 [64000/71432 (90%)]\tLoss: 0.048339\n",
      "Train Epoch: 5 [70400/71432 (98%)]\tLoss: 0.016156\n",
      "\n",
      "Test set: Average loss: 0.0869, Accuracy: 7859/8141 (97%), Positive accuracy: 3875/4091 (95%), Negative accuracy: 3984/4050 (98%), f1 score: 0.9656\n",
      "\n",
      "Train Epoch: 6 [0/71432 (0%)]\tLoss: 0.101432\n",
      "Train Epoch: 6 [6400/71432 (9%)]\tLoss: 0.182548\n",
      "Train Epoch: 6 [12800/71432 (18%)]\tLoss: 0.094978\n",
      "Train Epoch: 6 [19200/71432 (27%)]\tLoss: 0.020815\n",
      "Train Epoch: 6 [25600/71432 (36%)]\tLoss: 0.061119\n",
      "Train Epoch: 6 [32000/71432 (45%)]\tLoss: 0.061285\n",
      "Train Epoch: 6 [38400/71432 (54%)]\tLoss: 0.158339\n",
      "Train Epoch: 6 [44800/71432 (63%)]\tLoss: 0.038288\n",
      "Train Epoch: 6 [51200/71432 (72%)]\tLoss: 0.076611\n",
      "Train Epoch: 6 [57600/71432 (81%)]\tLoss: 0.059271\n",
      "Train Epoch: 6 [64000/71432 (90%)]\tLoss: 0.088861\n",
      "Train Epoch: 6 [70400/71432 (98%)]\tLoss: 0.162610\n",
      "\n",
      "Test set: Average loss: 0.0756, Accuracy: 7892/8141 (97%), Positive accuracy: 3904/4091 (95%), Negative accuracy: 3988/4050 (98%), f1 score: 0.9696\n",
      "\n",
      "Train Epoch: 7 [0/71432 (0%)]\tLoss: 0.046670\n",
      "Train Epoch: 7 [6400/71432 (9%)]\tLoss: 0.017126\n",
      "Train Epoch: 7 [12800/71432 (18%)]\tLoss: 0.038059\n",
      "Train Epoch: 7 [19200/71432 (27%)]\tLoss: 0.032511\n",
      "Train Epoch: 7 [25600/71432 (36%)]\tLoss: 0.056967\n",
      "Train Epoch: 7 [32000/71432 (45%)]\tLoss: 0.111108\n",
      "Train Epoch: 7 [38400/71432 (54%)]\tLoss: 0.043609\n",
      "Train Epoch: 7 [44800/71432 (63%)]\tLoss: 0.053077\n",
      "Train Epoch: 7 [51200/71432 (72%)]\tLoss: 0.072737\n",
      "Train Epoch: 7 [57600/71432 (81%)]\tLoss: 0.033031\n",
      "Train Epoch: 7 [64000/71432 (90%)]\tLoss: 0.024068\n",
      "Train Epoch: 7 [70400/71432 (98%)]\tLoss: 0.081536\n",
      "\n",
      "Test set: Average loss: 0.0716, Accuracy: 7934/8141 (97%), Positive accuracy: 3955/4091 (97%), Negative accuracy: 3979/4050 (98%), f1 score: 0.9746\n",
      "\n",
      "Train Epoch: 8 [0/71432 (0%)]\tLoss: 0.112383\n",
      "Train Epoch: 8 [6400/71432 (9%)]\tLoss: 0.040658\n",
      "Train Epoch: 8 [12800/71432 (18%)]\tLoss: 0.149486\n",
      "Train Epoch: 8 [19200/71432 (27%)]\tLoss: 0.047796\n",
      "Train Epoch: 8 [25600/71432 (36%)]\tLoss: 0.054382\n",
      "Train Epoch: 8 [32000/71432 (45%)]\tLoss: 0.124227\n",
      "Train Epoch: 8 [38400/71432 (54%)]\tLoss: 0.061127\n",
      "Train Epoch: 8 [44800/71432 (63%)]\tLoss: 0.054964\n",
      "Train Epoch: 8 [51200/71432 (72%)]\tLoss: 0.034498\n",
      "Train Epoch: 8 [57600/71432 (81%)]\tLoss: 0.053384\n",
      "Train Epoch: 8 [64000/71432 (90%)]\tLoss: 0.089780\n",
      "Train Epoch: 8 [70400/71432 (98%)]\tLoss: 0.047760\n",
      "\n",
      "Test set: Average loss: 0.0791, Accuracy: 7897/8141 (97%), Positive accuracy: 3891/4091 (95%), Negative accuracy: 4006/4050 (99%), f1 score: 0.9703\n",
      "\n",
      "Train Epoch: 9 [0/71432 (0%)]\tLoss: 0.299215\n",
      "Train Epoch: 9 [6400/71432 (9%)]\tLoss: 0.083593\n",
      "Train Epoch: 9 [12800/71432 (18%)]\tLoss: 0.016619\n",
      "Train Epoch: 9 [19200/71432 (27%)]\tLoss: 0.061787\n",
      "Train Epoch: 9 [25600/71432 (36%)]\tLoss: 0.122979\n",
      "Train Epoch: 9 [32000/71432 (45%)]\tLoss: 0.041699\n",
      "Train Epoch: 9 [38400/71432 (54%)]\tLoss: 0.122821\n",
      "Train Epoch: 9 [44800/71432 (63%)]\tLoss: 0.057222\n",
      "Train Epoch: 9 [51200/71432 (72%)]\tLoss: 0.093753\n",
      "Train Epoch: 9 [57600/71432 (81%)]\tLoss: 0.133074\n",
      "Train Epoch: 9 [64000/71432 (90%)]\tLoss: 0.007489\n",
      "Train Epoch: 9 [70400/71432 (98%)]\tLoss: 0.113581\n",
      "\n",
      "Test set: Average loss: 0.0736, Accuracy: 7900/8141 (97%), Positive accuracy: 3916/4091 (96%), Negative accuracy: 3984/4050 (98%), f1 score: 0.9705\n",
      "\n",
      "Train Epoch: 10 [0/71432 (0%)]\tLoss: 0.102106\n",
      "Train Epoch: 10 [6400/71432 (9%)]\tLoss: 0.090112\n",
      "Train Epoch: 10 [12800/71432 (18%)]\tLoss: 0.088391\n",
      "Train Epoch: 10 [19200/71432 (27%)]\tLoss: 0.090890\n",
      "Train Epoch: 10 [25600/71432 (36%)]\tLoss: 0.062337\n",
      "Train Epoch: 10 [32000/71432 (45%)]\tLoss: 0.077889\n",
      "Train Epoch: 10 [38400/71432 (54%)]\tLoss: 0.046779\n",
      "Train Epoch: 10 [44800/71432 (63%)]\tLoss: 0.059631\n",
      "Train Epoch: 10 [51200/71432 (72%)]\tLoss: 0.121322\n",
      "Train Epoch: 10 [57600/71432 (81%)]\tLoss: 0.041439\n",
      "Train Epoch: 10 [64000/71432 (90%)]\tLoss: 0.134543\n",
      "Train Epoch: 10 [70400/71432 (98%)]\tLoss: 0.068008\n",
      "\n",
      "Test set: Average loss: 0.0710, Accuracy: 7914/8141 (97%), Positive accuracy: 3929/4091 (96%), Negative accuracy: 3985/4050 (98%), f1 score: 0.9722\n",
      "\n",
      "Train Epoch: 11 [0/71432 (0%)]\tLoss: 0.044836\n",
      "Train Epoch: 11 [6400/71432 (9%)]\tLoss: 0.055144\n",
      "Train Epoch: 11 [12800/71432 (18%)]\tLoss: 0.076631\n",
      "Train Epoch: 11 [19200/71432 (27%)]\tLoss: 0.072470\n",
      "Train Epoch: 11 [25600/71432 (36%)]\tLoss: 0.039337\n",
      "Train Epoch: 11 [32000/71432 (45%)]\tLoss: 0.089661\n",
      "Train Epoch: 11 [38400/71432 (54%)]\tLoss: 0.020604\n",
      "Train Epoch: 11 [44800/71432 (63%)]\tLoss: 0.053046\n",
      "Train Epoch: 11 [51200/71432 (72%)]\tLoss: 0.055056\n",
      "Train Epoch: 11 [57600/71432 (81%)]\tLoss: 0.010499\n",
      "Train Epoch: 11 [64000/71432 (90%)]\tLoss: 0.029468\n",
      "Train Epoch: 11 [70400/71432 (98%)]\tLoss: 0.057460\n",
      "\n",
      "Test set: Average loss: 0.0742, Accuracy: 7902/8141 (97%), Positive accuracy: 3908/4091 (96%), Negative accuracy: 3994/4050 (99%), f1 score: 0.9708\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [0/71432 (0%)]\tLoss: 0.043702\n",
      "Train Epoch: 12 [6400/71432 (9%)]\tLoss: 0.042909\n",
      "Train Epoch: 12 [12800/71432 (18%)]\tLoss: 0.031084\n",
      "Train Epoch: 12 [19200/71432 (27%)]\tLoss: 0.089226\n",
      "Train Epoch: 12 [25600/71432 (36%)]\tLoss: 0.027980\n",
      "Train Epoch: 12 [32000/71432 (45%)]\tLoss: 0.109528\n",
      "Train Epoch: 12 [38400/71432 (54%)]\tLoss: 0.065105\n",
      "Train Epoch: 12 [44800/71432 (63%)]\tLoss: 0.025416\n",
      "Train Epoch: 12 [51200/71432 (72%)]\tLoss: 0.015892\n",
      "Train Epoch: 12 [57600/71432 (81%)]\tLoss: 0.056030\n",
      "Train Epoch: 12 [64000/71432 (90%)]\tLoss: 0.059143\n",
      "Train Epoch: 12 [70400/71432 (98%)]\tLoss: 0.012051\n",
      "\n",
      "Test set: Average loss: 0.0741, Accuracy: 7895/8141 (97%), Positive accuracy: 3915/4091 (96%), Negative accuracy: 3980/4050 (98%), f1 score: 0.9699\n",
      "\n",
      "Train Epoch: 13 [0/71432 (0%)]\tLoss: 0.039441\n",
      "Train Epoch: 13 [6400/71432 (9%)]\tLoss: 0.015968\n",
      "Train Epoch: 13 [12800/71432 (18%)]\tLoss: 0.090587\n",
      "Train Epoch: 13 [19200/71432 (27%)]\tLoss: 0.025017\n",
      "Train Epoch: 13 [25600/71432 (36%)]\tLoss: 0.101145\n",
      "Train Epoch: 13 [32000/71432 (45%)]\tLoss: 0.053210\n",
      "Train Epoch: 13 [38400/71432 (54%)]\tLoss: 0.057784\n",
      "Train Epoch: 13 [44800/71432 (63%)]\tLoss: 0.029721\n",
      "Train Epoch: 13 [51200/71432 (72%)]\tLoss: 0.021751\n",
      "Train Epoch: 13 [57600/71432 (81%)]\tLoss: 0.082223\n",
      "Train Epoch: 13 [64000/71432 (90%)]\tLoss: 0.064132\n",
      "Train Epoch: 13 [70400/71432 (98%)]\tLoss: 0.020512\n",
      "\n",
      "Test set: Average loss: 0.0705, Accuracy: 7909/8141 (97%), Positive accuracy: 3938/4091 (96%), Negative accuracy: 3971/4050 (98%), f1 score: 0.9715\n",
      "\n",
      "Train Epoch: 14 [0/71432 (0%)]\tLoss: 0.074300\n",
      "Train Epoch: 14 [6400/71432 (9%)]\tLoss: 0.041061\n",
      "Train Epoch: 14 [12800/71432 (18%)]\tLoss: 0.047584\n",
      "Train Epoch: 14 [19200/71432 (27%)]\tLoss: 0.087133\n",
      "Train Epoch: 14 [25600/71432 (36%)]\tLoss: 0.030818\n",
      "Train Epoch: 14 [32000/71432 (45%)]\tLoss: 0.059296\n",
      "Train Epoch: 14 [38400/71432 (54%)]\tLoss: 0.015819\n",
      "Train Epoch: 14 [44800/71432 (63%)]\tLoss: 0.033001\n",
      "Train Epoch: 14 [51200/71432 (72%)]\tLoss: 0.055709\n",
      "Train Epoch: 14 [57600/71432 (81%)]\tLoss: 0.092880\n",
      "Train Epoch: 14 [64000/71432 (90%)]\tLoss: 0.098463\n",
      "Train Epoch: 14 [70400/71432 (98%)]\tLoss: 0.012113\n",
      "\n",
      "Test set: Average loss: 0.0716, Accuracy: 7910/8141 (97%), Positive accuracy: 3929/4091 (96%), Negative accuracy: 3981/4050 (98%), f1 score: 0.9717\n",
      "\n",
      "Train Epoch: 15 [0/71432 (0%)]\tLoss: 0.028125\n",
      "Train Epoch: 15 [6400/71432 (9%)]\tLoss: 0.029430\n",
      "Train Epoch: 15 [12800/71432 (18%)]\tLoss: 0.083778\n",
      "Train Epoch: 15 [19200/71432 (27%)]\tLoss: 0.020091\n",
      "Train Epoch: 15 [25600/71432 (36%)]\tLoss: 0.119063\n",
      "Train Epoch: 15 [32000/71432 (45%)]\tLoss: 0.110622\n",
      "Train Epoch: 15 [38400/71432 (54%)]\tLoss: 0.034755\n",
      "Train Epoch: 15 [44800/71432 (63%)]\tLoss: 0.080161\n",
      "Train Epoch: 15 [51200/71432 (72%)]\tLoss: 0.076217\n",
      "Train Epoch: 15 [57600/71432 (81%)]\tLoss: 0.018541\n",
      "Train Epoch: 15 [64000/71432 (90%)]\tLoss: 0.004625\n",
      "Train Epoch: 15 [70400/71432 (98%)]\tLoss: 0.089725\n",
      "\n",
      "Test set: Average loss: 0.0740, Accuracy: 7901/8141 (97%), Positive accuracy: 3917/4091 (96%), Negative accuracy: 3984/4050 (98%), f1 score: 0.9706\n",
      "\n",
      "Train Epoch: 16 [0/71432 (0%)]\tLoss: 0.064958\n",
      "Train Epoch: 16 [6400/71432 (9%)]\tLoss: 0.042970\n",
      "Train Epoch: 16 [12800/71432 (18%)]\tLoss: 0.107259\n",
      "Train Epoch: 16 [19200/71432 (27%)]\tLoss: 0.031892\n",
      "Train Epoch: 16 [25600/71432 (36%)]\tLoss: 0.097569\n",
      "Train Epoch: 16 [32000/71432 (45%)]\tLoss: 0.085076\n",
      "Train Epoch: 16 [38400/71432 (54%)]\tLoss: 0.089944\n",
      "Train Epoch: 16 [44800/71432 (63%)]\tLoss: 0.137864\n",
      "Train Epoch: 16 [51200/71432 (72%)]\tLoss: 0.054100\n",
      "Train Epoch: 16 [57600/71432 (81%)]\tLoss: 0.061047\n",
      "Train Epoch: 16 [64000/71432 (90%)]\tLoss: 0.051743\n",
      "Train Epoch: 16 [70400/71432 (98%)]\tLoss: 0.023355\n",
      "\n",
      "Test set: Average loss: 0.0713, Accuracy: 7910/8141 (97%), Positive accuracy: 3929/4091 (96%), Negative accuracy: 3981/4050 (98%), f1 score: 0.9717\n",
      "\n",
      "Train Epoch: 17 [0/71432 (0%)]\tLoss: 0.141621\n",
      "Train Epoch: 17 [6400/71432 (9%)]\tLoss: 0.035880\n",
      "Train Epoch: 17 [12800/71432 (18%)]\tLoss: 0.024583\n",
      "Train Epoch: 17 [19200/71432 (27%)]\tLoss: 0.044781\n",
      "Train Epoch: 17 [25600/71432 (36%)]\tLoss: 0.037996\n",
      "Train Epoch: 17 [32000/71432 (45%)]\tLoss: 0.057356\n",
      "Train Epoch: 17 [38400/71432 (54%)]\tLoss: 0.087630\n",
      "Train Epoch: 17 [44800/71432 (63%)]\tLoss: 0.037217\n",
      "Train Epoch: 17 [51200/71432 (72%)]\tLoss: 0.103529\n",
      "Train Epoch: 17 [57600/71432 (81%)]\tLoss: 0.109357\n",
      "Train Epoch: 17 [64000/71432 (90%)]\tLoss: 0.026614\n",
      "Train Epoch: 17 [70400/71432 (98%)]\tLoss: 0.063408\n",
      "\n",
      "Test set: Average loss: 0.0723, Accuracy: 7908/8141 (97%), Positive accuracy: 3925/4091 (96%), Negative accuracy: 3983/4050 (98%), f1 score: 0.9715\n",
      "\n",
      "Train Epoch: 18 [0/71432 (0%)]\tLoss: 0.096990\n",
      "Train Epoch: 18 [6400/71432 (9%)]\tLoss: 0.134588\n",
      "Train Epoch: 18 [12800/71432 (18%)]\tLoss: 0.061432\n",
      "Train Epoch: 18 [19200/71432 (27%)]\tLoss: 0.096025\n",
      "Train Epoch: 18 [25600/71432 (36%)]\tLoss: 0.050187\n",
      "Train Epoch: 18 [32000/71432 (45%)]\tLoss: 0.061282\n",
      "Train Epoch: 18 [38400/71432 (54%)]\tLoss: 0.093810\n",
      "Train Epoch: 18 [44800/71432 (63%)]\tLoss: 0.126847\n",
      "Train Epoch: 18 [51200/71432 (72%)]\tLoss: 0.124114\n",
      "Train Epoch: 18 [57600/71432 (81%)]\tLoss: 0.013790\n",
      "Train Epoch: 18 [64000/71432 (90%)]\tLoss: 0.055092\n",
      "Train Epoch: 18 [70400/71432 (98%)]\tLoss: 0.025132\n",
      "\n",
      "Test set: Average loss: 0.0721, Accuracy: 7910/8141 (97%), Positive accuracy: 3927/4091 (96%), Negative accuracy: 3983/4050 (98%), f1 score: 0.9717\n",
      "\n",
      "Train Epoch: 19 [0/71432 (0%)]\tLoss: 0.111604\n",
      "Train Epoch: 19 [6400/71432 (9%)]\tLoss: 0.013760\n",
      "Train Epoch: 19 [12800/71432 (18%)]\tLoss: 0.046830\n",
      "Train Epoch: 19 [19200/71432 (27%)]\tLoss: 0.023451\n",
      "Train Epoch: 19 [25600/71432 (36%)]\tLoss: 0.020953\n",
      "Train Epoch: 19 [32000/71432 (45%)]\tLoss: 0.027673\n",
      "Train Epoch: 19 [38400/71432 (54%)]\tLoss: 0.030864\n",
      "Train Epoch: 19 [44800/71432 (63%)]\tLoss: 0.050189\n",
      "Train Epoch: 19 [51200/71432 (72%)]\tLoss: 0.094270\n",
      "Train Epoch: 19 [57600/71432 (81%)]\tLoss: 0.060088\n",
      "Train Epoch: 19 [64000/71432 (90%)]\tLoss: 0.023467\n",
      "Train Epoch: 19 [70400/71432 (98%)]\tLoss: 0.026175\n",
      "\n",
      "Test set: Average loss: 0.0719, Accuracy: 7909/8141 (97%), Positive accuracy: 3927/4091 (96%), Negative accuracy: 3982/4050 (98%), f1 score: 0.9716\n",
      "\n",
      "Train Epoch: 20 [0/71432 (0%)]\tLoss: 0.112018\n",
      "Train Epoch: 20 [6400/71432 (9%)]\tLoss: 0.041551\n",
      "Train Epoch: 20 [12800/71432 (18%)]\tLoss: 0.050518\n",
      "Train Epoch: 20 [19200/71432 (27%)]\tLoss: 0.038313\n",
      "Train Epoch: 20 [25600/71432 (36%)]\tLoss: 0.022157\n",
      "Train Epoch: 20 [32000/71432 (45%)]\tLoss: 0.141539\n",
      "Train Epoch: 20 [38400/71432 (54%)]\tLoss: 0.063987\n",
      "Train Epoch: 20 [44800/71432 (63%)]\tLoss: 0.015722\n",
      "Train Epoch: 20 [51200/71432 (72%)]\tLoss: 0.114543\n",
      "Train Epoch: 20 [57600/71432 (81%)]\tLoss: 0.082058\n",
      "Train Epoch: 20 [64000/71432 (90%)]\tLoss: 0.151478\n",
      "Train Epoch: 20 [70400/71432 (98%)]\tLoss: 0.138361\n",
      "\n",
      "Test set: Average loss: 0.0719, Accuracy: 7910/8141 (97%), Positive accuracy: 3928/4091 (96%), Negative accuracy: 3982/4050 (98%), f1 score: 0.9717\n",
      "\n",
      "[7542, 7698, 7836, 7804, 7859, 7892, 7934, 7897, 7900, 7914, 7902, 7895, 7909, 7910, 7901, 7910, 7908, 7910, 7909, 7910]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class CNNTrainedGabor(nn.Module):\n",
    "    def __init__(self, pretrain_gabor_model):\n",
    "        super(CNNTrainedGabor, self).__init__()\n",
    "        self.pretrain_gabor_model = pretrain_gabor_model\n",
    "        self.conv1 = nn.Conv2d(1, 4, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 5, 1)\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "#         self.dropout2 = nn.Dropout2d(0.5)\n",
    "        # self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc1 = nn.Linear(200, 128)\n",
    "#         self.fc2 = nn.Linear(128, 2)\n",
    "        self.fc2 = nn.Linear(128, 35)\n",
    "        self.fc3 = nn.Linear(36, 2)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        pc = self.pc(x).view(x.shape[0],1)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "#         x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "#         x = self.dropout2(x)\n",
    "#         print(pc.shape, x.shape)\n",
    "        x = torch.cat((pc, x), 1)\n",
    "        x = self.fc3(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    def pc(self, x):\n",
    "        filter_cos = self.pretrain_gabor_model['filter_cos']\n",
    "        filter_sin = self.pretrain_gabor_model['filter_sin']\n",
    "        bias1 = self.pretrain_gabor_model['bias1']\n",
    "        bias2 = self.pretrain_gabor_model['bias2']\n",
    "        weights = self.pretrain_gabor_model['weights']\n",
    "        w = self.pretrain_gabor_model['w']\n",
    "        b = self.pretrain_gabor_model['b']\n",
    "        \n",
    "        x_cos = F.conv2d(x, filter_cos, bias=bias1)\n",
    "        x_sin = F.conv2d(x, filter_sin, bias=bias2)\n",
    "        x_comb = torch.cat((x_cos, x_sin), 2)\n",
    "\n",
    "        x_cos = x_cos.view(len(x), 1, 1, 48)\n",
    "        x_sin = x_sin.view(len(x), 1, 1, 48)\n",
    "        weighted_cos = (torch.matmul(x_cos, weights)).view(len(x), 1)\n",
    "        weighted_sin = (torch.matmul(x_sin, weights)).view(len(x), 1)\n",
    "\n",
    "        numerator = torch.norm(torch.cat([weighted_cos, weighted_sin], 1), dim=1)\n",
    "    #         print(\"numerator\", numerator.size())\n",
    "        x_comb_norm = torch.norm(x_comb, dim=2)\n",
    "        x_comb_norm = x_comb_norm.view(len(x), 1, 48)\n",
    "    #         print(\"x_comb_norm\", x_comb_norm.size())\n",
    "        denominator = torch.matmul(x_comb_norm, torch.abs(weights))\n",
    "        denominator = denominator.view(len(x))\n",
    "    #         print(\"size:\", numerator.size(), denominator.size())\n",
    "        pc = numerator / denominator                \n",
    "        return torch.sigmoid(w * pc + b)\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "    train_loss = sum(loss_list)/len(loss_list)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "# def test(model, device, test_loader):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             output = model(data)\n",
    "#             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "#             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "#             correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "\n",
    "#     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         test_loss, correct, len(test_loader.dataset),\n",
    "#         100. * correct / len(test_loader.dataset)))\n",
    "def test(model, device, test_loader,epoch,train_loss):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    result= [[0,0], [0,0]] \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            cmat = confusion_matrix(target.view_as(pred), pred, labels=[0, 1]) \n",
    "            result = [[result[i][j] + cmat[i][j]  for j in range(len(result[0]))] for i in range(len(result))] \n",
    "            \n",
    "            # Store wrongly predicted images\n",
    "            if epoch == 7:\n",
    "                wrong_idx = (pred != target.view_as(pred)).nonzero()[:, 0]\n",
    "                wrong_samples = data[wrong_idx]\n",
    "                wrong_preds = pred[wrong_idx]\n",
    "                actual_preds = target.view_as(pred)[wrong_idx]\n",
    "                for i in range(len(wrong_idx)):\n",
    "                    sample = wrong_samples[i]\n",
    "                    wrong_pred = wrong_preds[i]\n",
    "                    actual_pred = actual_preds[i]\n",
    "                    sample = sample * 255.\n",
    "                    sample = sample.byte()\n",
    "                    img = TF.to_pil_image(sample)\n",
    "                    img.save('./wrong-mix/epoch{}_batch{}_idx{}_actual{}.png'.format(\n",
    "                        epoch,batch_idx,wrong_idx[i], actual_pred.item()))\n",
    "                    num = batch_idx * 64 + wrong_idx[i]\n",
    "                    img_ori = origin_dataset[num][0].numpy()\n",
    "                    plt.imsave('./wrong-mix/epoch{}_batch{}_idx{}_actual{}_ori.png'.format(\n",
    "                    epoch,batch_idx,wrong_idx[i], actual_pred.item()), img_ori[0], cmap = 'gray')\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    precision = result[1][1]/(result[1][1]+result[0][1])\n",
    "    recall = result[0][0]/(result[0][0]+result[1][0])\n",
    "    f1score = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Positive accuracy: {}/{} ({:.0f}%), Negative accuracy: {}/{} ({:.0f}%), f1 score: {:.4f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset), \n",
    "        result[1][1],result[1][1]+result[1][0],100. * result[1][1]/(result[1][1]+result[1][0]),\n",
    "        result[0][0],result[0][0]+result[0][1],100. * result[0][0]/(result[0][0]+result[0][1]),f1score))\n",
    "    return test_loss, correct\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=100, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'batch_size': args.batch_size}\n",
    "    if use_cuda:\n",
    "        kwargs.update({'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True},\n",
    "                     )\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    model = CNNTrainedGabor(pretrain_gabor_model).to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_accuracy_list = []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_result = test(model, device, test_loader,epoch,train_loss)\n",
    "        test_loss_list.append(test_result[0])\n",
    "        test_accuracy_list.append(test_result[1])\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "    plt.plot(train_loss_list, color='blue',label='train loss')  \n",
    "    plt.plot(test_loss_list, color='red',label='test loss')  \n",
    "    plt.legend()\n",
    "    print(test_accuracy_list)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9808aeb850>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [7542, 7698, 7836, 7804, 7859, 7892, 7934, 7897, 7900, 7914, 7902, 7895, 7909, 7910, 7901, 7910, 7908, 7910, 7909, 7910]\n",
    "plt.plot(x, color='blue',label='Accuracy')  \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/71432 (0%)]\tLoss: 0.802551\n",
      "Train Epoch: 1 [6400/71432 (9%)]\tLoss: 0.287228\n",
      "Train Epoch: 1 [12800/71432 (18%)]\tLoss: 0.173089\n",
      "Train Epoch: 1 [19200/71432 (27%)]\tLoss: 0.106474\n",
      "Train Epoch: 1 [25600/71432 (36%)]\tLoss: 0.068765\n",
      "Train Epoch: 1 [32000/71432 (45%)]\tLoss: 0.091925\n",
      "Train Epoch: 1 [38400/71432 (54%)]\tLoss: 0.066787\n",
      "Train Epoch: 1 [44800/71432 (63%)]\tLoss: 0.064277\n",
      "Train Epoch: 1 [51200/71432 (72%)]\tLoss: 0.187315\n",
      "Train Epoch: 1 [57600/71432 (81%)]\tLoss: 0.234266\n",
      "Train Epoch: 1 [64000/71432 (90%)]\tLoss: 0.128217\n",
      "Train Epoch: 1 [70400/71432 (98%)]\tLoss: 0.190527\n",
      "\n",
      "Test set: Average loss: 0.1296, Accuracy: 7754/8141 (95%), Positive accuracy: 3943/4091 (96%), Negative accuracy: 3811/4050 (94%), f1 score: 0.9526\n",
      "\n",
      "Train Epoch: 2 [0/71432 (0%)]\tLoss: 0.200559\n",
      "Train Epoch: 2 [6400/71432 (9%)]\tLoss: 0.170902\n",
      "Train Epoch: 2 [12800/71432 (18%)]\tLoss: 0.166049\n",
      "Train Epoch: 2 [19200/71432 (27%)]\tLoss: 0.239587\n",
      "Train Epoch: 2 [25600/71432 (36%)]\tLoss: 0.067142\n",
      "Train Epoch: 2 [32000/71432 (45%)]\tLoss: 0.135882\n",
      "Train Epoch: 2 [38400/71432 (54%)]\tLoss: 0.100906\n",
      "Train Epoch: 2 [44800/71432 (63%)]\tLoss: 0.153928\n",
      "Train Epoch: 2 [51200/71432 (72%)]\tLoss: 0.158849\n",
      "Train Epoch: 2 [57600/71432 (81%)]\tLoss: 0.064422\n",
      "Train Epoch: 2 [64000/71432 (90%)]\tLoss: 0.121392\n",
      "Train Epoch: 2 [70400/71432 (98%)]\tLoss: 0.088200\n",
      "\n",
      "Test set: Average loss: 0.1259, Accuracy: 7751/8141 (95%), Positive accuracy: 3798/4091 (93%), Negative accuracy: 3953/4050 (98%), f1 score: 0.9525\n",
      "\n",
      "Train Epoch: 3 [0/71432 (0%)]\tLoss: 0.243761\n",
      "Train Epoch: 3 [6400/71432 (9%)]\tLoss: 0.141860\n",
      "Train Epoch: 3 [12800/71432 (18%)]\tLoss: 0.035864\n",
      "Train Epoch: 3 [19200/71432 (27%)]\tLoss: 0.083060\n",
      "Train Epoch: 3 [25600/71432 (36%)]\tLoss: 0.162307\n",
      "Train Epoch: 3 [32000/71432 (45%)]\tLoss: 0.113629\n",
      "Train Epoch: 3 [38400/71432 (54%)]\tLoss: 0.120268\n",
      "Train Epoch: 3 [44800/71432 (63%)]\tLoss: 0.262298\n",
      "Train Epoch: 3 [51200/71432 (72%)]\tLoss: 0.222512\n",
      "Train Epoch: 3 [57600/71432 (81%)]\tLoss: 0.086218\n",
      "Train Epoch: 3 [64000/71432 (90%)]\tLoss: 0.142938\n",
      "Train Epoch: 3 [70400/71432 (98%)]\tLoss: 0.114159\n",
      "\n",
      "Test set: Average loss: 0.1210, Accuracy: 7746/8141 (95%), Positive accuracy: 3815/4091 (93%), Negative accuracy: 3931/4050 (97%), f1 score: 0.9517\n",
      "\n",
      "Train Epoch: 4 [0/71432 (0%)]\tLoss: 0.095777\n",
      "Train Epoch: 4 [6400/71432 (9%)]\tLoss: 0.105944\n",
      "Train Epoch: 4 [12800/71432 (18%)]\tLoss: 0.107445\n",
      "Train Epoch: 4 [19200/71432 (27%)]\tLoss: 0.061131\n",
      "Train Epoch: 4 [25600/71432 (36%)]\tLoss: 0.151174\n",
      "Train Epoch: 4 [32000/71432 (45%)]\tLoss: 0.086934\n",
      "Train Epoch: 4 [38400/71432 (54%)]\tLoss: 0.051173\n",
      "Train Epoch: 4 [44800/71432 (63%)]\tLoss: 0.031261\n",
      "Train Epoch: 4 [51200/71432 (72%)]\tLoss: 0.045964\n",
      "Train Epoch: 4 [57600/71432 (81%)]\tLoss: 0.032168\n",
      "Train Epoch: 4 [64000/71432 (90%)]\tLoss: 0.072352\n",
      "Train Epoch: 4 [70400/71432 (98%)]\tLoss: 0.049954\n",
      "\n",
      "Test set: Average loss: 0.1059, Accuracy: 7783/8141 (96%), Positive accuracy: 3819/4091 (93%), Negative accuracy: 3964/4050 (98%), f1 score: 0.9564\n",
      "\n",
      "Train Epoch: 5 [0/71432 (0%)]\tLoss: 0.106842\n",
      "Train Epoch: 5 [6400/71432 (9%)]\tLoss: 0.107894\n",
      "Train Epoch: 5 [12800/71432 (18%)]\tLoss: 0.110062\n",
      "Train Epoch: 5 [19200/71432 (27%)]\tLoss: 0.114342\n",
      "Train Epoch: 5 [25600/71432 (36%)]\tLoss: 0.081175\n",
      "Train Epoch: 5 [32000/71432 (45%)]\tLoss: 0.113646\n",
      "Train Epoch: 5 [38400/71432 (54%)]\tLoss: 0.130203\n",
      "Train Epoch: 5 [44800/71432 (63%)]\tLoss: 0.078855\n",
      "Train Epoch: 5 [51200/71432 (72%)]\tLoss: 0.134364\n",
      "Train Epoch: 5 [57600/71432 (81%)]\tLoss: 0.067459\n",
      "Train Epoch: 5 [64000/71432 (90%)]\tLoss: 0.123056\n",
      "Train Epoch: 5 [70400/71432 (98%)]\tLoss: 0.198903\n",
      "\n",
      "Test set: Average loss: 0.1234, Accuracy: 7761/8141 (95%), Positive accuracy: 3776/4091 (92%), Negative accuracy: 3985/4050 (98%), f1 score: 0.9541\n",
      "\n",
      "Train Epoch: 6 [0/71432 (0%)]\tLoss: 0.185560\n",
      "Train Epoch: 6 [6400/71432 (9%)]\tLoss: 0.024536\n",
      "Train Epoch: 6 [12800/71432 (18%)]\tLoss: 0.178930\n",
      "Train Epoch: 6 [19200/71432 (27%)]\tLoss: 0.085280\n",
      "Train Epoch: 6 [25600/71432 (36%)]\tLoss: 0.129135\n",
      "Train Epoch: 6 [32000/71432 (45%)]\tLoss: 0.128811\n",
      "Train Epoch: 6 [38400/71432 (54%)]\tLoss: 0.091141\n",
      "Train Epoch: 6 [44800/71432 (63%)]\tLoss: 0.093197\n",
      "Train Epoch: 6 [51200/71432 (72%)]\tLoss: 0.041369\n",
      "Train Epoch: 6 [57600/71432 (81%)]\tLoss: 0.057863\n",
      "Train Epoch: 6 [64000/71432 (90%)]\tLoss: 0.074239\n",
      "Train Epoch: 6 [70400/71432 (98%)]\tLoss: 0.130830\n",
      "\n",
      "Test set: Average loss: 0.0995, Accuracy: 7818/8141 (96%), Positive accuracy: 3887/4091 (95%), Negative accuracy: 3931/4050 (97%), f1 score: 0.9604\n",
      "\n",
      "Train Epoch: 7 [0/71432 (0%)]\tLoss: 0.177869\n",
      "Train Epoch: 7 [6400/71432 (9%)]\tLoss: 0.070452\n",
      "Train Epoch: 7 [12800/71432 (18%)]\tLoss: 0.110678\n",
      "Train Epoch: 7 [19200/71432 (27%)]\tLoss: 0.191827\n",
      "Train Epoch: 7 [25600/71432 (36%)]\tLoss: 0.023544\n",
      "Train Epoch: 7 [32000/71432 (45%)]\tLoss: 0.063999\n",
      "Train Epoch: 7 [38400/71432 (54%)]\tLoss: 0.142504\n",
      "Train Epoch: 7 [44800/71432 (63%)]\tLoss: 0.071378\n",
      "Train Epoch: 7 [51200/71432 (72%)]\tLoss: 0.103505\n",
      "Train Epoch: 7 [57600/71432 (81%)]\tLoss: 0.026476\n",
      "Train Epoch: 7 [64000/71432 (90%)]\tLoss: 0.069539\n",
      "Train Epoch: 7 [70400/71432 (98%)]\tLoss: 0.128947\n",
      "\n",
      "Test set: Average loss: 0.1003, Accuracy: 7802/8141 (96%), Positive accuracy: 3831/4091 (94%), Negative accuracy: 3971/4050 (98%), f1 score: 0.9587\n",
      "\n",
      "Train Epoch: 8 [0/71432 (0%)]\tLoss: 0.084030\n",
      "Train Epoch: 8 [6400/71432 (9%)]\tLoss: 0.122515\n",
      "Train Epoch: 8 [12800/71432 (18%)]\tLoss: 0.063807\n",
      "Train Epoch: 8 [19200/71432 (27%)]\tLoss: 0.107402\n",
      "Train Epoch: 8 [25600/71432 (36%)]\tLoss: 0.040990\n",
      "Train Epoch: 8 [32000/71432 (45%)]\tLoss: 0.098159\n",
      "Train Epoch: 8 [38400/71432 (54%)]\tLoss: 0.124211\n",
      "Train Epoch: 8 [44800/71432 (63%)]\tLoss: 0.060023\n",
      "Train Epoch: 8 [51200/71432 (72%)]\tLoss: 0.214237\n",
      "Train Epoch: 8 [57600/71432 (81%)]\tLoss: 0.146771\n",
      "Train Epoch: 8 [64000/71432 (90%)]\tLoss: 0.168905\n",
      "Train Epoch: 8 [70400/71432 (98%)]\tLoss: 0.156250\n",
      "\n",
      "Test set: Average loss: 0.0920, Accuracy: 7837/8141 (96%), Positive accuracy: 3885/4091 (95%), Negative accuracy: 3952/4050 (98%), f1 score: 0.9628\n",
      "\n",
      "Train Epoch: 9 [0/71432 (0%)]\tLoss: 0.111387\n",
      "Train Epoch: 9 [6400/71432 (9%)]\tLoss: 0.089929\n",
      "Train Epoch: 9 [12800/71432 (18%)]\tLoss: 0.071850\n",
      "Train Epoch: 9 [19200/71432 (27%)]\tLoss: 0.157667\n",
      "Train Epoch: 9 [25600/71432 (36%)]\tLoss: 0.079394\n",
      "Train Epoch: 9 [32000/71432 (45%)]\tLoss: 0.159371\n",
      "Train Epoch: 9 [38400/71432 (54%)]\tLoss: 0.060006\n",
      "Train Epoch: 9 [44800/71432 (63%)]\tLoss: 0.065271\n",
      "Train Epoch: 9 [51200/71432 (72%)]\tLoss: 0.089436\n",
      "Train Epoch: 9 [57600/71432 (81%)]\tLoss: 0.110605\n",
      "Train Epoch: 9 [64000/71432 (90%)]\tLoss: 0.111194\n",
      "Train Epoch: 9 [70400/71432 (98%)]\tLoss: 0.104541\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 7831/8141 (96%), Positive accuracy: 3876/4091 (95%), Negative accuracy: 3955/4050 (98%), f1 score: 0.9621\n",
      "\n",
      "Train Epoch: 10 [0/71432 (0%)]\tLoss: 0.058014\n",
      "Train Epoch: 10 [6400/71432 (9%)]\tLoss: 0.142916\n",
      "Train Epoch: 10 [12800/71432 (18%)]\tLoss: 0.035802\n",
      "Train Epoch: 10 [19200/71432 (27%)]\tLoss: 0.071383\n",
      "Train Epoch: 10 [25600/71432 (36%)]\tLoss: 0.131104\n",
      "Train Epoch: 10 [32000/71432 (45%)]\tLoss: 0.159029\n",
      "Train Epoch: 10 [38400/71432 (54%)]\tLoss: 0.118748\n",
      "Train Epoch: 10 [44800/71432 (63%)]\tLoss: 0.077351\n",
      "Train Epoch: 10 [51200/71432 (72%)]\tLoss: 0.120623\n",
      "Train Epoch: 10 [57600/71432 (81%)]\tLoss: 0.052192\n",
      "Train Epoch: 10 [64000/71432 (90%)]\tLoss: 0.165275\n",
      "Train Epoch: 10 [70400/71432 (98%)]\tLoss: 0.110234\n",
      "\n",
      "Test set: Average loss: 0.0905, Accuracy: 7852/8141 (96%), Positive accuracy: 3887/4091 (95%), Negative accuracy: 3965/4050 (98%), f1 score: 0.9646\n",
      "\n",
      "Train Epoch: 11 [0/71432 (0%)]\tLoss: 0.058302\n",
      "Train Epoch: 11 [6400/71432 (9%)]\tLoss: 0.064160\n",
      "Train Epoch: 11 [12800/71432 (18%)]\tLoss: 0.052328\n",
      "Train Epoch: 11 [19200/71432 (27%)]\tLoss: 0.064623\n",
      "Train Epoch: 11 [25600/71432 (36%)]\tLoss: 0.109187\n",
      "Train Epoch: 11 [32000/71432 (45%)]\tLoss: 0.046537\n",
      "Train Epoch: 11 [38400/71432 (54%)]\tLoss: 0.179028\n",
      "Train Epoch: 11 [44800/71432 (63%)]\tLoss: 0.081305\n",
      "Train Epoch: 11 [51200/71432 (72%)]\tLoss: 0.062688\n",
      "Train Epoch: 11 [57600/71432 (81%)]\tLoss: 0.116216\n",
      "Train Epoch: 11 [64000/71432 (90%)]\tLoss: 0.093215\n",
      "Train Epoch: 11 [70400/71432 (98%)]\tLoss: 0.104658\n",
      "\n",
      "Test set: Average loss: 0.0927, Accuracy: 7842/8141 (96%), Positive accuracy: 3876/4091 (95%), Negative accuracy: 3966/4050 (98%), f1 score: 0.9634\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [0/71432 (0%)]\tLoss: 0.132416\n",
      "Train Epoch: 12 [6400/71432 (9%)]\tLoss: 0.140588\n",
      "Train Epoch: 12 [12800/71432 (18%)]\tLoss: 0.131506\n",
      "Train Epoch: 12 [19200/71432 (27%)]\tLoss: 0.031217\n",
      "Train Epoch: 12 [25600/71432 (36%)]\tLoss: 0.026059\n",
      "Train Epoch: 12 [32000/71432 (45%)]\tLoss: 0.103146\n",
      "Train Epoch: 12 [38400/71432 (54%)]\tLoss: 0.079938\n",
      "Train Epoch: 12 [44800/71432 (63%)]\tLoss: 0.081722\n",
      "Train Epoch: 12 [51200/71432 (72%)]\tLoss: 0.101554\n",
      "Train Epoch: 12 [57600/71432 (81%)]\tLoss: 0.069748\n",
      "Train Epoch: 12 [64000/71432 (90%)]\tLoss: 0.108026\n",
      "Train Epoch: 12 [70400/71432 (98%)]\tLoss: 0.056605\n",
      "\n",
      "Test set: Average loss: 0.0908, Accuracy: 7847/8141 (96%), Positive accuracy: 3895/4091 (95%), Negative accuracy: 3952/4050 (98%), f1 score: 0.9640\n",
      "\n",
      "Train Epoch: 13 [0/71432 (0%)]\tLoss: 0.098588\n",
      "Train Epoch: 13 [6400/71432 (9%)]\tLoss: 0.042876\n",
      "Train Epoch: 13 [12800/71432 (18%)]\tLoss: 0.078693\n",
      "Train Epoch: 13 [19200/71432 (27%)]\tLoss: 0.129667\n",
      "Train Epoch: 13 [25600/71432 (36%)]\tLoss: 0.033524\n",
      "Train Epoch: 13 [32000/71432 (45%)]\tLoss: 0.052581\n",
      "Train Epoch: 13 [38400/71432 (54%)]\tLoss: 0.079073\n",
      "Train Epoch: 13 [44800/71432 (63%)]\tLoss: 0.059707\n",
      "Train Epoch: 13 [51200/71432 (72%)]\tLoss: 0.073669\n",
      "Train Epoch: 13 [57600/71432 (81%)]\tLoss: 0.138937\n",
      "Train Epoch: 13 [64000/71432 (90%)]\tLoss: 0.155052\n",
      "Train Epoch: 13 [70400/71432 (98%)]\tLoss: 0.156074\n",
      "\n",
      "Test set: Average loss: 0.0904, Accuracy: 7847/8141 (96%), Positive accuracy: 3899/4091 (95%), Negative accuracy: 3948/4050 (97%), f1 score: 0.9640\n",
      "\n",
      "Train Epoch: 14 [0/71432 (0%)]\tLoss: 0.037326\n",
      "Train Epoch: 14 [6400/71432 (9%)]\tLoss: 0.061544\n",
      "Train Epoch: 14 [12800/71432 (18%)]\tLoss: 0.078638\n",
      "Train Epoch: 14 [19200/71432 (27%)]\tLoss: 0.088786\n",
      "Train Epoch: 14 [25600/71432 (36%)]\tLoss: 0.230270\n",
      "Train Epoch: 14 [32000/71432 (45%)]\tLoss: 0.100151\n",
      "Train Epoch: 14 [38400/71432 (54%)]\tLoss: 0.058403\n",
      "Train Epoch: 14 [44800/71432 (63%)]\tLoss: 0.145426\n",
      "Train Epoch: 14 [51200/71432 (72%)]\tLoss: 0.083084\n",
      "Train Epoch: 14 [57600/71432 (81%)]\tLoss: 0.072233\n",
      "Train Epoch: 14 [64000/71432 (90%)]\tLoss: 0.051816\n",
      "Train Epoch: 14 [70400/71432 (98%)]\tLoss: 0.172421\n",
      "\n",
      "Test set: Average loss: 0.0916, Accuracy: 7849/8141 (96%), Positive accuracy: 3879/4091 (95%), Negative accuracy: 3970/4050 (98%), f1 score: 0.9643\n",
      "\n",
      "Train Epoch: 15 [0/71432 (0%)]\tLoss: 0.116993\n",
      "Train Epoch: 15 [6400/71432 (9%)]\tLoss: 0.064330\n",
      "Train Epoch: 15 [12800/71432 (18%)]\tLoss: 0.078250\n",
      "Train Epoch: 15 [19200/71432 (27%)]\tLoss: 0.034003\n",
      "Train Epoch: 15 [25600/71432 (36%)]\tLoss: 0.085582\n",
      "Train Epoch: 15 [32000/71432 (45%)]\tLoss: 0.092199\n",
      "Train Epoch: 15 [38400/71432 (54%)]\tLoss: 0.142687\n",
      "Train Epoch: 15 [44800/71432 (63%)]\tLoss: 0.079093\n",
      "Train Epoch: 15 [51200/71432 (72%)]\tLoss: 0.038795\n",
      "Train Epoch: 15 [57600/71432 (81%)]\tLoss: 0.080061\n",
      "Train Epoch: 15 [64000/71432 (90%)]\tLoss: 0.109244\n",
      "Train Epoch: 15 [70400/71432 (98%)]\tLoss: 0.080903\n",
      "\n",
      "Test set: Average loss: 0.0891, Accuracy: 7851/8141 (96%), Positive accuracy: 3896/4091 (95%), Negative accuracy: 3955/4050 (98%), f1 score: 0.9645\n",
      "\n",
      "Train Epoch: 16 [0/71432 (0%)]\tLoss: 0.038049\n",
      "Train Epoch: 16 [6400/71432 (9%)]\tLoss: 0.185651\n",
      "Train Epoch: 16 [12800/71432 (18%)]\tLoss: 0.168365\n",
      "Train Epoch: 16 [19200/71432 (27%)]\tLoss: 0.017885\n",
      "Train Epoch: 16 [25600/71432 (36%)]\tLoss: 0.064272\n",
      "Train Epoch: 16 [32000/71432 (45%)]\tLoss: 0.109204\n",
      "Train Epoch: 16 [38400/71432 (54%)]\tLoss: 0.054000\n",
      "Train Epoch: 16 [44800/71432 (63%)]\tLoss: 0.097288\n",
      "Train Epoch: 16 [51200/71432 (72%)]\tLoss: 0.148856\n",
      "Train Epoch: 16 [57600/71432 (81%)]\tLoss: 0.060974\n",
      "Train Epoch: 16 [64000/71432 (90%)]\tLoss: 0.080860\n",
      "Train Epoch: 16 [70400/71432 (98%)]\tLoss: 0.114983\n",
      "\n",
      "Test set: Average loss: 0.0897, Accuracy: 7853/8141 (96%), Positive accuracy: 3892/4091 (95%), Negative accuracy: 3961/4050 (98%), f1 score: 0.9647\n",
      "\n",
      "Train Epoch: 17 [0/71432 (0%)]\tLoss: 0.121141\n",
      "Train Epoch: 17 [6400/71432 (9%)]\tLoss: 0.111833\n",
      "Train Epoch: 17 [12800/71432 (18%)]\tLoss: 0.039325\n",
      "Train Epoch: 17 [19200/71432 (27%)]\tLoss: 0.112615\n",
      "Train Epoch: 17 [25600/71432 (36%)]\tLoss: 0.073478\n",
      "Train Epoch: 17 [32000/71432 (45%)]\tLoss: 0.045246\n",
      "Train Epoch: 17 [38400/71432 (54%)]\tLoss: 0.083878\n",
      "Train Epoch: 17 [44800/71432 (63%)]\tLoss: 0.055780\n",
      "Train Epoch: 17 [51200/71432 (72%)]\tLoss: 0.140069\n",
      "Train Epoch: 17 [57600/71432 (81%)]\tLoss: 0.047506\n",
      "Train Epoch: 17 [64000/71432 (90%)]\tLoss: 0.055534\n",
      "Train Epoch: 17 [70400/71432 (98%)]\tLoss: 0.108747\n",
      "\n",
      "Test set: Average loss: 0.0898, Accuracy: 7855/8141 (96%), Positive accuracy: 3892/4091 (95%), Negative accuracy: 3963/4050 (98%), f1 score: 0.9650\n",
      "\n",
      "Train Epoch: 18 [0/71432 (0%)]\tLoss: 0.099400\n",
      "Train Epoch: 18 [6400/71432 (9%)]\tLoss: 0.044856\n",
      "Train Epoch: 18 [12800/71432 (18%)]\tLoss: 0.025684\n",
      "Train Epoch: 18 [19200/71432 (27%)]\tLoss: 0.086024\n",
      "Train Epoch: 18 [25600/71432 (36%)]\tLoss: 0.191926\n",
      "Train Epoch: 18 [32000/71432 (45%)]\tLoss: 0.101709\n",
      "Train Epoch: 18 [38400/71432 (54%)]\tLoss: 0.116231\n",
      "Train Epoch: 18 [44800/71432 (63%)]\tLoss: 0.098816\n",
      "Train Epoch: 18 [51200/71432 (72%)]\tLoss: 0.053091\n",
      "Train Epoch: 18 [57600/71432 (81%)]\tLoss: 0.139007\n",
      "Train Epoch: 18 [64000/71432 (90%)]\tLoss: 0.104037\n",
      "Train Epoch: 18 [70400/71432 (98%)]\tLoss: 0.061350\n",
      "\n",
      "Test set: Average loss: 0.0899, Accuracy: 7855/8141 (96%), Positive accuracy: 3892/4091 (95%), Negative accuracy: 3963/4050 (98%), f1 score: 0.9650\n",
      "\n",
      "Train Epoch: 19 [0/71432 (0%)]\tLoss: 0.095924\n",
      "Train Epoch: 19 [6400/71432 (9%)]\tLoss: 0.048799\n",
      "Train Epoch: 19 [12800/71432 (18%)]\tLoss: 0.124519\n",
      "Train Epoch: 19 [19200/71432 (27%)]\tLoss: 0.093079\n",
      "Train Epoch: 19 [25600/71432 (36%)]\tLoss: 0.045975\n",
      "Train Epoch: 19 [32000/71432 (45%)]\tLoss: 0.022457\n",
      "Train Epoch: 19 [38400/71432 (54%)]\tLoss: 0.134293\n",
      "Train Epoch: 19 [44800/71432 (63%)]\tLoss: 0.182495\n",
      "Train Epoch: 19 [51200/71432 (72%)]\tLoss: 0.062918\n",
      "Train Epoch: 19 [57600/71432 (81%)]\tLoss: 0.074792\n",
      "Train Epoch: 19 [64000/71432 (90%)]\tLoss: 0.104111\n",
      "Train Epoch: 19 [70400/71432 (98%)]\tLoss: 0.151702\n",
      "\n",
      "Test set: Average loss: 0.0904, Accuracy: 7854/8141 (96%), Positive accuracy: 3888/4091 (95%), Negative accuracy: 3966/4050 (98%), f1 score: 0.9649\n",
      "\n",
      "Train Epoch: 20 [0/71432 (0%)]\tLoss: 0.137153\n",
      "Train Epoch: 20 [6400/71432 (9%)]\tLoss: 0.080275\n",
      "Train Epoch: 20 [12800/71432 (18%)]\tLoss: 0.058607\n",
      "Train Epoch: 20 [19200/71432 (27%)]\tLoss: 0.057746\n",
      "Train Epoch: 20 [25600/71432 (36%)]\tLoss: 0.134909\n",
      "Train Epoch: 20 [32000/71432 (45%)]\tLoss: 0.216509\n",
      "Train Epoch: 20 [38400/71432 (54%)]\tLoss: 0.081488\n",
      "Train Epoch: 20 [44800/71432 (63%)]\tLoss: 0.052191\n",
      "Train Epoch: 20 [51200/71432 (72%)]\tLoss: 0.128329\n",
      "Train Epoch: 20 [57600/71432 (81%)]\tLoss: 0.068671\n",
      "Train Epoch: 20 [64000/71432 (90%)]\tLoss: 0.074807\n",
      "Train Epoch: 20 [70400/71432 (98%)]\tLoss: 0.160225\n",
      "\n",
      "Test set: Average loss: 0.0900, Accuracy: 7855/8141 (96%), Positive accuracy: 3891/4091 (95%), Negative accuracy: 3964/4050 (98%), f1 score: 0.9650\n",
      "\n",
      "[7754, 7751, 7746, 7783, 7761, 7818, 7802, 7837, 7831, 7852, 7842, 7847, 7847, 7849, 7851, 7853, 7855, 7855, 7854, 7855]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CNNTrainedGabor(nn.Module):\n",
    "    def __init__(self, pretrain_gabor_model):\n",
    "        super(CNNTrainedGabor, self).__init__()\n",
    "        self.pretrain_gabor_model = pretrain_gabor_model\n",
    "        self.conv1 = nn.Conv2d(1, 4, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 5, 1)\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "#         self.dropout2 = nn.Dropout2d(0.5)\n",
    "        # self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc1 = nn.Linear(200, 128)\n",
    "#         self.fc2 = nn.Linear(128, 2)\n",
    "        self.fc2 = nn.Linear(128, 35)\n",
    "        self.fc3 = nn.Linear(42, 2)\n",
    "        self.fc_gabor1 = nn.Linear(96, 36)\n",
    "        self.fc_gabor2 = nn.Linear(36, 6)\n",
    "\n",
    "        \n",
    "   \n",
    "    def forward(self, x):\n",
    "        pc, x_cos, x_sin = self.pc(x)\n",
    "        pc = pc.view(x.shape[0],1)\n",
    "        x_comb = torch.cat((x_cos, x_sin), 3)\n",
    "        x_comb_fc = self.fc_gabor1(x_comb)\n",
    "        x_comb_fc = self.fc_gabor2(x_comb_fc)\n",
    "        x_comb_fc = torch.squeeze(x_comb_fc)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "#         x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "#         x = self.dropout2(x)\n",
    "#         print(pc.shape, x.shape)\n",
    "#         print(x_comb_fc.shape, x.shape)\n",
    "        x = torch.cat((x_comb_fc,pc, x), 1)\n",
    "        x = self.fc3(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def pc(self, x):\n",
    "        filter_cos = self.pretrain_gabor_model['filter_cos']\n",
    "        filter_sin = self.pretrain_gabor_model['filter_sin']\n",
    "        bias1 = self.pretrain_gabor_model['bias1']\n",
    "        bias2 = self.pretrain_gabor_model['bias2']\n",
    "        weights = self.pretrain_gabor_model['weights']\n",
    "        w = self.pretrain_gabor_model['w']\n",
    "        b = self.pretrain_gabor_model['b']\n",
    "        \n",
    "        x_cos = F.conv2d(x, filter_cos, bias=bias1)\n",
    "        x_sin = F.conv2d(x, filter_sin, bias=bias2)\n",
    "        x_comb = torch.cat((x_cos, x_sin), 2)\n",
    "\n",
    "        x_cos = x_cos.view(len(x), 1, 1, 48)\n",
    "        x_sin = x_sin.view(len(x), 1, 1, 48)\n",
    "        weighted_cos = (torch.matmul(x_cos, weights)).view(len(x), 1)\n",
    "        weighted_sin = (torch.matmul(x_sin, weights)).view(len(x), 1)\n",
    "\n",
    "        numerator = torch.norm(torch.cat([weighted_cos, weighted_sin], 1), dim=1)\n",
    "    #         print(\"numerator\", numerator.size())\n",
    "        x_comb_norm = torch.norm(x_comb, dim=2)\n",
    "        x_comb_norm = x_comb_norm.view(len(x), 1, 48)\n",
    "    #         print(\"x_comb_norm\", x_comb_norm.size())\n",
    "        denominator = torch.matmul(x_comb_norm, torch.abs(weights))\n",
    "        denominator = denominator.view(len(x))\n",
    "    #         print(\"size:\", numerator.size(), denominator.size())\n",
    "        pc = numerator / denominator                \n",
    "        return torch.sigmoid(w * pc + b),x_cos, x_sin\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "    train_loss = sum(loss_list)/len(loss_list)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "# def test(model, device, test_loader):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             output = model(data)\n",
    "#             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "#             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "#             correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "\n",
    "#     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         test_loss, correct, len(test_loader.dataset),\n",
    "#         100. * correct / len(test_loader.dataset)))\n",
    "def test(model, device, test_loader,epoch,train_loss):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    result= [[0,0], [0,0]] \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            cmat = confusion_matrix(target.view_as(pred), pred, labels=[0, 1]) \n",
    "            result = [[result[i][j] + cmat[i][j]  for j in range(len(result[0]))] for i in range(len(result))] \n",
    "            \n",
    "            # Store wrongly predicted images\n",
    "            if epoch == 7:\n",
    "                wrong_idx = (pred != target.view_as(pred)).nonzero()[:, 0]\n",
    "                wrong_samples = data[wrong_idx]\n",
    "                wrong_preds = pred[wrong_idx]\n",
    "                actual_preds = target.view_as(pred)[wrong_idx]\n",
    "                for i in range(len(wrong_idx)):\n",
    "                    sample = wrong_samples[i]\n",
    "                    wrong_pred = wrong_preds[i]\n",
    "                    actual_pred = actual_preds[i]\n",
    "                    sample = sample * 255.\n",
    "                    sample = sample.byte()\n",
    "                    img = TF.to_pil_image(sample)\n",
    "                    img.save('./wrong-mix/epoch{}_batch{}_idx{}_actual{}.png'.format(\n",
    "                        epoch,batch_idx,wrong_idx[i], actual_pred.item()))\n",
    "                    num = batch_idx * 64 + wrong_idx[i]\n",
    "                    img_ori = origin_dataset[num][0].numpy()\n",
    "                    plt.imsave('./wrong-mix/epoch{}_batch{}_idx{}_actual{}_ori.png'.format(\n",
    "                    epoch,batch_idx,wrong_idx[i], actual_pred.item()), img_ori[0], cmap = 'gray')\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    precision = result[1][1]/(result[1][1]+result[0][1])\n",
    "    recall = result[0][0]/(result[0][0]+result[1][0])\n",
    "    f1score = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Positive accuracy: {}/{} ({:.0f}%), Negative accuracy: {}/{} ({:.0f}%), f1 score: {:.4f}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset), \n",
    "        result[1][1],result[1][1]+result[1][0],100. * result[1][1]/(result[1][1]+result[1][0]),\n",
    "        result[0][0],result[0][0]+result[0][1],100. * result[0][0]/(result[0][0]+result[0][1]),f1score))\n",
    "    return test_loss, correct\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=100, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'batch_size': args.batch_size}\n",
    "    if use_cuda:\n",
    "        kwargs.update({'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True},\n",
    "                     )\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    model = CNNTrainedGabor(pretrain_gabor_model).to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_accuracy_list = []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_result = test(model, device, test_loader,epoch,train_loss)\n",
    "        test_loss_list.append(test_result[0])\n",
    "        test_accuracy_list.append(test_result[1])\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "    plt.plot(train_loss_list, color='blue',label='train loss')  \n",
    "    plt.plot(test_loss_list, color='red',label='test loss')  \n",
    "    plt.legend()\n",
    "    print(test_accuracy_list)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
